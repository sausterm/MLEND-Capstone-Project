{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_hyper_param\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_hyper_param\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  363122\n",
      " 0:  357809\n",
      " 1:  456392\n"
     ]
    }
   ],
   "source": [
    "stock_hyper_param_data = pd.read_csv('stock_indicator_data.csv',parse_dates=True, index_col=[0,1])\n",
    "get_target_distribution(stock_hyper_param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_hyper_param_data.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = []\n",
    "for ts in timeseries:\n",
    "    tickers.append(ts.index[1][1])\n",
    "cat = {}\n",
    "for ticker in enumerate(tickers):\n",
    "    cat[ticker[1]] = ticker[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "        \"cat\" : cat[ts.index[1][1]],\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training].values.T.tolist()\n",
    "        \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"cat\" : cat[ts.index[1][1]], # input stock ticker id\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 8s, sys: 3.72 s, total: 1min 12s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train_hyper_param.json\", training_data)\n",
    "write_json_dataset(\"test_hyper_param.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_hyper_param/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_hyper_param/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train_hyper_param.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"train_hyper_param.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, 1, 1, 0, -1, -1, 1, 1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, -1, -1, 1, -1, -1, -1, 1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 0, 0, 0, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 0, 1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, -1, 1, 1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 0, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 1, 1, 0, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 0, 1, 1, -1, -1, 0, -1, -1, -1, -1, 1, -1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, 0, 0, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 0, -1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 0, 0, -1, -1, -1, -1, 0, -1, 0, 0, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 0, 1, 0, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, -1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, -1, -1, -1, -1, -1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, -1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, -1, 0, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 0, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, -1, 0, 0, -1, -1, -1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 0, -1, 1, 1, 1, 1, -1, 0, -1, -1, -1, -1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, -1, 0, -1, -1, -1, -1, -1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, 0, 0, 0, 0, -1, 0, -1, 0, -1, -1, -1, 1, 1, 0, 0, 1, -1, 1, 1, 1, 0, 0, 0, -1, 0, 0, 1, -1, 0, 1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, 1, 0, 0, 1, 0, 1, 0, -1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, -1, 0, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, 0, 0, 1, 0, 0, -1, 0, -1, 1, 1, 1, -1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, -1, -1, -1, -1, 1, 1, 1, -1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, -1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, 0, -1, 0, -1, 0, 1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, 0, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, -1, 0, 0, 0, 1, 0, 1, 0, 0, -1, -1, -1, -1, -1, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, 0, 0, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, -1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, -1, -1, -1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, -1, 0, 0, -1, 0, 0, 1, 0, 0, 1, 0, -1, -1, 0, -1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, -1, -1, -1, 0, -1, -1, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, -1, -1, -1, -1, -1, -1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 1, 1, 1, 1, 0, 0, -1, 0, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, -1, -1, -1, 1, -1, -1, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, -1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 0, -1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, -1, -1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, 1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, -1, -1, -1], \"cat\": 0, \"dynamic_feat\": [[0.005604005948935651, 0.005597295489396916, 0.005610714448643913, 0.005604005948935651, 0.005654322146535707, 0.00571973295843697, 0.005684511375138122, 0.005793532334719271, 0.005753280356554463, 0.0057683730110251975, 0.005805273679081583, 0.0057666973559709865, 0.005793532334719271, 0.005838816177622894, 0.005795207989773483, 0.005749925616742712, 0.005699607949269802, 0.0057398609073498474, 0.0057365061675380994, 0.005684511375138122, 0.0057700491560370255, 0.005974673095867564, 0.005914292678832259, 0.0058958418548464495, 0.0060853687305876866, 0.006058533261881784, 0.006110526094451289, 0.006144071532738307, 0.00617426125129834, 0.006073628856098228, 0.006144071532738307, 0.006243027292968973, 0.00608033784576411, 0.006165875136705396, 0.005929385823260612, 0.005828753918018121, 0.005580522770253406, 0.005344035906596716, 0.005733151427726349, 0.0057130244587287075, 0.005837140032611064, 0.00583210816787225, 0.005677802875429861, 0.005733151427726349, 0.005377582324798971, 0.005464796740667323, 0.005204827188286006, 0.00525514387584368, 0.005265207115363693, 0.005238371646657788, 0.005280302219622518, 0.00549163122945799, 0.005426219927599108, 0.005278622644907361, 0.0054027396886625716, 0.0054329303871378425, 0.0050874225639000025, 0.005047169605819957, 0.0050035609280129284, 0.004949889010685885, 0.005105869958182486, 0.00515283484567412, 0.005275269374968465, 0.005399382499062732, 0.005471506710248438, 0.005444669771669682, 0.005468150010606217, 0.005437960292046185, 0.0053691942503755535, 0.005360808625740226, 0.0051578637706672275, 0.005146122916262532, 0.00512767454206481, 0.004897894708243528, 0.004767071614568146, 0.004741914250704544, 0.004659730229702153, 0.004653019770163419, 0.00479055479325039, 0.004830805301542344, 0.00480396983283644, 0.004728495291457547, 0.0048207425519799525, 0.004810678332544703, 0.004762040239786949, 0.0045322604059656645, 0.004617800146695043, 0.004614443937010439, 0.004624507666488069, 0.004815710687241136, 0.004912988832587117, 0.004970015489725908, 0.004840869031019972, 0.004825774416718765, 0.0047201081969493654, 0.004683210468638689, 0.004832482916427027, 0.004668115854337481, 0.004743590395716374, 0.004772102009434105, 0.004817387812168203, 0.00500020520828594, 0.004939825771165874, 0.004731850521226915, 0.004616120082022268, 0.004585930853419852, 0.00455406498984799, 0.004909635562648222, 0.004936470541396507, 0.004785521458638722, 0.0047888737486623785, 0.004688241353462266, 0.004587608468304537, 0.0046429580005162615, 0.0046127668120...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:10000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator_hyper_param = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": 50,\n",
    "    \"num_layers\":2,\n",
    "    \"dropout_rate\":0.1,\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_hyper_param.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "estimator_hyper_param_tuner = HyperparameterTuner(estimator = estimator_hyper_param, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'test:RMSE', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 30, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'epochs': IntegerParameter(50, 200),\n",
    "                                                    'context_length': IntegerParameter(10, 100),\n",
    "                                                    'mini_batch_size': IntegerParameter(32, 256),\n",
    "                                                    'learning_rate': ContinuousParameter(\"1E-5\", \"1E-3\"),\n",
    "                                                    'num_cells': IntegerParameter(30, 200),\n",
    "                                                    'dropout_rate': ContinuousParameter(0,0.2),\n",
    "                                                    'num_layers': IntegerParameter(1,3)\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "#estimator_hyper_param_tuner.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator_hyper_param = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"96\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"212\",\n",
    "    \"learning_rate\": \"4.177378470748047e-05\",\n",
    "    \"context_length\": \"94\",\n",
    "    \"prediction_length\": \"1\",\n",
    "    \"num_cells\": 115,\n",
    "    \"num_layers\":3,\n",
    "    \"dropout_rate\":0.04030803446099004,\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_hyper_param.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 01:43:48 Starting - Starting the training job...\n",
      "2020-06-09 01:43:50 Starting - Launching requested ML instances.........\n",
      "2020-06-09 01:45:22 Starting - Preparing the instances for training......\n",
      "2020-06-09 01:46:25 Downloading - Downloading input data...\n",
      "2020-06-09 01:47:08 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.04030803446099004', u'learning_rate': u'4.177378470748047e-05', u'num_cells': u'115', u'prediction_length': u'1', u'epochs': u'96', u'time_freq': u'D', u'context_length': u'94', u'num_layers': u'3', u'mini_batch_size': u'212', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] Final configuration: {u'dropout_rate': u'0.04030803446099004', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'4.177378470748047e-05', u'num_layers': u'3', u'epochs': u'96', u'embedding_dimension': u'10', u'num_cells': u'115', u'_num_kv_servers': u'auto', u'mini_batch_size': u'212', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'94', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:10 INFO 139663651043136] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] [cardinality=auto] Inferred value of cardinality=[491] from dataset.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] Real time series\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:11 INFO 139663651043136] Small number of time series. Doing 5 passes over dataset with prob 0.863543788187 per epoch.\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] Real time series\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] nvidia-smi took: 0.0252320766449 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:12 INFO 139663651043136] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 644.2370414733887, \"sum\": 644.2370414733887, \"min\": 644.2370414733887}}, \"EndTime\": 1591667233.13827, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667232.493013}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:13 INFO 139663651043136] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1163.1479263305664, \"sum\": 1163.1479263305664, \"min\": 1163.1479263305664}}, \"EndTime\": 1591667233.656308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667233.138367}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:19 INFO 139663651043136] Epoch[0] Batch[0] avg_epoch_loss=1.515083\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.51508259323\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:25 INFO 139663651043136] Epoch[0] Batch[5] avg_epoch_loss=1.431055\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:25 INFO 139663651043136] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.43105488903\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:25 INFO 139663651043136] Epoch[0] Batch [5]#011Speed: 177.10 samples/sec#011loss=1.431055\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] processed a total of 2114 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}, \"update.time\": {\"count\": 1, \"max\": 16978.18899154663, \"sum\": 16978.18899154663, \"min\": 16978.18899154663}}, \"EndTime\": 1591667250.634674, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667233.656381}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.511531785 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.40834380816\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:30 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_ecd01fa3-f2db-4c24-8b5c-7b752e274989-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 57.21712112426758, \"sum\": 57.21712112426758, \"min\": 57.21712112426758}}, \"EndTime\": 1591667250.69263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667250.634782}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:36 INFO 139663651043136] Epoch[1] Batch[0] avg_epoch_loss=1.360247\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:36 INFO 139663651043136] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.36024691024\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:41 INFO 139663651043136] Epoch[1] Batch[5] avg_epoch_loss=1.337282\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.33728226476\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:41 INFO 139663651043136] Epoch[1] Batch [5]#011Speed: 188.98 samples/sec#011loss=1.337282\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] processed a total of 2029 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16124.69482421875, \"sum\": 16124.69482421875, \"min\": 16124.69482421875}}, \"EndTime\": 1591667266.817491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667250.692711}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.830886222 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.31568282505\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:46 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_dce0bca7-f646-42d0-af35-ef107a4fb364-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.48704147338867, \"sum\": 50.48704147338867, \"min\": 50.48704147338867}}, \"EndTime\": 1591667266.868643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667266.817566}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:52 INFO 139663651043136] Epoch[2] Batch[0] avg_epoch_loss=1.289787\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.28978686063\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:57 INFO 139663651043136] Epoch[2] Batch[5] avg_epoch_loss=1.288094\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:57 INFO 139663651043136] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.28809399875\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:47:57 INFO 139663651043136] Epoch[2] Batch [5]#011Speed: 188.63 samples/sec#011loss=1.288094\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] processed a total of 2072 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16210.479021072388, \"sum\": 16210.479021072388, \"min\": 16210.479021072388}}, \"EndTime\": 1591667283.079286, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667266.868721}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.817451232 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.28975232682\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:03 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_3908b8c1-6bfd-4853-98f7-06646aff0aac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.389102935791016, \"sum\": 46.389102935791016, \"min\": 46.389102935791016}}, \"EndTime\": 1591667283.126381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667283.07938}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:08 INFO 139663651043136] Epoch[3] Batch[0] avg_epoch_loss=1.291468\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:08 INFO 139663651043136] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.29146835039\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:14 INFO 139663651043136] Epoch[3] Batch[5] avg_epoch_loss=1.283086\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:14 INFO 139663651043136] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.28308616494\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:14 INFO 139663651043136] Epoch[3] Batch [5]#011Speed: 188.77 samples/sec#011loss=1.283086\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] processed a total of 2077 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16152.667045593262, \"sum\": 16152.667045593262, \"min\": 16152.667045593262}}, \"EndTime\": 1591667299.279207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667283.126465}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.584669027 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.27433342124\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:19 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_7a698bd6-87ba-400a-b895-3feef00655d1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 74.6760368347168, \"sum\": 74.6760368347168, \"min\": 74.6760368347168}}, \"EndTime\": 1591667299.354697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667299.279282}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:24 INFO 139663651043136] Epoch[4] Batch[0] avg_epoch_loss=1.293256\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.29325636378\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:30 INFO 139663651043136] Epoch[4] Batch[5] avg_epoch_loss=1.284660\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.28465995549\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:30 INFO 139663651043136] Epoch[4] Batch [5]#011Speed: 187.82 samples/sec#011loss=1.284660\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] Epoch[4] Batch[10] avg_epoch_loss=1.286349\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.28837493321\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] Epoch[4] Batch [10]#011Speed: 167.79 samples/sec#011loss=1.288375\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] processed a total of 2175 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17379.06813621521, \"sum\": 17379.06813621521, \"min\": 17379.06813621521}}, \"EndTime\": 1591667316.73394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667299.35479}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.149647178 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.28634858172\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:36 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:42 INFO 139663651043136] Epoch[5] Batch[0] avg_epoch_loss=1.297114\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.29711381444\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:47 INFO 139663651043136] Epoch[5] Batch[5] avg_epoch_loss=1.288626\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:47 INFO 139663651043136] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.2886256092\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:47 INFO 139663651043136] Epoch[5] Batch [5]#011Speed: 189.51 samples/sec#011loss=1.288626\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:52 INFO 139663651043136] processed a total of 2109 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16199.4948387146, \"sum\": 16199.4948387146, \"min\": 16199.4948387146}}, \"EndTime\": 1591667332.933975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667316.734022}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:52 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=130.188367291 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:52 INFO 139663651043136] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.28322301901\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:52 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:58 INFO 139663651043136] Epoch[6] Batch[0] avg_epoch_loss=1.257887\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:48:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.25788721049\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:03 INFO 139663651043136] Epoch[6] Batch[5] avg_epoch_loss=1.264858\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:03 INFO 139663651043136] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.26485757408\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:03 INFO 139663651043136] Epoch[6] Batch [5]#011Speed: 187.85 samples/sec#011loss=1.264858\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] Epoch[6] Batch[10] avg_epoch_loss=1.257216\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.24804693258\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] Epoch[6] Batch [10]#011Speed: 166.37 samples/sec#011loss=1.248047\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17408.282995224, \"sum\": 17408.282995224, \"min\": 17408.282995224}}, \"EndTime\": 1591667350.342825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667332.934046}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.503475455 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.2572163734\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:10 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_4ceb31bb-94f7-4633-8b83-c87e43bb394c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 53.83706092834473, \"sum\": 53.83706092834473, \"min\": 53.83706092834473}}, \"EndTime\": 1591667350.39729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667350.342915}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:15 INFO 139663651043136] Epoch[7] Batch[0] avg_epoch_loss=1.209214\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:15 INFO 139663651043136] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.20921412054\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:21 INFO 139663651043136] Epoch[7] Batch[5] avg_epoch_loss=1.229345\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:21 INFO 139663651043136] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.22934509373\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:21 INFO 139663651043136] Epoch[7] Batch [5]#011Speed: 188.58 samples/sec#011loss=1.229345\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] Epoch[7] Batch[10] avg_epoch_loss=1.249023\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.27263698938\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] Epoch[7] Batch [10]#011Speed: 167.15 samples/sec#011loss=1.272637\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] processed a total of 2210 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17393.423080444336, \"sum\": 17393.423080444336, \"min\": 17393.423080444336}}, \"EndTime\": 1591667367.790875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667350.397371}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.058548868 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.24902322812\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:27 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_afd87605-dc2d-4aae-8704-751eb5da9ffd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.52294921875, \"sum\": 48.52294921875, \"min\": 48.52294921875}}, \"EndTime\": 1591667367.840065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667367.790965}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:33 INFO 139663651043136] Epoch[8] Batch[0] avg_epoch_loss=1.286948\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.28694815006\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:38 INFO 139663651043136] Epoch[8] Batch[5] avg_epoch_loss=1.246905\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:38 INFO 139663651043136] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.24690472105\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:38 INFO 139663651043136] Epoch[8] Batch [5]#011Speed: 189.91 samples/sec#011loss=1.246905\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] processed a total of 2080 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16080.952882766724, \"sum\": 16080.952882766724, \"min\": 16080.952882766724}}, \"EndTime\": 1591667383.921178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667367.840137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.34438575 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.24494009198\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:43 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_c51194b1-4856-4b83-8b44-9a02a5208556-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.738935470581055, \"sum\": 45.738935470581055, \"min\": 45.738935470581055}}, \"EndTime\": 1591667383.967593, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667383.921277}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:49 INFO 139663651043136] Epoch[9] Batch[0] avg_epoch_loss=1.226796\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.22679570036\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:54 INFO 139663651043136] Epoch[9] Batch[5] avg_epoch_loss=1.228700\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.22869991806\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:49:54 INFO 139663651043136] Epoch[9] Batch [5]#011Speed: 189.05 samples/sec#011loss=1.228700\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:00 INFO 139663651043136] processed a total of 2027 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16170.07303237915, \"sum\": 16170.07303237915, \"min\": 16170.07303237915}}, \"EndTime\": 1591667400.137808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667383.967661}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:00 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.353892078 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:00 INFO 139663651043136] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:00 INFO 139663651043136] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.24659366968\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:00 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:05 INFO 139663651043136] Epoch[10] Batch[0] avg_epoch_loss=1.210135\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:05 INFO 139663651043136] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.21013454221\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:11 INFO 139663651043136] Epoch[10] Batch[5] avg_epoch_loss=1.222744\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.22274389207\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:11 INFO 139663651043136] Epoch[10] Batch [5]#011Speed: 187.95 samples/sec#011loss=1.222744\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] Epoch[10] Batch[10] avg_epoch_loss=1.127543\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=1.01330219485\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] Epoch[10] Batch [10]#011Speed: 167.35 samples/sec#011loss=1.013302\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] processed a total of 2131 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17411.02409362793, \"sum\": 17411.02409362793, \"min\": 17411.02409362793}}, \"EndTime\": 1591667417.549443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667400.137906}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.392849711 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.12754312061\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:17 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_385ebe99-7df3-43a4-b7b2-925f7cbbb12f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.264835357666016, \"sum\": 50.264835357666016, \"min\": 50.264835357666016}}, \"EndTime\": 1591667417.600333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667417.549522}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:23 INFO 139663651043136] Epoch[11] Batch[0] avg_epoch_loss=1.205558\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.20555841698\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:28 INFO 139663651043136] Epoch[11] Batch[5] avg_epoch_loss=1.218944\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.21894387779\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:28 INFO 139663651043136] Epoch[11] Batch [5]#011Speed: 187.61 samples/sec#011loss=1.218944\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:33 INFO 139663651043136] processed a total of 2090 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16255.166053771973, \"sum\": 16255.166053771973, \"min\": 16255.166053771973}}, \"EndTime\": 1591667433.855669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667417.600421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:33 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.573607942 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:33 INFO 139663651043136] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.21587075287\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:33 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:39 INFO 139663651043136] Epoch[12] Batch[0] avg_epoch_loss=1.254755\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.25475469625\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:44 INFO 139663651043136] Epoch[12] Batch[5] avg_epoch_loss=1.221548\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:44 INFO 139663651043136] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.22154764859\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:44 INFO 139663651043136] Epoch[12] Batch [5]#011Speed: 189.15 samples/sec#011loss=1.221548\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] Epoch[12] Batch[10] avg_epoch_loss=1.231961\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.24445636677\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] Epoch[12] Batch [10]#011Speed: 166.64 samples/sec#011loss=1.244456\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] processed a total of 2172 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17413.94805908203, \"sum\": 17413.94805908203, \"min\": 17413.94805908203}}, \"EndTime\": 1591667451.270233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667433.855743}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.726636453 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.23196070231\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:51 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:56 INFO 139663651043136] Epoch[13] Batch[0] avg_epoch_loss=1.205791\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:50:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.20579082561\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:02 INFO 139663651043136] Epoch[13] Batch[5] avg_epoch_loss=1.205000\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.20499981574\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:02 INFO 139663651043136] Epoch[13] Batch [5]#011Speed: 187.09 samples/sec#011loss=1.205000\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:07 INFO 139663651043136] processed a total of 2069 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16250.560998916626, \"sum\": 16250.560998916626, \"min\": 16250.560998916626}}, \"EndTime\": 1591667467.521307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667451.270325}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:07 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.317820383 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:07 INFO 139663651043136] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.2084520232\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:07 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:12 INFO 139663651043136] Epoch[14] Batch[0] avg_epoch_loss=1.174735\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.17473465542\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:18 INFO 139663651043136] Epoch[14] Batch[5] avg_epoch_loss=1.187769\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.1877690921\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:18 INFO 139663651043136] Epoch[14] Batch [5]#011Speed: 189.59 samples/sec#011loss=1.187769\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] Epoch[14] Batch[10] avg_epoch_loss=1.208025\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=1.23233246713\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] Epoch[14] Batch [10]#011Speed: 168.18 samples/sec#011loss=1.232332\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] processed a total of 2205 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17276.65400505066, \"sum\": 17276.65400505066, \"min\": 17276.65400505066}}, \"EndTime\": 1591667484.798487, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667467.52138}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.627911254 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.20802517166\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:24 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:30 INFO 139663651043136] Epoch[15] Batch[0] avg_epoch_loss=1.206316\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.20631595827\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:35 INFO 139663651043136] Epoch[15] Batch[5] avg_epoch_loss=1.184385\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.18438462791\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:35 INFO 139663651043136] Epoch[15] Batch [5]#011Speed: 190.26 samples/sec#011loss=1.184385\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] Epoch[15] Batch[10] avg_epoch_loss=1.190198\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=1.19717411545\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] Epoch[15] Batch [10]#011Speed: 168.87 samples/sec#011loss=1.197174\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] processed a total of 2181 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17231.034994125366, \"sum\": 17231.034994125366, \"min\": 17231.034994125366}}, \"EndTime\": 1591667502.030089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667484.798577}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=126.572979126 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.19019803134\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:42 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:47 INFO 139663651043136] Epoch[16] Batch[0] avg_epoch_loss=1.213191\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:47 INFO 139663651043136] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.2131907625\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:52 INFO 139663651043136] Epoch[16] Batch[5] avg_epoch_loss=1.177316\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.177315634\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:52 INFO 139663651043136] Epoch[16] Batch [5]#011Speed: 190.18 samples/sec#011loss=1.177316\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:58 INFO 139663651043136] processed a total of 2093 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16084.595918655396, \"sum\": 16084.595918655396, \"min\": 16084.595918655396}}, \"EndTime\": 1591667518.115244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667502.030175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:58 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=130.12335034 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:58 INFO 139663651043136] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.17734043193\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:51:58 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:03 INFO 139663651043136] Epoch[17] Batch[0] avg_epoch_loss=1.205004\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:03 INFO 139663651043136] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.20500442217\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:09 INFO 139663651043136] Epoch[17] Batch[5] avg_epoch_loss=1.166638\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:09 INFO 139663651043136] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.1666381524\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:09 INFO 139663651043136] Epoch[17] Batch [5]#011Speed: 190.32 samples/sec#011loss=1.166638\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:14 INFO 139663651043136] processed a total of 2081 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16171.358108520508, \"sum\": 16171.358108520508, \"min\": 16171.358108520508}}, \"EndTime\": 1591667534.287215, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667518.115341}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:14 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.683137611 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:14 INFO 139663651043136] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:14 INFO 139663651043136] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.16949133963\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:14 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:19 INFO 139663651043136] Epoch[18] Batch[0] avg_epoch_loss=1.155016\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.15501562155\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:25 INFO 139663651043136] Epoch[18] Batch[5] avg_epoch_loss=1.167125\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:25 INFO 139663651043136] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.16712492217\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:25 INFO 139663651043136] Epoch[18] Batch [5]#011Speed: 188.43 samples/sec#011loss=1.167125\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] Epoch[18] Batch[10] avg_epoch_loss=1.169482\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=1.17230991867\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] Epoch[18] Batch [10]#011Speed: 166.91 samples/sec#011loss=1.172310\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] processed a total of 2153 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17353.456020355225, \"sum\": 17353.456020355225, \"min\": 17353.456020355225}}, \"EndTime\": 1591667551.641301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667534.287316}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.066654916 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.16948173876\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:31 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:37 INFO 139663651043136] Epoch[19] Batch[0] avg_epoch_loss=1.156205\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:37 INFO 139663651043136] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.15620451153\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:42 INFO 139663651043136] Epoch[19] Batch[5] avg_epoch_loss=1.155118\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.15511771868\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:42 INFO 139663651043136] Epoch[19] Batch [5]#011Speed: 188.80 samples/sec#011loss=1.155118\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] Epoch[19] Batch[10] avg_epoch_loss=1.171329\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.19078154654\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] Epoch[19] Batch [10]#011Speed: 168.33 samples/sec#011loss=1.190782\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] processed a total of 2143 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17335.519075393677, \"sum\": 17335.519075393677, \"min\": 17335.519075393677}}, \"EndTime\": 1591667568.977361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667551.641374}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.61816499 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.17132854952\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:48 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:54 INFO 139663651043136] Epoch[20] Batch[0] avg_epoch_loss=1.147010\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.14700979557\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:59 INFO 139663651043136] Epoch[20] Batch[5] avg_epoch_loss=1.134424\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:59 INFO 139663651043136] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.1344239097\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:52:59 INFO 139663651043136] Epoch[20] Batch [5]#011Speed: 188.61 samples/sec#011loss=1.134424\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:05 INFO 139663651043136] processed a total of 2100 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16178.039073944092, \"sum\": 16178.039073944092, \"min\": 16178.039073944092}}, \"EndTime\": 1591667585.155957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667568.977434}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:05 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.804425021 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:05 INFO 139663651043136] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:05 INFO 139663651043136] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.13638158834\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:05 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:10 INFO 139663651043136] Epoch[21] Batch[0] avg_epoch_loss=1.151703\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=1.15170252098\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:16 INFO 139663651043136] Epoch[21] Batch[5] avg_epoch_loss=1.140852\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.14085178255\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:16 INFO 139663651043136] Epoch[21] Batch [5]#011Speed: 188.76 samples/sec#011loss=1.140852\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] Epoch[21] Batch[10] avg_epoch_loss=1.142515\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=1.14451069382\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] Epoch[21] Batch [10]#011Speed: 167.39 samples/sec#011loss=1.144511\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] processed a total of 2165 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17408.65182876587, \"sum\": 17408.65182876587, \"min\": 17408.65182876587}}, \"EndTime\": 1591667602.565285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667585.156056}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.362491028 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.14251492404\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:22 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:28 INFO 139663651043136] Epoch[22] Batch[0] avg_epoch_loss=1.129508\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.12950753266\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:33 INFO 139663651043136] Epoch[22] Batch[5] avg_epoch_loss=1.130427\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.13042729456\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:33 INFO 139663651043136] Epoch[22] Batch [5]#011Speed: 189.85 samples/sec#011loss=1.130427\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] processed a total of 2090 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16289.453029632568, \"sum\": 16289.453029632568, \"min\": 16289.453029632568}}, \"EndTime\": 1591667618.855295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667602.565376}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.302911243 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.12514978805\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:38 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_9e968347-b685-43c8-8785-91a3b6363d4c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.5390682220459, \"sum\": 46.5390682220459, \"min\": 46.5390682220459}}, \"EndTime\": 1591667618.902445, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667618.855372}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:44 INFO 139663651043136] Epoch[23] Batch[0] avg_epoch_loss=1.120809\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:44 INFO 139663651043136] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.12080915919\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:49 INFO 139663651043136] Epoch[23] Batch[5] avg_epoch_loss=1.127983\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.12798302129\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:49 INFO 139663651043136] Epoch[23] Batch [5]#011Speed: 190.07 samples/sec#011loss=1.127983\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:55 INFO 139663651043136] processed a total of 2116 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16170.723915100098, \"sum\": 16170.723915100098, \"min\": 16170.723915100098}}, \"EndTime\": 1591667635.073315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667618.90251}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:55 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=130.852575737 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:55 INFO 139663651043136] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:55 INFO 139663651043136] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.12802084437\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:53:55 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:00 INFO 139663651043136] Epoch[24] Batch[0] avg_epoch_loss=1.102090\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:00 INFO 139663651043136] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.10208950403\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:06 INFO 139663651043136] Epoch[24] Batch[5] avg_epoch_loss=1.122047\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.12204698047\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:06 INFO 139663651043136] Epoch[24] Batch [5]#011Speed: 188.95 samples/sec#011loss=1.122047\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:11 INFO 139663651043136] processed a total of 2034 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16213.23013305664, \"sum\": 16213.23013305664, \"min\": 16213.23013305664}}, \"EndTime\": 1591667651.28717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667635.073415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:11 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.451741548 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:11 INFO 139663651043136] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.13537096707\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:11 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:16 INFO 139663651043136] Epoch[25] Batch[0] avg_epoch_loss=1.125767\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.1257668981\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:22 INFO 139663651043136] Epoch[25] Batch[5] avg_epoch_loss=1.116103\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.11610296237\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:22 INFO 139663651043136] Epoch[25] Batch [5]#011Speed: 189.16 samples/sec#011loss=1.116103\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] Epoch[25] Batch[10] avg_epoch_loss=1.106647\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=1.09530012023\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] Epoch[25] Batch [10]#011Speed: 164.22 samples/sec#011loss=1.095300\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17565.903902053833, \"sum\": 17565.903902053833, \"min\": 17565.903902053833}}, \"EndTime\": 1591667668.85372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667651.287271}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.395286092 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.10664712503\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:28 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_775d5eec-52b9-4fa2-b780-06fed368ab5b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.532941818237305, \"sum\": 45.532941818237305, \"min\": 45.532941818237305}}, \"EndTime\": 1591667668.89987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667668.853807}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:34 INFO 139663651043136] Epoch[26] Batch[0] avg_epoch_loss=1.085537\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:34 INFO 139663651043136] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.08553717271\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:39 INFO 139663651043136] Epoch[26] Batch[5] avg_epoch_loss=1.108588\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.10858786481\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:39 INFO 139663651043136] Epoch[26] Batch [5]#011Speed: 189.16 samples/sec#011loss=1.108588\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:45 INFO 139663651043136] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16216.357946395874, \"sum\": 16216.357946395874, \"min\": 16216.357946395874}}, \"EndTime\": 1591667685.116385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667668.899951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:45 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.805950687 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:45 INFO 139663651043136] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.10971190974\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:45 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:50 INFO 139663651043136] Epoch[27] Batch[0] avg_epoch_loss=1.093320\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.09331973094\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:56 INFO 139663651043136] Epoch[27] Batch[5] avg_epoch_loss=1.097751\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.09775084969\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:54:56 INFO 139663651043136] Epoch[27] Batch [5]#011Speed: 188.52 samples/sec#011loss=1.097751\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] Epoch[27] Batch[10] avg_epoch_loss=1.092720\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=27, batch=10 train loss <loss>=1.08668199935\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] Epoch[27] Batch [10]#011Speed: 166.48 samples/sec#011loss=1.086682\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] processed a total of 2122 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17365.312099456787, \"sum\": 17365.312099456787, \"min\": 17365.312099456787}}, \"EndTime\": 1591667702.482297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667685.116492}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.196632755 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.09271955408\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:02 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_c7925cd5-1ade-4db2-97bd-22040720b989-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.76112937927246, \"sum\": 48.76112937927246, \"min\": 48.76112937927246}}, \"EndTime\": 1591667702.531676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667702.482394}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:07 INFO 139663651043136] Epoch[28] Batch[0] avg_epoch_loss=1.106683\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.10668311929\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:13 INFO 139663651043136] Epoch[28] Batch[5] avg_epoch_loss=1.106478\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:13 INFO 139663651043136] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.1064775095\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:13 INFO 139663651043136] Epoch[28] Batch [5]#011Speed: 188.71 samples/sec#011loss=1.106478\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] Epoch[28] Batch[10] avg_epoch_loss=1.087679\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=1.06512097053\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] Epoch[28] Batch [10]#011Speed: 163.82 samples/sec#011loss=1.065121\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] processed a total of 2124 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17518.959999084473, \"sum\": 17518.959999084473, \"min\": 17518.959999084473}}, \"EndTime\": 1591667720.05081, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667702.531767}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=121.239146338 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.0876790827\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:20 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_cc02eb32-ddcb-4110-b8e4-d26eaae619a7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.59206008911133, \"sum\": 55.59206008911133, \"min\": 55.59206008911133}}, \"EndTime\": 1591667720.107058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667720.050899}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:25 INFO 139663651043136] Epoch[29] Batch[0] avg_epoch_loss=1.090847\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:25 INFO 139663651043136] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.09084694341\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:31 INFO 139663651043136] Epoch[29] Batch[5] avg_epoch_loss=1.093291\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.09329052091\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:31 INFO 139663651043136] Epoch[29] Batch [5]#011Speed: 187.46 samples/sec#011loss=1.093291\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] Epoch[29] Batch[10] avg_epoch_loss=1.106704\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=1.12280074785\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] Epoch[29] Batch [10]#011Speed: 167.15 samples/sec#011loss=1.122801\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] processed a total of 2128 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17396.92711830139, \"sum\": 17396.92711830139, \"min\": 17396.92711830139}}, \"EndTime\": 1591667737.504158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667720.107128}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.31958096 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.10670426043\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:37 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:42 INFO 139663651043136] Epoch[30] Batch[0] avg_epoch_loss=1.062328\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.0623278348\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:48 INFO 139663651043136] Epoch[30] Batch[5] avg_epoch_loss=1.075974\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.07597387062\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:48 INFO 139663651043136] Epoch[30] Batch [5]#011Speed: 187.18 samples/sec#011loss=1.075974\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] Epoch[30] Batch[10] avg_epoch_loss=1.072596\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=1.06854249486\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] Epoch[30] Batch [10]#011Speed: 167.46 samples/sec#011loss=1.068542\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] processed a total of 2153 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17429.01301383972, \"sum\": 17429.01301383972, \"min\": 17429.01301383972}}, \"EndTime\": 1591667754.933737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667737.504234}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.52882058 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.07259597255\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:55:54 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_bfe2850e-e101-437d-bf82-7ffd861ee85e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 49.17097091674805, \"sum\": 49.17097091674805, \"min\": 49.17097091674805}}, \"EndTime\": 1591667754.983486, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667754.933812}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:00 INFO 139663651043136] Epoch[31] Batch[0] avg_epoch_loss=1.083919\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:00 INFO 139663651043136] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.08391851749\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:06 INFO 139663651043136] Epoch[31] Batch[5] avg_epoch_loss=1.077595\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.07759503299\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:06 INFO 139663651043136] Epoch[31] Batch [5]#011Speed: 188.74 samples/sec#011loss=1.077595\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] processed a total of 2085 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16259.957790374756, \"sum\": 16259.957790374756, \"min\": 16259.957790374756}}, \"EndTime\": 1591667771.243599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667754.983568}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.228103613 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.07213538548\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:11 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_9404b6ea-6ce1-4c9e-a1f2-6c00926fc1d1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.426151275634766, \"sum\": 48.426151275634766, \"min\": 48.426151275634766}}, \"EndTime\": 1591667771.2927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667771.24368}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:16 INFO 139663651043136] Epoch[32] Batch[0] avg_epoch_loss=1.069681\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.06968084371\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:22 INFO 139663651043136] Epoch[32] Batch[5] avg_epoch_loss=1.065373\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.06537312682\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:22 INFO 139663651043136] Epoch[32] Batch [5]#011Speed: 189.21 samples/sec#011loss=1.065373\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] processed a total of 2072 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16200.32000541687, \"sum\": 16200.32000541687, \"min\": 16200.32000541687}}, \"EndTime\": 1591667787.493162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667771.292768}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.897719945 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.06368843654\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:27 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_3d4559ac-9d7d-4726-af4b-8c71b850a994-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.34485054016113, \"sum\": 48.34485054016113, \"min\": 48.34485054016113}}, \"EndTime\": 1591667787.542207, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667787.493245}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:32 INFO 139663651043136] Epoch[33] Batch[0] avg_epoch_loss=1.062558\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:32 INFO 139663651043136] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.06255758034\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:38 INFO 139663651043136] Epoch[33] Batch[5] avg_epoch_loss=1.060215\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:38 INFO 139663651043136] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.06021486438\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:38 INFO 139663651043136] Epoch[33] Batch [5]#011Speed: 189.25 samples/sec#011loss=1.060215\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] processed a total of 2078 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16174.567937850952, \"sum\": 16174.567937850952, \"min\": 16174.567937850952}}, \"EndTime\": 1591667803.716943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667787.542299}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.472283284 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.05696115314\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:43 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_a0270a1b-c362-4b27-9518-8ea58e23f498-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.496795654296875, \"sum\": 47.496795654296875, \"min\": 47.496795654296875}}, \"EndTime\": 1591667803.765101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667803.71703}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:49 INFO 139663651043136] Epoch[34] Batch[0] avg_epoch_loss=1.040655\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.04065481222\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:54 INFO 139663651043136] Epoch[34] Batch[5] avg_epoch_loss=1.053750\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.05375038003\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:54 INFO 139663651043136] Epoch[34] Batch [5]#011Speed: 189.63 samples/sec#011loss=1.053750\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] processed a total of 2073 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16166.493892669678, \"sum\": 16166.493892669678, \"min\": 16166.493892669678}}, \"EndTime\": 1591667819.931753, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667803.765177}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.22725356 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.05375211464\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:56:59 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_0f0d7ff0-5239-4421-bbeb-fad4c1293420-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.873952865600586, \"sum\": 44.873952865600586, \"min\": 44.873952865600586}}, \"EndTime\": 1591667819.977245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667819.931828}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:05 INFO 139663651043136] Epoch[35] Batch[0] avg_epoch_loss=1.028271\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:05 INFO 139663651043136] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.02827057748\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:10 INFO 139663651043136] Epoch[35] Batch[5] avg_epoch_loss=1.040054\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.04005412935\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:10 INFO 139663651043136] Epoch[35] Batch [5]#011Speed: 189.82 samples/sec#011loss=1.040054\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] Epoch[35] Batch[10] avg_epoch_loss=1.045897\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=1.0529085987\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] Epoch[35] Batch [10]#011Speed: 166.83 samples/sec#011loss=1.052909\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] processed a total of 2147 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17356.39715194702, \"sum\": 17356.39715194702, \"min\": 17356.39715194702}}, \"EndTime\": 1591667837.333795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667819.977325}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.69991264 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.04589706997\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:17 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_b1dd46b2-2dee-4f7f-9ce4-6279fdc7c519-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.24311828613281, \"sum\": 47.24311828613281, \"min\": 47.24311828613281}}, \"EndTime\": 1591667837.381639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667837.333875}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:22 INFO 139663651043136] Epoch[36] Batch[0] avg_epoch_loss=1.035643\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.03564309174\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:28 INFO 139663651043136] Epoch[36] Batch[5] avg_epoch_loss=1.047511\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.04751055496\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:28 INFO 139663651043136] Epoch[36] Batch [5]#011Speed: 187.39 samples/sec#011loss=1.047511\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] processed a total of 2089 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16273.083925247192, \"sum\": 16273.083925247192, \"min\": 16273.083925247192}}, \"EndTime\": 1591667853.654879, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667837.381719}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.370344627 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.04089161855\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:33 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_549e1050-41f6-42f4-ae4b-5bac44e4b699-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.91004753112793, \"sum\": 46.91004753112793, \"min\": 46.91004753112793}}, \"EndTime\": 1591667853.702492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667853.654976}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:39 INFO 139663651043136] Epoch[37] Batch[0] avg_epoch_loss=1.030648\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.03064785364\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:44 INFO 139663651043136] Epoch[37] Batch[5] avg_epoch_loss=1.018510\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:44 INFO 139663651043136] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.01850991879\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:44 INFO 139663651043136] Epoch[37] Batch [5]#011Speed: 188.24 samples/sec#011loss=1.018510\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] processed a total of 2070 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16205.47604560852, \"sum\": 16205.47604560852, \"min\": 16205.47604560852}}, \"EndTime\": 1591667869.908118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667853.702576}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.733709206 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.02687178558\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:49 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_5c8837ec-1b33-4eb2-b8a1-bcaaf3a70176-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.766923904418945, \"sum\": 47.766923904418945, \"min\": 47.766923904418945}}, \"EndTime\": 1591667869.956548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667869.908192}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:55 INFO 139663651043136] Epoch[38] Batch[0] avg_epoch_loss=1.036427\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:57:55 INFO 139663651043136] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.03642733592\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:01 INFO 139663651043136] Epoch[38] Batch[5] avg_epoch_loss=1.030669\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:01 INFO 139663651043136] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.03066915836\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:01 INFO 139663651043136] Epoch[38] Batch [5]#011Speed: 186.53 samples/sec#011loss=1.030669\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] Epoch[38] Batch[10] avg_epoch_loss=1.035922\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=1.04222511435\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] Epoch[38] Batch [10]#011Speed: 165.22 samples/sec#011loss=1.042225\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] processed a total of 2126 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17589.399099349976, \"sum\": 17589.399099349976, \"min\": 17589.399099349976}}, \"EndTime\": 1591667887.546107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667869.95662}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=120.867281587 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.03592186563\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:07 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:12 INFO 139663651043136] Epoch[39] Batch[0] avg_epoch_loss=1.035568\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.03556787743\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:18 INFO 139663651043136] Epoch[39] Batch[5] avg_epoch_loss=1.026338\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.02633792949\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:18 INFO 139663651043136] Epoch[39] Batch [5]#011Speed: 188.57 samples/sec#011loss=1.026338\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] processed a total of 2107 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16262.3131275177, \"sum\": 16262.3131275177, \"min\": 16262.3131275177}}, \"EndTime\": 1591667903.808971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667887.546202}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.562289184 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.02566331827\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:23 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_1514ffd4-f451-4836-9b1b-d9881c582a5e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.00613021850586, \"sum\": 47.00613021850586, \"min\": 47.00613021850586}}, \"EndTime\": 1591667903.85665, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667903.809058}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:29 INFO 139663651043136] Epoch[40] Batch[0] avg_epoch_loss=0.980841\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:29 INFO 139663651043136] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.980840934897\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:34 INFO 139663651043136] Epoch[40] Batch[5] avg_epoch_loss=1.011708\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:34 INFO 139663651043136] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.01170822959\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:34 INFO 139663651043136] Epoch[40] Batch [5]#011Speed: 187.93 samples/sec#011loss=1.011708\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] processed a total of 2114 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16287.47010231018, \"sum\": 16287.47010231018, \"min\": 16287.47010231018}}, \"EndTime\": 1591667920.14429, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667903.856732}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.792005775 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.01491744563\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:40 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_164a13e4-615a-4de5-8e71-96fd1aa858a5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.395944595336914, \"sum\": 47.395944595336914, \"min\": 47.395944595336914}}, \"EndTime\": 1591667920.192362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667920.144372}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:45 INFO 139663651043136] Epoch[41] Batch[0] avg_epoch_loss=1.016471\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.01647099909\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:51 INFO 139663651043136] Epoch[41] Batch[5] avg_epoch_loss=1.008610\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:51 INFO 139663651043136] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.00860960379\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:51 INFO 139663651043136] Epoch[41] Batch [5]#011Speed: 189.13 samples/sec#011loss=1.008610\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] processed a total of 2056 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16291.437864303589, \"sum\": 16291.437864303589, \"min\": 16291.437864303589}}, \"EndTime\": 1591667936.483952, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667920.192434}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=126.200276486 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.00077818385\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:58:56 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_643d5214-62bf-4a68-8132-e0a17c00efff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 47.36781120300293, \"sum\": 47.36781120300293, \"min\": 47.36781120300293}}, \"EndTime\": 1591667936.532003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667936.48404}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:01 INFO 139663651043136] Epoch[42] Batch[0] avg_epoch_loss=0.991935\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:01 INFO 139663651043136] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.991935154177\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:07 INFO 139663651043136] Epoch[42] Batch[5] avg_epoch_loss=1.012335\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.01233525546\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:07 INFO 139663651043136] Epoch[42] Batch [5]#011Speed: 188.17 samples/sec#011loss=1.012335\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:12 INFO 139663651043136] processed a total of 2107 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16300.315856933594, \"sum\": 16300.315856933594, \"min\": 16300.315856933594}}, \"EndTime\": 1591667952.832492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667936.532092}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:12 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.260124804 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:12 INFO 139663651043136] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.0095846644\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:12 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:18 INFO 139663651043136] Epoch[43] Batch[0] avg_epoch_loss=1.003720\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.00372012156\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:23 INFO 139663651043136] Epoch[43] Batch[5] avg_epoch_loss=0.997630\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.99763030526\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:23 INFO 139663651043136] Epoch[43] Batch [5]#011Speed: 188.64 samples/sec#011loss=0.997630\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] Epoch[43] Batch[10] avg_epoch_loss=0.988819\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.978244594358\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] Epoch[43] Batch [10]#011Speed: 165.46 samples/sec#011loss=0.978245\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] processed a total of 2169 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17487.11085319519, \"sum\": 17487.11085319519, \"min\": 17487.11085319519}}, \"EndTime\": 1591667970.320197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667952.832592}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.033245554 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.988818618487\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:30 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_fc48edf1-e9b9-4f10-b925-b7741a21ad57-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 57.57904052734375, \"sum\": 57.57904052734375, \"min\": 57.57904052734375}}, \"EndTime\": 1591667970.378392, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667970.320287}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:35 INFO 139663651043136] Epoch[44] Batch[0] avg_epoch_loss=0.984009\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.984009220915\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:41 INFO 139663651043136] Epoch[44] Batch[5] avg_epoch_loss=0.991621\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.991621377333\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:41 INFO 139663651043136] Epoch[44] Batch [5]#011Speed: 187.37 samples/sec#011loss=0.991621\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:46 INFO 139663651043136] processed a total of 2030 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16337.505102157593, \"sum\": 16337.505102157593, \"min\": 16337.505102157593}}, \"EndTime\": 1591667986.716065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667970.378478}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:46 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.253089528 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:46 INFO 139663651043136] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:46 INFO 139663651043136] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.993725312431\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:46 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:52 INFO 139663651043136] Epoch[45] Batch[0] avg_epoch_loss=0.993856\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.993855602336\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:57 INFO 139663651043136] Epoch[45] Batch[5] avg_epoch_loss=0.994731\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:57 INFO 139663651043136] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.994731111347\u001b[0m\n",
      "\u001b[34m[06/09/2020 01:59:57 INFO 139663651043136] Epoch[45] Batch [5]#011Speed: 187.53 samples/sec#011loss=0.994731\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] Epoch[45] Batch[10] avg_epoch_loss=0.979062\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=0.96025911727\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] Epoch[45] Batch [10]#011Speed: 163.54 samples/sec#011loss=0.960259\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] processed a total of 2133 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17665.61007499695, \"sum\": 17665.61007499695, \"min\": 17665.61007499695}}, \"EndTime\": 1591668004.382308, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591667986.716139}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=120.742257195 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.97906202313\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:04 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_e818ea96-4cff-4916-ac4a-77e81433211d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 49.340009689331055, \"sum\": 49.340009689331055, \"min\": 49.340009689331055}}, \"EndTime\": 1591668004.432251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668004.382387}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:09 INFO 139663651043136] Epoch[46] Batch[0] avg_epoch_loss=1.013445\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:09 INFO 139663651043136] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.01344515243\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:15 INFO 139663651043136] Epoch[46] Batch[5] avg_epoch_loss=0.978594\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:15 INFO 139663651043136] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.978594198167\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:15 INFO 139663651043136] Epoch[46] Batch [5]#011Speed: 188.36 samples/sec#011loss=0.978594\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] Epoch[46] Batch[10] avg_epoch_loss=1.010256\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=1.04825045028\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] Epoch[46] Batch [10]#011Speed: 167.83 samples/sec#011loss=1.048250\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] processed a total of 2177 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17426.294088363647, \"sum\": 17426.294088363647, \"min\": 17426.294088363647}}, \"EndTime\": 1591668021.858697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668004.432329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.925235139 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.01025613094\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:21 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:27 INFO 139663651043136] Epoch[47] Batch[0] avg_epoch_loss=0.972602\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.972601692632\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:32 INFO 139663651043136] Epoch[47] Batch[5] avg_epoch_loss=0.975509\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:32 INFO 139663651043136] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.975508767854\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:32 INFO 139663651043136] Epoch[47] Batch [5]#011Speed: 187.98 samples/sec#011loss=0.975509\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] Epoch[47] Batch[10] avg_epoch_loss=0.923844\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.861846578346\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] Epoch[47] Batch [10]#011Speed: 167.25 samples/sec#011loss=0.861847\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] processed a total of 2168 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17415.38715362549, \"sum\": 17415.38715362549, \"min\": 17415.38715362549}}, \"EndTime\": 1591668039.274655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668021.85878}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.486629317 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.923844136259\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:39 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_58a7bb55-57ab-40e6-a630-0aa7f6ffc1c9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.01786613464355, \"sum\": 66.01786613464355, \"min\": 66.01786613464355}}, \"EndTime\": 1591668039.341304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668039.274748}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:44 INFO 139663651043136] Epoch[48] Batch[0] avg_epoch_loss=0.975711\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:44 INFO 139663651043136] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.975710670903\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:50 INFO 139663651043136] Epoch[48] Batch[5] avg_epoch_loss=0.978796\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.978796137204\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:50 INFO 139663651043136] Epoch[48] Batch [5]#011Speed: 188.35 samples/sec#011loss=0.978796\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] Epoch[48] Batch[10] avg_epoch_loss=0.957111\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.931089149331\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] Epoch[48] Batch [10]#011Speed: 166.48 samples/sec#011loss=0.931089\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] processed a total of 2143 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17434.36598777771, \"sum\": 17434.36598777771, \"min\": 17434.36598777771}}, \"EndTime\": 1591668056.775843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668039.341395}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.917191094 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.957111142716\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:00:56 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:02 INFO 139663651043136] Epoch[49] Batch[0] avg_epoch_loss=0.953199\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.953198990732\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:07 INFO 139663651043136] Epoch[49] Batch[5] avg_epoch_loss=0.973044\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.973044017576\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:07 INFO 139663651043136] Epoch[49] Batch [5]#011Speed: 188.75 samples/sec#011loss=0.973044\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:13 INFO 139663651043136] processed a total of 2096 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16241.141080856323, \"sum\": 16241.141080856323, \"min\": 16241.141080856323}}, \"EndTime\": 1591668073.017523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668056.775933}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:13 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.054058392 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:13 INFO 139663651043136] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:13 INFO 139663651043136] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.975930807725\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:13 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:18 INFO 139663651043136] Epoch[50] Batch[0] avg_epoch_loss=0.984158\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.984158066084\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:24 INFO 139663651043136] Epoch[50] Batch[5] avg_epoch_loss=0.973352\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.973351892435\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:24 INFO 139663651043136] Epoch[50] Batch [5]#011Speed: 189.38 samples/sec#011loss=0.973352\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] Epoch[50] Batch[10] avg_epoch_loss=0.979271\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=0.986374462775\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] Epoch[50] Batch [10]#011Speed: 167.51 samples/sec#011loss=0.986374\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17324.806213378906, \"sum\": 17324.806213378906, \"min\": 17324.806213378906}}, \"EndTime\": 1591668090.342947, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668073.017599}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.944184494 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.97927124259\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:30 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:35 INFO 139663651043136] Epoch[51] Batch[0] avg_epoch_loss=0.969177\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.969176814241\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:41 INFO 139663651043136] Epoch[51] Batch[5] avg_epoch_loss=0.964572\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.964572498633\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:41 INFO 139663651043136] Epoch[51] Batch [5]#011Speed: 189.19 samples/sec#011loss=0.964572\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] Epoch[51] Batch[10] avg_epoch_loss=0.950623\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.933882904053\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] Epoch[51] Batch [10]#011Speed: 168.05 samples/sec#011loss=0.933883\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] processed a total of 2128 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17364.394903182983, \"sum\": 17364.394903182983, \"min\": 17364.394903182983}}, \"EndTime\": 1591668107.707924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668090.343028}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.548659395 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.950622682915\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:47 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:53 INFO 139663651043136] Epoch[52] Batch[0] avg_epoch_loss=0.954417\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.95441681484\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:58 INFO 139663651043136] Epoch[52] Batch[5] avg_epoch_loss=0.953174\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.953173727359\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:01:58 INFO 139663651043136] Epoch[52] Batch [5]#011Speed: 189.55 samples/sec#011loss=0.953174\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:03 INFO 139663651043136] processed a total of 2107 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16204.568862915039, \"sum\": 16204.568862915039, \"min\": 16204.568862915039}}, \"EndTime\": 1591668123.913078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668107.708013}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:03 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=130.023902133 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:03 INFO 139663651043136] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:03 INFO 139663651043136] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.954345868668\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:03 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:09 INFO 139663651043136] Epoch[53] Batch[0] avg_epoch_loss=0.933211\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:09 INFO 139663651043136] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.933211056691\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:14 INFO 139663651043136] Epoch[53] Batch[5] avg_epoch_loss=0.946529\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:14 INFO 139663651043136] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.946529448407\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:14 INFO 139663651043136] Epoch[53] Batch [5]#011Speed: 188.99 samples/sec#011loss=0.946529\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:20 INFO 139663651043136] processed a total of 2051 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16169.43907737732, \"sum\": 16169.43907737732, \"min\": 16169.43907737732}}, \"EndTime\": 1591668140.083136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668123.913175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:20 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=126.843082326 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:20 INFO 139663651043136] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:20 INFO 139663651043136] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.936696473607\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:20 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:25 INFO 139663651043136] Epoch[54] Batch[0] avg_epoch_loss=0.927614\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:25 INFO 139663651043136] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.927614391975\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:31 INFO 139663651043136] Epoch[54] Batch[5] avg_epoch_loss=0.944074\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.944073635077\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:31 INFO 139663651043136] Epoch[54] Batch [5]#011Speed: 186.99 samples/sec#011loss=0.944074\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:36 INFO 139663651043136] processed a total of 2081 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16330.534934997559, \"sum\": 16330.534934997559, \"min\": 16330.534934997559}}, \"EndTime\": 1591668156.414265, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668140.083244}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:36 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.428848042 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:36 INFO 139663651043136] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:36 INFO 139663651043136] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.936533485269\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:36 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:41 INFO 139663651043136] Epoch[55] Batch[0] avg_epoch_loss=0.970286\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.970285523613\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:47 INFO 139663651043136] Epoch[55] Batch[5] avg_epoch_loss=0.934849\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:47 INFO 139663651043136] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.934848977335\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:47 INFO 139663651043136] Epoch[55] Batch [5]#011Speed: 189.19 samples/sec#011loss=0.934849\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] Epoch[55] Batch[10] avg_epoch_loss=0.925140\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.913489028643\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] Epoch[55] Batch [10]#011Speed: 166.25 samples/sec#011loss=0.913489\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] processed a total of 2182 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17428.95221710205, \"sum\": 17428.95221710205, \"min\": 17428.95221710205}}, \"EndTime\": 1591668173.843933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668156.414363}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.192989291 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.925139909748\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:53 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:59 INFO 139663651043136] Epoch[56] Batch[0] avg_epoch_loss=0.919267\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:02:59 INFO 139663651043136] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.9192665388\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:05 INFO 139663651043136] Epoch[56] Batch[5] avg_epoch_loss=0.920348\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:05 INFO 139663651043136] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.920348473315\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:05 INFO 139663651043136] Epoch[56] Batch [5]#011Speed: 186.97 samples/sec#011loss=0.920348\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] Epoch[56] Batch[10] avg_epoch_loss=0.928789\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=0.93891685054\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] Epoch[56] Batch [10]#011Speed: 164.88 samples/sec#011loss=0.938917\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] processed a total of 2143 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17606.606006622314, \"sum\": 17606.606006622314, \"min\": 17606.606006622314}}, \"EndTime\": 1591668191.451065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668173.844027}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=121.714661784 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.928788644781\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:11 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:16 INFO 139663651043136] Epoch[57] Batch[0] avg_epoch_loss=0.937233\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.937233115142\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:22 INFO 139663651043136] Epoch[57] Batch[5] avg_epoch_loss=0.914788\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.914787544394\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:22 INFO 139663651043136] Epoch[57] Batch [5]#011Speed: 188.60 samples/sec#011loss=0.914788\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] Epoch[57] Batch[10] avg_epoch_loss=0.938341\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.966604527887\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] Epoch[57] Batch [10]#011Speed: 167.33 samples/sec#011loss=0.966605\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] processed a total of 2128 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17417.764902114868, \"sum\": 17417.764902114868, \"min\": 17417.764902114868}}, \"EndTime\": 1591668208.869395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668191.451166}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.173172775 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.938340718709\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:28 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:34 INFO 139663651043136] Epoch[58] Batch[0] avg_epoch_loss=0.929646\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:34 INFO 139663651043136] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.929646114133\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:39 INFO 139663651043136] Epoch[58] Batch[5] avg_epoch_loss=0.927906\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.9279056765\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:39 INFO 139663651043136] Epoch[58] Batch [5]#011Speed: 188.03 samples/sec#011loss=0.927906\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:45 INFO 139663651043136] processed a total of 2084 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16336.419105529785, \"sum\": 16336.419105529785, \"min\": 16336.419105529785}}, \"EndTime\": 1591668225.206351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668208.869485}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:45 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.566628284 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:45 INFO 139663651043136] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.927457701485\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:45 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:50 INFO 139663651043136] Epoch[59] Batch[0] avg_epoch_loss=0.921388\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.921388374185\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:56 INFO 139663651043136] Epoch[59] Batch[5] avg_epoch_loss=0.914909\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.914909362793\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:03:56 INFO 139663651043136] Epoch[59] Batch [5]#011Speed: 187.77 samples/sec#011loss=0.914909\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] processed a total of 2063 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16378.355979919434, \"sum\": 16378.355979919434, \"min\": 16378.355979919434}}, \"EndTime\": 1591668241.585315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668225.206446}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.958017852 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.921013806901\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:01 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_acf27b8d-63ba-4d1f-adfe-264ad46f3444-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.13084411621094, \"sum\": 50.13084411621094, \"min\": 50.13084411621094}}, \"EndTime\": 1591668241.636104, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668241.585392}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:07 INFO 139663651043136] Epoch[60] Batch[0] avg_epoch_loss=0.919750\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.919750357574\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:12 INFO 139663651043136] Epoch[60] Batch[5] avg_epoch_loss=0.907473\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.907473066318\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:12 INFO 139663651043136] Epoch[60] Batch [5]#011Speed: 189.20 samples/sec#011loss=0.907473\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] Epoch[60] Batch[10] avg_epoch_loss=0.944199\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=0.988270597638\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] Epoch[60] Batch [10]#011Speed: 167.13 samples/sec#011loss=0.988271\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] processed a total of 2169 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17483.805894851685, \"sum\": 17483.805894851685, \"min\": 17483.805894851685}}, \"EndTime\": 1591668259.120066, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668241.636177}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.056782688 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.944199216918\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:19 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:24 INFO 139663651043136] Epoch[61] Batch[0] avg_epoch_loss=0.932244\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.932243922971\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:30 INFO 139663651043136] Epoch[61] Batch[5] avg_epoch_loss=0.913946\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.913946415643\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:30 INFO 139663651043136] Epoch[61] Batch [5]#011Speed: 188.72 samples/sec#011loss=0.913946\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] processed a total of 2102 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16354.22396659851, \"sum\": 16354.22396659851, \"min\": 16354.22396659851}}, \"EndTime\": 1591668275.474851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668259.120144}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.52835424 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.911443458413\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:35 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_090a9132-45b9-443a-9af7-73ec372bdb26-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 61.4781379699707, \"sum\": 61.4781379699707, \"min\": 61.4781379699707}}, \"EndTime\": 1591668275.537083, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668275.474948}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:40 INFO 139663651043136] Epoch[62] Batch[0] avg_epoch_loss=0.932207\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:40 INFO 139663651043136] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.932206783655\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:46 INFO 139663651043136] Epoch[62] Batch[5] avg_epoch_loss=0.910494\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:46 INFO 139663651043136] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.910494090626\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:46 INFO 139663651043136] Epoch[62] Batch [5]#011Speed: 187.65 samples/sec#011loss=0.910494\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] Epoch[62] Batch[10] avg_epoch_loss=0.890411\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.86631114168\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] Epoch[62] Batch [10]#011Speed: 166.61 samples/sec#011loss=0.866311\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] processed a total of 2134 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17469.80619430542, \"sum\": 17469.80619430542, \"min\": 17469.80619430542}}, \"EndTime\": 1591668293.007055, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668275.537166}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.152527547 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.890410932014\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:53 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_547dad96-4b67-4f83-81bf-f48074291b91-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.02503776550293, \"sum\": 46.02503776550293, \"min\": 46.02503776550293}}, \"EndTime\": 1591668293.053686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668293.007164}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:58 INFO 139663651043136] Epoch[63] Batch[0] avg_epoch_loss=0.894313\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:04:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.894313452379\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:04 INFO 139663651043136] Epoch[63] Batch[5] avg_epoch_loss=0.905095\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:04 INFO 139663651043136] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.905095142388\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:04 INFO 139663651043136] Epoch[63] Batch [5]#011Speed: 187.26 samples/sec#011loss=0.905095\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] Epoch[63] Batch[10] avg_epoch_loss=0.905599\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.906203619039\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] Epoch[63] Batch [10]#011Speed: 166.71 samples/sec#011loss=0.906204\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] processed a total of 2141 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17825.38390159607, \"sum\": 17825.38390159607, \"min\": 17825.38390159607}}, \"EndTime\": 1591668310.87922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668293.053763}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=120.108704257 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.905598995412\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:10 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:16 INFO 139663651043136] Epoch[64] Batch[0] avg_epoch_loss=0.893457\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.893456944879\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:22 INFO 139663651043136] Epoch[64] Batch[5] avg_epoch_loss=0.896823\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.896822947376\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:22 INFO 139663651043136] Epoch[64] Batch [5]#011Speed: 184.50 samples/sec#011loss=0.896823\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] Epoch[64] Batch[10] avg_epoch_loss=0.912973\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.932353642302\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] Epoch[64] Batch [10]#011Speed: 167.83 samples/sec#011loss=0.932354\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] processed a total of 2131 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17553.364038467407, \"sum\": 17553.364038467407, \"min\": 17553.364038467407}}, \"EndTime\": 1591668328.433126, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668310.879311}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=121.400470707 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.912973263251\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:28 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:33 INFO 139663651043136] Epoch[65] Batch[0] avg_epoch_loss=0.925546\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.925546394204\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:39 INFO 139663651043136] Epoch[65] Batch[5] avg_epoch_loss=0.903832\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.903831685864\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:39 INFO 139663651043136] Epoch[65] Batch [5]#011Speed: 189.06 samples/sec#011loss=0.903832\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:44 INFO 139663651043136] processed a total of 2120 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16192.851066589355, \"sum\": 16192.851066589355, \"min\": 16192.851066589355}}, \"EndTime\": 1591668344.626503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668328.433194}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:44 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=130.920970001 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:44 INFO 139663651043136] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:44 INFO 139663651043136] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.894527572056\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:44 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:50 INFO 139663651043136] Epoch[66] Batch[0] avg_epoch_loss=0.897585\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.897585454977\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:55 INFO 139663651043136] Epoch[66] Batch[5] avg_epoch_loss=0.896303\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:55 INFO 139663651043136] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.896302589081\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:05:55 INFO 139663651043136] Epoch[66] Batch [5]#011Speed: 188.93 samples/sec#011loss=0.896303\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:00 INFO 139663651043136] processed a total of 2108 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16243.892908096313, \"sum\": 16243.892908096313, \"min\": 16243.892908096313}}, \"EndTime\": 1591668360.871025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668344.626582}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:00 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.770745092 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:00 INFO 139663651043136] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:00 INFO 139663651043136] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.896888912849\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:00 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:06 INFO 139663651043136] Epoch[67] Batch[0] avg_epoch_loss=0.911330\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.911330457004\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:11 INFO 139663651043136] Epoch[67] Batch[5] avg_epoch_loss=0.898897\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.898897231\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:11 INFO 139663651043136] Epoch[67] Batch [5]#011Speed: 189.02 samples/sec#011loss=0.898897\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] Epoch[67] Batch[10] avg_epoch_loss=0.880177\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=0.857712151869\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] Epoch[67] Batch [10]#011Speed: 167.78 samples/sec#011loss=0.857712\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] processed a total of 2174 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17396.57688140869, \"sum\": 17396.57688140869, \"min\": 17396.57688140869}}, \"EndTime\": 1591668378.268183, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668360.871117}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.966156264 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.880176740486\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:18 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_39884365-c81e-4ec3-82ef-b7346a064683-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.28506088256836, \"sum\": 67.28506088256836, \"min\": 67.28506088256836}}, \"EndTime\": 1591668378.336054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668378.268273}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:23 INFO 139663651043136] Epoch[68] Batch[0] avg_epoch_loss=0.869662\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.869661799017\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:29 INFO 139663651043136] Epoch[68] Batch[5] avg_epoch_loss=0.874208\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:29 INFO 139663651043136] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.874207538629\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:29 INFO 139663651043136] Epoch[68] Batch [5]#011Speed: 189.90 samples/sec#011loss=0.874208\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] processed a total of 2090 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16300.796031951904, \"sum\": 16300.796031951904, \"min\": 16300.796031951904}}, \"EndTime\": 1591668394.637024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668378.336144}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.213329683 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.879574476998\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:34 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_f96ad600-9a70-429a-920f-4bcfb7f20f47-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.06294631958008, \"sum\": 46.06294631958008, \"min\": 46.06294631958008}}, \"EndTime\": 1591668394.683809, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668394.637137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:40 INFO 139663651043136] Epoch[69] Batch[0] avg_epoch_loss=0.884173\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:40 INFO 139663651043136] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.884173483219\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:45 INFO 139663651043136] Epoch[69] Batch[5] avg_epoch_loss=0.883708\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.883707622312\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:45 INFO 139663651043136] Epoch[69] Batch [5]#011Speed: 188.65 samples/sec#011loss=0.883708\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:50 INFO 139663651043136] processed a total of 2103 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16299.139022827148, \"sum\": 16299.139022827148, \"min\": 16299.139022827148}}, \"EndTime\": 1591668410.983099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668394.683887}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:50 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.023715916 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:50 INFO 139663651043136] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.880110398778\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:50 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:56 INFO 139663651043136] Epoch[70] Batch[0] avg_epoch_loss=0.873127\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:06:56 INFO 139663651043136] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.87312655179\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:02 INFO 139663651043136] Epoch[70] Batch[5] avg_epoch_loss=0.879405\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.879404679784\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:02 INFO 139663651043136] Epoch[70] Batch [5]#011Speed: 189.72 samples/sec#011loss=0.879405\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] Epoch[70] Batch[10] avg_epoch_loss=0.881291\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.883554264285\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] Epoch[70] Batch [10]#011Speed: 167.16 samples/sec#011loss=0.883554\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] processed a total of 2133 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17407.371997833252, \"sum\": 17407.371997833252, \"min\": 17407.371997833252}}, \"EndTime\": 1591668428.391219, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668410.983243}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.533503022 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.881290854557\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:08 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:13 INFO 139663651043136] Epoch[71] Batch[0] avg_epoch_loss=0.897214\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:13 INFO 139663651043136] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.897213989834\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:19 INFO 139663651043136] Epoch[71] Batch[5] avg_epoch_loss=0.890781\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.890781258637\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:19 INFO 139663651043136] Epoch[71] Batch [5]#011Speed: 188.01 samples/sec#011loss=0.890781\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:24 INFO 139663651043136] processed a total of 2107 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16332.715034484863, \"sum\": 16332.715034484863, \"min\": 16332.715034484863}}, \"EndTime\": 1591668444.724484, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668428.391292}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:24 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.003990871 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:24 INFO 139663651043136] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.886261432576\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:24 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:30 INFO 139663651043136] Epoch[72] Batch[0] avg_epoch_loss=0.866464\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.866463787151\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:35 INFO 139663651043136] Epoch[72] Batch[5] avg_epoch_loss=0.876294\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.876293746181\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:35 INFO 139663651043136] Epoch[72] Batch [5]#011Speed: 188.35 samples/sec#011loss=0.876294\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:41 INFO 139663651043136] processed a total of 2100 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16322.576999664307, \"sum\": 16322.576999664307, \"min\": 16322.576999664307}}, \"EndTime\": 1591668461.047695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668444.724558}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:41 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.65498822 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:41 INFO 139663651043136] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.879979842564\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:41 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:46 INFO 139663651043136] Epoch[73] Batch[0] avg_epoch_loss=0.873047\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:46 INFO 139663651043136] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.873047090926\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:52 INFO 139663651043136] Epoch[73] Batch[5] avg_epoch_loss=0.872701\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.872700709217\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:52 INFO 139663651043136] Epoch[73] Batch [5]#011Speed: 188.41 samples/sec#011loss=0.872701\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] Epoch[73] Batch[10] avg_epoch_loss=0.879792\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.888302094082\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] Epoch[73] Batch [10]#011Speed: 165.83 samples/sec#011loss=0.888302\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] processed a total of 2207 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17547.749996185303, \"sum\": 17547.749996185303, \"min\": 17547.749996185303}}, \"EndTime\": 1591668478.596011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668461.047797}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=125.770257582 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.879792247792\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:07:58 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:04 INFO 139663651043136] Epoch[74] Batch[0] avg_epoch_loss=0.856152\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:04 INFO 139663651043136] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.856152444516\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:09 INFO 139663651043136] Epoch[74] Batch[5] avg_epoch_loss=0.859690\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:09 INFO 139663651043136] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.859689532586\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:09 INFO 139663651043136] Epoch[74] Batch [5]#011Speed: 188.72 samples/sec#011loss=0.859690\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] Epoch[74] Batch[10] avg_epoch_loss=0.864531\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.870340613599\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] Epoch[74] Batch [10]#011Speed: 167.45 samples/sec#011loss=0.870341\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] processed a total of 2164 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17497.270822525024, \"sum\": 17497.270822525024, \"min\": 17497.270822525024}}, \"EndTime\": 1591668496.093864, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668478.596091}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.675468314 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.864530933046\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:16 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_b04a6541-43c8-45b4-898c-1fdda978a128-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 70.4641342163086, \"sum\": 70.4641342163086, \"min\": 70.4641342163086}}, \"EndTime\": 1591668496.164939, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668496.093956}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:21 INFO 139663651043136] Epoch[75] Batch[0] avg_epoch_loss=0.874820\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:21 INFO 139663651043136] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.874819989474\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:27 INFO 139663651043136] Epoch[75] Batch[5] avg_epoch_loss=0.858575\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.858574585345\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:27 INFO 139663651043136] Epoch[75] Batch [5]#011Speed: 189.42 samples/sec#011loss=0.858575\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16270.646095275879, \"sum\": 16270.646095275879, \"min\": 16270.646095275879}}, \"EndTime\": 1591668512.435761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668496.16503}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.37292024 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.857805324051\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:32 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_b95d9786-aceb-4a3a-bc72-96baaf2938e7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.2210807800293, \"sum\": 55.2210807800293, \"min\": 55.2210807800293}}, \"EndTime\": 1591668512.491625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668512.43586}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:37 INFO 139663651043136] Epoch[76] Batch[0] avg_epoch_loss=0.884784\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:37 INFO 139663651043136] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.884784122683\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:43 INFO 139663651043136] Epoch[76] Batch[5] avg_epoch_loss=0.856493\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:43 INFO 139663651043136] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.856492696318\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:43 INFO 139663651043136] Epoch[76] Batch [5]#011Speed: 186.49 samples/sec#011loss=0.856493\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:48 INFO 139663651043136] processed a total of 2095 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16372.952938079834, \"sum\": 16372.952938079834, \"min\": 16372.952938079834}}, \"EndTime\": 1591668528.864725, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668512.491703}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:48 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.954058858 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:48 INFO 139663651043136] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.866326861112\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:48 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:54 INFO 139663651043136] Epoch[77] Batch[0] avg_epoch_loss=0.861708\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.861708299169\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:59 INFO 139663651043136] Epoch[77] Batch[5] avg_epoch_loss=0.858102\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:59 INFO 139663651043136] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.858102174675\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:08:59 INFO 139663651043136] Epoch[77] Batch [5]#011Speed: 189.01 samples/sec#011loss=0.858102\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] Epoch[77] Batch[10] avg_epoch_loss=0.858326\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.858594757656\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] Epoch[77] Batch [10]#011Speed: 166.11 samples/sec#011loss=0.858595\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] processed a total of 2166 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17455.384016036987, \"sum\": 17455.384016036987, \"min\": 17455.384016036987}}, \"EndTime\": 1591668546.320719, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668528.864798}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.086878356 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.85832607603\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:06 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:11 INFO 139663651043136] Epoch[78] Batch[0] avg_epoch_loss=0.872133\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:11 INFO 139663651043136] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.872133219017\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:17 INFO 139663651043136] Epoch[78] Batch[5] avg_epoch_loss=0.861892\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.861892436285\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:17 INFO 139663651043136] Epoch[78] Batch [5]#011Speed: 187.92 samples/sec#011loss=0.861892\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] Epoch[78] Batch[10] avg_epoch_loss=0.882524\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.907281062288\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] Epoch[78] Batch [10]#011Speed: 165.88 samples/sec#011loss=0.907281\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] processed a total of 2171 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17550.696849822998, \"sum\": 17550.696849822998, \"min\": 17550.696849822998}}, \"EndTime\": 1591668563.871923, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668546.320805}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.697840957 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.882523629923\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:23 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:29 INFO 139663651043136] Epoch[79] Batch[0] avg_epoch_loss=0.882951\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:29 INFO 139663651043136] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.882951340585\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:35 INFO 139663651043136] Epoch[79] Batch[5] avg_epoch_loss=0.864334\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:35 INFO 139663651043136] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.864334274388\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:35 INFO 139663651043136] Epoch[79] Batch [5]#011Speed: 188.86 samples/sec#011loss=0.864334\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] Epoch[79] Batch[10] avg_epoch_loss=0.843844\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=0.819254821202\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] Epoch[79] Batch [10]#011Speed: 166.72 samples/sec#011loss=0.819255\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] processed a total of 2183 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17496.479034423828, \"sum\": 17496.479034423828, \"min\": 17496.479034423828}}, \"EndTime\": 1591668581.368959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668563.872015}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=124.767105031 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.843843613849\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:41 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_d01b0ecc-6648-4bc3-87ac-f0bbab9dbdb8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.10617256164551, \"sum\": 45.10617256164551, \"min\": 45.10617256164551}}, \"EndTime\": 1591668581.414686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668581.369033}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:46 INFO 139663651043136] Epoch[80] Batch[0] avg_epoch_loss=0.832772\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:46 INFO 139663651043136] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.832771733122\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:52 INFO 139663651043136] Epoch[80] Batch[5] avg_epoch_loss=0.856943\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:52 INFO 139663651043136] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.856942746624\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:52 INFO 139663651043136] Epoch[80] Batch [5]#011Speed: 189.02 samples/sec#011loss=0.856943\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:57 INFO 139663651043136] processed a total of 1969 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16262.401103973389, \"sum\": 16262.401103973389, \"min\": 16262.401103973389}}, \"EndTime\": 1591668597.677258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668581.414771}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:57 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=121.075745272 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:57 INFO 139663651043136] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:57 INFO 139663651043136] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.862823025685\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:09:57 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:03 INFO 139663651043136] Epoch[81] Batch[0] avg_epoch_loss=0.839243\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:03 INFO 139663651043136] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.839243187095\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:08 INFO 139663651043136] Epoch[81] Batch[5] avg_epoch_loss=0.848165\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:08 INFO 139663651043136] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.848164564409\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:08 INFO 139663651043136] Epoch[81] Batch [5]#011Speed: 186.70 samples/sec#011loss=0.848165\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:14 INFO 139663651043136] processed a total of 2073 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16374.159097671509, \"sum\": 16374.159097671509, \"min\": 16374.159097671509}}, \"EndTime\": 1591668614.052048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668597.677358}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:14 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=126.600793057 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:14 INFO 139663651043136] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:14 INFO 139663651043136] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.844007967103\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:14 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:19 INFO 139663651043136] Epoch[82] Batch[0] avg_epoch_loss=0.862948\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:19 INFO 139663651043136] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.86294807578\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:25 INFO 139663651043136] Epoch[82] Batch[5] avg_epoch_loss=0.862817\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:25 INFO 139663651043136] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.862817020536\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:25 INFO 139663651043136] Epoch[82] Batch [5]#011Speed: 188.50 samples/sec#011loss=0.862817\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] Epoch[82] Batch[10] avg_epoch_loss=0.861087\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.859011855215\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] Epoch[82] Batch [10]#011Speed: 166.04 samples/sec#011loss=0.859012\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] processed a total of 2137 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17511.924982070923, \"sum\": 17511.924982070923, \"min\": 17511.924982070923}}, \"EndTime\": 1591668631.564567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668614.05215}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.030203281 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.861087399936\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:31 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:37 INFO 139663651043136] Epoch[83] Batch[0] avg_epoch_loss=0.829570\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:37 INFO 139663651043136] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.829570122485\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:42 INFO 139663651043136] Epoch[83] Batch[5] avg_epoch_loss=0.855008\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:42 INFO 139663651043136] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.855007579492\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:42 INFO 139663651043136] Epoch[83] Batch [5]#011Speed: 187.06 samples/sec#011loss=0.855008\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] Epoch[83] Batch[10] avg_epoch_loss=0.864371\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=0.875606997508\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] Epoch[83] Batch [10]#011Speed: 165.97 samples/sec#011loss=0.875607\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] processed a total of 2121 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17585.397958755493, \"sum\": 17585.397958755493, \"min\": 17585.397958755493}}, \"EndTime\": 1591668649.150567, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668631.564654}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=120.610503126 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.864370951317\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:49 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:54 INFO 139663651043136] Epoch[84] Batch[0] avg_epoch_loss=0.884661\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:10:54 INFO 139663651043136] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.884660612862\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:00 INFO 139663651043136] Epoch[84] Batch[5] avg_epoch_loss=0.858656\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:00 INFO 139663651043136] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.858655521705\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:00 INFO 139663651043136] Epoch[84] Batch [5]#011Speed: 188.00 samples/sec#011loss=0.858656\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] Epoch[84] Batch[10] avg_epoch_loss=0.827060\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=0.789146149833\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] Epoch[84] Batch [10]#011Speed: 163.20 samples/sec#011loss=0.789146\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] processed a total of 2125 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17653.205156326294, \"sum\": 17653.205156326294, \"min\": 17653.205156326294}}, \"EndTime\": 1591668666.80436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668649.150657}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=120.373829049 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.827060352672\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:06 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_8688326b-64b5-419d-b54d-828633d59c20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 50.99201202392578, \"sum\": 50.99201202392578, \"min\": 50.99201202392578}}, \"EndTime\": 1591668666.855962, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668666.804451}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:12 INFO 139663651043136] Epoch[85] Batch[0] avg_epoch_loss=0.840314\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.84031396542\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:17 INFO 139663651043136] Epoch[85] Batch[5] avg_epoch_loss=0.840189\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:17 INFO 139663651043136] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.840189040082\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:17 INFO 139663651043136] Epoch[85] Batch [5]#011Speed: 187.63 samples/sec#011loss=0.840189\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:23 INFO 139663651043136] processed a total of 2072 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16357.409954071045, \"sum\": 16357.409954071045, \"min\": 16357.409954071045}}, \"EndTime\": 1591668683.213523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668666.85604}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:23 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=126.669320291 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:23 INFO 139663651043136] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:23 INFO 139663651043136] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.833685446685\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:23 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:28 INFO 139663651043136] Epoch[86] Batch[0] avg_epoch_loss=0.843158\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:28 INFO 139663651043136] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.84315814612\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:34 INFO 139663651043136] Epoch[86] Batch[5] avg_epoch_loss=0.835342\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:34 INFO 139663651043136] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.83534192739\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:34 INFO 139663651043136] Epoch[86] Batch [5]#011Speed: 181.20 samples/sec#011loss=0.835342\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:39 INFO 139663651043136] processed a total of 2110 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16577.001094818115, \"sum\": 16577.001094818115, \"min\": 16577.001094818115}}, \"EndTime\": 1591668699.791174, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668683.21362}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:39 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.28339431 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:39 INFO 139663651043136] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.838685276823\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:39 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:45 INFO 139663651043136] Epoch[87] Batch[0] avg_epoch_loss=0.835224\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.83522400766\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:50 INFO 139663651043136] Epoch[87] Batch[5] avg_epoch_loss=0.839938\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.839937785886\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:50 INFO 139663651043136] Epoch[87] Batch [5]#011Speed: 189.93 samples/sec#011loss=0.839938\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] Epoch[87] Batch[10] avg_epoch_loss=0.879434\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=0.926828981795\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] Epoch[87] Batch [10]#011Speed: 167.49 samples/sec#011loss=0.926829\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] processed a total of 2129 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17396.19207382202, \"sum\": 17396.19207382202, \"min\": 17396.19207382202}}, \"EndTime\": 1591668717.187974, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668699.791285}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.382205913 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.879433784027\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:11:57 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:02 INFO 139663651043136] Epoch[88] Batch[0] avg_epoch_loss=0.846092\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:02 INFO 139663651043136] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.846092224121\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:08 INFO 139663651043136] Epoch[88] Batch[5] avg_epoch_loss=0.840016\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:08 INFO 139663651043136] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.840015591316\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:08 INFO 139663651043136] Epoch[88] Batch [5]#011Speed: 189.06 samples/sec#011loss=0.840016\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:13 INFO 139663651043136] processed a total of 2098 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16217.668056488037, \"sum\": 16217.668056488037, \"min\": 16217.668056488037}}, \"EndTime\": 1591668733.406146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668717.188061}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:13 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=129.36395302 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:13 INFO 139663651043136] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:13 INFO 139663651043136] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.844046877015\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:13 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:18 INFO 139663651043136] Epoch[89] Batch[0] avg_epoch_loss=0.879871\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:18 INFO 139663651043136] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.879870792605\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:24 INFO 139663651043136] Epoch[89] Batch[5] avg_epoch_loss=0.856953\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:24 INFO 139663651043136] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.856953267031\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:24 INFO 139663651043136] Epoch[89] Batch [5]#011Speed: 188.70 samples/sec#011loss=0.856953\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] Epoch[89] Batch[10] avg_epoch_loss=0.833466\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.805280548672\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] Epoch[89] Batch [10]#011Speed: 167.12 samples/sec#011loss=0.805281\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] processed a total of 2128 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17385.585069656372, \"sum\": 17385.585069656372, \"min\": 17385.585069656372}}, \"EndTime\": 1591668750.792343, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668733.406243}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.399339241 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.833465667777\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:30 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:36 INFO 139663651043136] Epoch[90] Batch[0] avg_epoch_loss=0.842907\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:36 INFO 139663651043136] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.842907455732\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:41 INFO 139663651043136] Epoch[90] Batch[5] avg_epoch_loss=0.839105\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:41 INFO 139663651043136] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.839105342169\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:41 INFO 139663651043136] Epoch[90] Batch [5]#011Speed: 189.22 samples/sec#011loss=0.839105\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] Epoch[90] Batch[10] avg_epoch_loss=0.832986\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=0.825641833611\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] Epoch[90] Batch [10]#011Speed: 166.66 samples/sec#011loss=0.825642\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] processed a total of 2161 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17456.988096237183, \"sum\": 17456.988096237183, \"min\": 17456.988096237183}}, \"EndTime\": 1591668768.249832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668750.79243}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.789088989 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.832985565552\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:48 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:53 INFO 139663651043136] Epoch[91] Batch[0] avg_epoch_loss=0.870649\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:53 INFO 139663651043136] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.870649229805\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:59 INFO 139663651043136] Epoch[91] Batch[5] avg_epoch_loss=0.847025\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:59 INFO 139663651043136] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.847024833631\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:12:59 INFO 139663651043136] Epoch[91] Batch [5]#011Speed: 187.77 samples/sec#011loss=0.847025\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:04 INFO 139663651043136] processed a total of 2100 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16350.141048431396, \"sum\": 16350.141048431396, \"min\": 16350.141048431396}}, \"EndTime\": 1591668784.600558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668768.24991}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:04 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.438270063 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:04 INFO 139663651043136] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:04 INFO 139663651043136] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.84600379512\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:04 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:10 INFO 139663651043136] Epoch[92] Batch[0] avg_epoch_loss=0.833831\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:10 INFO 139663651043136] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.833830779453\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:15 INFO 139663651043136] Epoch[92] Batch[5] avg_epoch_loss=0.841732\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:15 INFO 139663651043136] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.841732001155\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:15 INFO 139663651043136] Epoch[92] Batch [5]#011Speed: 187.76 samples/sec#011loss=0.841732\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] Epoch[92] Batch[10] avg_epoch_loss=0.833270\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.823115827452\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] Epoch[92] Batch [10]#011Speed: 167.19 samples/sec#011loss=0.823116\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] processed a total of 2136 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17474.278926849365, \"sum\": 17474.278926849365, \"min\": 17474.278926849365}}, \"EndTime\": 1591668802.075452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668784.600641}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=122.23590793 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.833270104017\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:22 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:27 INFO 139663651043136] Epoch[93] Batch[0] avg_epoch_loss=0.822303\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:27 INFO 139663651043136] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.822302836292\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:33 INFO 139663651043136] Epoch[93] Batch[5] avg_epoch_loss=0.833328\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:33 INFO 139663651043136] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.833327851206\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:33 INFO 139663651043136] Epoch[93] Batch [5]#011Speed: 188.94 samples/sec#011loss=0.833328\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] Epoch[93] Batch[10] avg_epoch_loss=0.813218\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.789085172257\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] Epoch[93] Batch [10]#011Speed: 166.47 samples/sec#011loss=0.789085\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] processed a total of 2162 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 17470.59988975525, \"sum\": 17470.59988975525, \"min\": 17470.59988975525}}, \"EndTime\": 1591668819.546604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668802.075534}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=123.749815162 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.813217542593\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:39 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/state_ef748765-5f5a-4bfb-a274-f2f334d73332-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 48.82001876831055, \"sum\": 48.82001876831055, \"min\": 48.82001876831055}}, \"EndTime\": 1591668819.596048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668819.546693}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:45 INFO 139663651043136] Epoch[94] Batch[0] avg_epoch_loss=0.842812\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:45 INFO 139663651043136] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.842811656448\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:50 INFO 139663651043136] Epoch[94] Batch[5] avg_epoch_loss=0.826325\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:50 INFO 139663651043136] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.826324618837\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:50 INFO 139663651043136] Epoch[94] Batch [5]#011Speed: 188.86 samples/sec#011loss=0.826325\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:55 INFO 139663651043136] processed a total of 2092 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16364.061832427979, \"sum\": 16364.061832427979, \"min\": 16364.061832427979}}, \"EndTime\": 1591668835.960258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668819.596129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:55 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=127.840164957 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:55 INFO 139663651043136] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:55 INFO 139663651043136] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.831806125281\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:13:55 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:01 INFO 139663651043136] Epoch[95] Batch[0] avg_epoch_loss=0.819494\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:01 INFO 139663651043136] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.81949356367\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:07 INFO 139663651043136] Epoch[95] Batch[5] avg_epoch_loss=0.821946\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:07 INFO 139663651043136] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.821945778229\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:07 INFO 139663651043136] Epoch[95] Batch [5]#011Speed: 187.32 samples/sec#011loss=0.821946\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] processed a total of 2116 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16429.176092147827, \"sum\": 16429.176092147827, \"min\": 16429.176092147827}}, \"EndTime\": 1591668852.390031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668835.960344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] #throughput_metric: host=algo-1, train throughput=128.793629809 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.825139466772\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] Final loss: 0.813217542593 (occurred at epoch 93)\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] #quality_metric: host=algo-1, train final_loss <loss>=0.813217542593\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 WARNING 139663651043136] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:12 INFO 139663651043136] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 677.3638725280762, \"sum\": 677.3638725280762, \"min\": 677.3638725280762}}, \"EndTime\": 1591668853.068588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668852.390194}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:13 INFO 139663651043136] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 839.3049240112305, \"sum\": 839.3049240112305, \"min\": 839.3049240112305}}, \"EndTime\": 1591668853.230458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668853.068674}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:13 INFO 139663651043136] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:13 INFO 139663651043136] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 26.275157928466797, \"sum\": 26.275157928466797, \"min\": 26.275157928466797}}, \"EndTime\": 1591668853.256878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668853.230544}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:13 INFO 139663651043136] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:13 INFO 139663651043136] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.049114227294921875, \"sum\": 0.049114227294921875, \"min\": 0.049114227294921875}}, \"EndTime\": 1591668853.257718, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668853.25694}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 5985.872030258179, \"sum\": 5985.872030258179, \"min\": 5985.872030258179}}, \"EndTime\": 1591668859.243539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668853.257778}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, RMSE): 0.647560342142\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, mean_absolute_QuantileLoss): 190.0824083207175\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, mean_wQuantileLoss): 0.41322262678416855\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.1]): 0.31235922686431716\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.2]): 0.4498996635753176\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.3]): 0.503994849998018\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.4]): 0.5152839703793112\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.5]): 0.4972944588339685\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.6]): 0.4601046328181806\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.7]): 0.40985899365466577\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.8]): 0.33932174059359915\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #test_score (algo-1, wQuantileLoss[0.9]): 0.2308861043401386\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.413222626784\u001b[0m\n",
      "\u001b[34m[06/09/2020 02:14:19 INFO 139663651043136] #quality_metric: host=algo-1, test RMSE <loss>=0.647560342142\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1628870.0878620148, \"sum\": 1628870.0878620148, \"min\": 1628870.0878620148}, \"setuptime\": {\"count\": 1, \"max\": 8.573055267333984, \"sum\": 8.573055267333984, \"min\": 8.573055267333984}}, \"EndTime\": 1591668859.292041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591668859.243619}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-09 02:14:32 Uploading - Uploading generated training model\n",
      "2020-06-09 02:14:32 Completed - Training job completed\n",
      "Training seconds: 1687\n",
      "Billable seconds: 1687\n",
      "CPU times: user 4.52 s, sys: 223 ms, total: 4.74 s\n",
      "Wall time: 30min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "estimator_hyper_param.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper_param_predictor = estimator_hyper_param.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type='ml.m4.xlarge',\n",
    "#    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_param_predictor = sagemaker.predictor.RealTimePredictor(endpoint='hyper-param-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>F</th>\n",
       "      <td>0.00187</td>\n",
       "      <td>0.055249</td>\n",
       "      <td>-1.929559</td>\n",
       "      <td>0.396906</td>\n",
       "      <td>-1.196386</td>\n",
       "      <td>-0.85156</td>\n",
       "      <td>-0.963647</td>\n",
       "      <td>0.654602</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1       PC2       PC3      PC4  \\\n",
       "Date       Ticker                                                               \n",
       "2019-01-02 F         0.00187  0.055249 -1.929559  0.396906 -1.196386 -0.85156   \n",
       "\n",
       "                        PC5       PC6  target  prediction  \n",
       "Date       Ticker                                          \n",
       "2019-01-02 F      -0.963647  0.654602       1           1  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('F', '2019-01-02',stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "#acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc['PEP'] = get_prediction_accuracy('PEP', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['PEP'])\n",
    "acc['IBM'] = get_prediction_accuracy('IBM', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['IBM'])\n",
    "acc['PXD'] = get_prediction_accuracy('PXD', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['PXD'])\n",
    "acc['VLO'] = get_prediction_accuracy('VLO', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['VLO'])\n",
    "acc['KMX'] = get_prediction_accuracy('KMX', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['KMX'])\n",
    "acc['YUM'] = get_prediction_accuracy('YUM', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['YUM'])\n",
    "acc['AIG'] = get_prediction_accuracy('AIG', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['AIG'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc['A'] = get_prediction_accuracy('A', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['A'])\n",
    "#acc['F'] = get_prediction_accuracy('F', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['F'])\n",
    "#acc['GE'] = get_prediction_accuracy('GE', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['GE'])\n",
    "#acc['DAL'] = get_prediction_accuracy('DAL', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['DAL'])\n",
    "#acc['UAL'] = get_prediction_accuracy('UAL', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['UAL'])\n",
    "#acc['ABC'] = get_prediction_accuracy('ABC', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['ABC'])\n",
    "#acc['CAT'] = get_prediction_accuracy('CAT', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['CAT'])\n",
    "#acc['DE'] = get_prediction_accuracy('DE', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['DE'])\n",
    "#acc['D'] = get_prediction_accuracy('D', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['D'])\n",
    "acc['FB'] = get_prediction_accuracy('FB', date_index, stock_hyper_param_data,hyper_param_predictor, dynamic_feat, cat['FB'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6693121693121693"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(acc.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.6388888888888888,\n",
       " 'F': 0.6785714285714286,\n",
       " 'GE': 0.5595238095238095,\n",
       " 'DAL': 0.7063492063492064,\n",
       " 'UAL': 0.7380952380952381,\n",
       " 'ABC': 0.6428571428571429,\n",
       " 'CAT': 0.6626984126984127,\n",
       " 'DE': 0.6309523809523809,\n",
       " 'D': 0.7658730158730159}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
