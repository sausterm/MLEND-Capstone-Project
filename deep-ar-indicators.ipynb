{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_indicator\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_indicator\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  390748\n",
      " 0:  389090\n",
      " 1:  397485\n"
     ]
    }
   ],
   "source": [
    "stock_indicator_data = pd.read_csv('stock_data.csv',parse_dates=True, index_col=[0,1])\n",
    "get_target_distribution(stock_indicator_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_indicator_data.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 7s, sys: 1.33 s, total: 1min 8s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train_indicator.json\", training_data)\n",
    "write_json_dataset(\"test_indicator.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_indicator/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_indicator/test/test.json\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train_indicator.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"test_indicator.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator_indicator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deep-ar-indicators-1',\n",
    "    output_path=s3_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"dropout_rate\": 0.04030803446099004,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_indicator.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 01:14:23 Starting - Starting the training job...\n",
      "2020-06-12 01:14:25 Starting - Launching requested ML instances......\n",
      "2020-06-12 01:15:29 Starting - Preparing the instances for training...\n",
      "2020-06-12 01:16:09 Downloading - Downloading input data......\n",
      "2020-06-12 01:16:57 Training - Downloading the training image\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.04030803446099004', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] Final configuration: {u'dropout_rate': u'0.04030803446099004', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:13 INFO 140292449302336] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] Real time series\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] min/mean/max target: -1.0/0.00129573177972/1.0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] mean abs(target): 0.671014271099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:15 INFO 140292449302336] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] Real time series\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] number of observations: 10562640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] mean target length: 2151\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] min/mean/max target: -1.0/0.00289700302197/1.0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] mean abs(target): 0.671493300917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] nvidia-smi took: 0.0252330303192 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 234.18116569519043, \"sum\": 234.18116569519043, \"min\": 234.18116569519043}}, \"EndTime\": 1591924647.941908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924647.706879}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:27 INFO 140292449302336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 426.5129566192627, \"sum\": 426.5129566192627, \"min\": 426.5129566192627}}, \"EndTime\": 1591924648.133526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924647.94198}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:29 INFO 140292449302336] Epoch[0] Batch[0] avg_epoch_loss=1.376350\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.3763500452\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:29 INFO 140292449302336] Epoch[0] Batch[5] avg_epoch_loss=1.363099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.36309893926\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:29 INFO 140292449302336] Epoch[0] Batch [5]#011Speed: 1091.42 samples/sec#011loss=1.363099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] Epoch[0] Batch[10] avg_epoch_loss=1.330525\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=1.29143607616\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] Epoch[0] Batch [10]#011Speed: 295.43 samples/sec#011loss=1.291436\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 2800.647020339966, \"sum\": 2800.647020339966, \"min\": 2800.647020339966}}, \"EndTime\": 1591924650.934307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924648.133584}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=237.076809414 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.33052491058\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:30 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_cead6446-a438-4caf-a86e-c98939650fdc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.68091583251953, \"sum\": 33.68091583251953, \"min\": 33.68091583251953}}, \"EndTime\": 1591924650.968695, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924650.934403}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:32 INFO 140292449302336] Epoch[1] Batch[0] avg_epoch_loss=1.280293\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.2802926302\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:32 INFO 140292449302336] Epoch[1] Batch[5] avg_epoch_loss=1.285085\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.28508500258\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:32 INFO 140292449302336] Epoch[1] Batch [5]#011Speed: 1102.72 samples/sec#011loss=1.285085\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] Epoch[1] Batch[10] avg_epoch_loss=1.291813\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=1.29988605976\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] Epoch[1] Batch [10]#011Speed: 331.16 samples/sec#011loss=1.299886\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2565.042018890381, \"sum\": 2565.042018890381, \"min\": 2565.042018890381}}, \"EndTime\": 1591924653.533869, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924650.968758}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=252.226072229 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.29181275584\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:33 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_7c004b12-43b8-47df-8d79-1b5b682ff2de-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.098012924194336, \"sum\": 26.098012924194336, \"min\": 26.098012924194336}}, \"EndTime\": 1591924653.560505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924653.533947}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:34 INFO 140292449302336] Epoch[2] Batch[0] avg_epoch_loss=1.261047\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.26104652882\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:35 INFO 140292449302336] Epoch[2] Batch[5] avg_epoch_loss=1.233184\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:35 INFO 140292449302336] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.23318441709\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:35 INFO 140292449302336] Epoch[2] Batch [5]#011Speed: 1012.88 samples/sec#011loss=1.233184\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] Epoch[2] Batch[10] avg_epoch_loss=1.233655\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.23422009945\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] Epoch[2] Batch [10]#011Speed: 334.46 samples/sec#011loss=1.234220\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2546.525001525879, \"sum\": 2546.525001525879, \"min\": 2546.525001525879}}, \"EndTime\": 1591924656.107157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924653.560572}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.274792871 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.2336551818\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:36 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_5809cb38-1834-4e35-9653-c00d3213bbce-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.327009201049805, \"sum\": 31.327009201049805, \"min\": 31.327009201049805}}, \"EndTime\": 1591924656.139016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924656.107236}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:37 INFO 140292449302336] Epoch[3] Batch[0] avg_epoch_loss=1.239029\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.23902893066\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:37 INFO 140292449302336] Epoch[3] Batch[5] avg_epoch_loss=1.246994\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.24699397882\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:37 INFO 140292449302336] Epoch[3] Batch [5]#011Speed: 1096.02 samples/sec#011loss=1.246994\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2452.6939392089844, \"sum\": 2452.6939392089844, \"min\": 2452.6939392089844}}, \"EndTime\": 1591924658.591831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924656.139079}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=248.28605066 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.18404353261\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:38 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_070fe85d-140c-4d08-94c4-dfe5003a6b94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.270036697387695, \"sum\": 21.270036697387695, \"min\": 21.270036697387695}}, \"EndTime\": 1591924658.613679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924658.591914}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:39 INFO 140292449302336] Epoch[4] Batch[0] avg_epoch_loss=1.218852\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.21885180473\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:40 INFO 140292449302336] Epoch[4] Batch[5] avg_epoch_loss=1.192284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:40 INFO 140292449302336] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.19228446484\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:40 INFO 140292449302336] Epoch[4] Batch [5]#011Speed: 1085.36 samples/sec#011loss=1.192284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:41 INFO 140292449302336] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2516.524076461792, \"sum\": 2516.524076461792, \"min\": 2516.524076461792}}, \"EndTime\": 1591924661.130316, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924658.613731}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:41 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=244.767401878 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:41 INFO 140292449302336] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.19923024178\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:41 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:42 INFO 140292449302336] Epoch[5] Batch[0] avg_epoch_loss=1.121403\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:42 INFO 140292449302336] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.12140309811\u001b[0m\n",
      "\n",
      "2020-06-12 01:17:36 Training - Training image download completed. Training in progress.\u001b[34m[06/12/2020 01:17:42 INFO 140292449302336] Epoch[5] Batch[5] avg_epoch_loss=1.196014\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:42 INFO 140292449302336] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.19601442417\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:42 INFO 140292449302336] Epoch[5] Batch [5]#011Speed: 1088.99 samples/sec#011loss=1.196014\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] Epoch[5] Batch[10] avg_epoch_loss=1.198366\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.20118787289\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] Epoch[5] Batch [10]#011Speed: 337.81 samples/sec#011loss=1.201188\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2530.5418968200684, \"sum\": 2530.5418968200684, \"min\": 2530.5418968200684}}, \"EndTime\": 1591924663.661468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924661.130428}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=255.269673635 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.19836599177\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:43 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:44 INFO 140292449302336] Epoch[6] Batch[0] avg_epoch_loss=1.189102\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:44 INFO 140292449302336] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.18910181522\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:45 INFO 140292449302336] Epoch[6] Batch[5] avg_epoch_loss=1.168510\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:45 INFO 140292449302336] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.16851035754\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:45 INFO 140292449302336] Epoch[6] Batch [5]#011Speed: 1043.43 samples/sec#011loss=1.168510\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] Epoch[6] Batch[10] avg_epoch_loss=1.174813\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.18237674236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] Epoch[6] Batch [10]#011Speed: 316.19 samples/sec#011loss=1.182377\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2584.8419666290283, \"sum\": 2584.8419666290283, \"min\": 2584.8419666290283}}, \"EndTime\": 1591924666.246783, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924663.661542}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=255.323007178 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.17481325973\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:46 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_e22763c2-7995-4e69-91c2-119e6cfe95a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.199932098388672, \"sum\": 31.199932098388672, \"min\": 31.199932098388672}}, \"EndTime\": 1591924666.278541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924666.246864}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:47 INFO 140292449302336] Epoch[7] Batch[0] avg_epoch_loss=1.059398\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.05939781666\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:47 INFO 140292449302336] Epoch[7] Batch[5] avg_epoch_loss=1.134344\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.13434439898\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:47 INFO 140292449302336] Epoch[7] Batch [5]#011Speed: 1092.02 samples/sec#011loss=1.134344\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2491.374969482422, \"sum\": 2491.374969482422, \"min\": 2491.374969482422}}, \"EndTime\": 1591924668.77004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924666.27861}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.249240074 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.13590788841\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:48 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_5f2a6316-efa5-481e-9dfb-3328a52beb5d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.683122634887695, \"sum\": 19.683122634887695, \"min\": 19.683122634887695}}, \"EndTime\": 1591924668.790302, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924668.770113}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:50 INFO 140292449302336] Epoch[8] Batch[0] avg_epoch_loss=1.137467\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:50 INFO 140292449302336] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.13746738434\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:50 INFO 140292449302336] Epoch[8] Batch[5] avg_epoch_loss=1.144729\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:50 INFO 140292449302336] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.14472879966\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:50 INFO 140292449302336] Epoch[8] Batch [5]#011Speed: 1065.07 samples/sec#011loss=1.144729\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] Epoch[8] Batch[10] avg_epoch_loss=1.112843\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.07457903624\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] Epoch[8] Batch [10]#011Speed: 318.34 samples/sec#011loss=1.074579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2587.0020389556885, \"sum\": 2587.0020389556885, \"min\": 2587.0020389556885}}, \"EndTime\": 1591924671.37743, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924668.79037}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.337167979 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.11284254356\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:51 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_ac3d1efe-d558-4214-b0fe-7bbb7693dfcb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.63111686706543, \"sum\": 26.63111686706543, \"min\": 26.63111686706543}}, \"EndTime\": 1591924671.404651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924671.377508}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:52 INFO 140292449302336] Epoch[9] Batch[0] avg_epoch_loss=1.193841\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.19384145737\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:53 INFO 140292449302336] Epoch[9] Batch[5] avg_epoch_loss=1.119045\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.11904476086\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:53 INFO 140292449302336] Epoch[9] Batch [5]#011Speed: 915.54 samples/sec#011loss=1.119045\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] Epoch[9] Batch[10] avg_epoch_loss=1.124748\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.13159265518\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] Epoch[9] Batch [10]#011Speed: 328.40 samples/sec#011loss=1.131593\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2602.018117904663, \"sum\": 2602.018117904663, \"min\": 2602.018117904663}}, \"EndTime\": 1591924674.006794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924671.404714}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.412482355 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.12474834919\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:54 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:55 INFO 140292449302336] Epoch[10] Batch[0] avg_epoch_loss=1.173697\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:55 INFO 140292449302336] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.1736972332\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:55 INFO 140292449302336] Epoch[10] Batch[5] avg_epoch_loss=1.111141\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:55 INFO 140292449302336] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.11114068826\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:55 INFO 140292449302336] Epoch[10] Batch [5]#011Speed: 1094.06 samples/sec#011loss=1.111141\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] Epoch[10] Batch[10] avg_epoch_loss=1.089180\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=1.06282627583\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] Epoch[10] Batch [10]#011Speed: 332.20 samples/sec#011loss=1.062826\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2535.259008407593, \"sum\": 2535.259008407593, \"min\": 2535.259008407593}}, \"EndTime\": 1591924676.542569, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924674.006854}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.610719114 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.0891795917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:56 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_b5162947-08ee-4df7-ba68-9db671a6469b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.227909088134766, \"sum\": 20.227909088134766, \"min\": 20.227909088134766}}, \"EndTime\": 1591924676.563381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924676.542651}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:57 INFO 140292449302336] Epoch[11] Batch[0] avg_epoch_loss=1.071992\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.07199239731\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:58 INFO 140292449302336] Epoch[11] Batch[5] avg_epoch_loss=1.072805\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.07280528545\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:58 INFO 140292449302336] Epoch[11] Batch [5]#011Speed: 1093.61 samples/sec#011loss=1.072805\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] Epoch[11] Batch[10] avg_epoch_loss=1.053827\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.03105224371\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] Epoch[11] Batch [10]#011Speed: 325.33 samples/sec#011loss=1.031052\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2541.626214981079, \"sum\": 2541.626214981079, \"min\": 2541.626214981079}}, \"EndTime\": 1591924679.10512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924676.56344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.697585534 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.05382663012\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:17:59 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_eb2d45aa-853e-4b0f-88fb-5b20f422015c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.37395668029785, \"sum\": 28.37395668029785, \"min\": 28.37395668029785}}, \"EndTime\": 1591924679.13407, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924679.105199}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:00 INFO 140292449302336] Epoch[12] Batch[0] avg_epoch_loss=1.125013\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:00 INFO 140292449302336] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.12501347065\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:00 INFO 140292449302336] Epoch[12] Batch[5] avg_epoch_loss=1.075385\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:00 INFO 140292449302336] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.07538495461\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:00 INFO 140292449302336] Epoch[12] Batch [5]#011Speed: 1102.14 samples/sec#011loss=1.075385\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:01 INFO 140292449302336] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2472.0020294189453, \"sum\": 2472.0020294189453, \"min\": 2472.0020294189453}}, \"EndTime\": 1591924681.606185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924679.134129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:01 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=255.650596209 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:01 INFO 140292449302336] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.06737817526\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:01 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:02 INFO 140292449302336] Epoch[13] Batch[0] avg_epoch_loss=0.983185\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:02 INFO 140292449302336] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=0.983184754848\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:03 INFO 140292449302336] Epoch[13] Batch[5] avg_epoch_loss=1.029120\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:03 INFO 140292449302336] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.02912033598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:03 INFO 140292449302336] Epoch[13] Batch [5]#011Speed: 1096.81 samples/sec#011loss=1.029120\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2450.721025466919, \"sum\": 2450.721025466919, \"min\": 2450.721025466919}}, \"EndTime\": 1591924684.05745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924681.60627}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.279422733 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.00823699236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:04 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_4c6d0924-8488-412f-8b09-969840564665-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.364999771118164, \"sum\": 20.364999771118164, \"min\": 20.364999771118164}}, \"EndTime\": 1591924684.078425, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924684.057522}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:05 INFO 140292449302336] Epoch[14] Batch[0] avg_epoch_loss=1.052949\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:05 INFO 140292449302336] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.05294859409\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:05 INFO 140292449302336] Epoch[14] Batch[5] avg_epoch_loss=1.014640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:05 INFO 140292449302336] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.01463976502\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:05 INFO 140292449302336] Epoch[14] Batch [5]#011Speed: 1088.15 samples/sec#011loss=1.014640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:06 INFO 140292449302336] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2462.2790813446045, \"sum\": 2462.2790813446045, \"min\": 2462.2790813446045}}, \"EndTime\": 1591924686.54082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924684.078484}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:06 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.067002839 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:06 INFO 140292449302336] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.00938520432\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:06 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:07 INFO 140292449302336] Epoch[15] Batch[0] avg_epoch_loss=1.059403\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:07 INFO 140292449302336] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.05940318108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:08 INFO 140292449302336] Epoch[15] Batch[5] avg_epoch_loss=1.004974\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:08 INFO 140292449302336] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.00497406721\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:08 INFO 140292449302336] Epoch[15] Batch [5]#011Speed: 338.12 samples/sec#011loss=1.004974\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2472.3281860351562, \"sum\": 2472.3281860351562, \"min\": 2472.3281860351562}}, \"EndTime\": 1591924689.013766, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924686.540893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=246.718839846 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.00437446833\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:09 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_6a249d62-6bd1-4598-a8ba-1570d1d804ea-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.52098846435547, \"sum\": 29.52098846435547, \"min\": 29.52098846435547}}, \"EndTime\": 1591924689.043865, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924689.013849}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:10 INFO 140292449302336] Epoch[16] Batch[0] avg_epoch_loss=1.012035\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:10 INFO 140292449302336] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.01203525066\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:10 INFO 140292449302336] Epoch[16] Batch[5] avg_epoch_loss=0.959996\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:10 INFO 140292449302336] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.959996422132\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:10 INFO 140292449302336] Epoch[16] Batch [5]#011Speed: 1069.97 samples/sec#011loss=0.959996\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] Epoch[16] Batch[10] avg_epoch_loss=0.986032\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.01727441549\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] Epoch[16] Batch [10]#011Speed: 328.51 samples/sec#011loss=1.017274\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2547.559976577759, \"sum\": 2547.559976577759, \"min\": 2547.559976577759}}, \"EndTime\": 1591924691.591559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924689.043935}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.098082154 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.98603187366\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:11 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_b99c579e-ca69-4aec-92a2-bf6ffcf40692-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.651817321777344, \"sum\": 20.651817321777344, \"min\": 20.651817321777344}}, \"EndTime\": 1591924691.612805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924691.591625}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:12 INFO 140292449302336] Epoch[17] Batch[0] avg_epoch_loss=0.914589\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:12 INFO 140292449302336] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.91458940506\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:13 INFO 140292449302336] Epoch[17] Batch[5] avg_epoch_loss=0.953025\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:13 INFO 140292449302336] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.953024715185\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:13 INFO 140292449302336] Epoch[17] Batch [5]#011Speed: 1103.12 samples/sec#011loss=0.953025\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.9440727233887, \"sum\": 2468.9440727233887, \"min\": 2468.9440727233887}}, \"EndTime\": 1591924694.081872, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924691.612868}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=251.512824276 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.958102375269\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:14 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_5da71c39-608e-49a3-8d76-7610ec8ebdc1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.34306526184082, \"sum\": 20.34306526184082, \"min\": 20.34306526184082}}, \"EndTime\": 1591924694.102788, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924694.081949}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:15 INFO 140292449302336] Epoch[18] Batch[0] avg_epoch_loss=1.031311\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:15 INFO 140292449302336] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.03131067753\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:15 INFO 140292449302336] Epoch[18] Batch[5] avg_epoch_loss=0.955340\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:15 INFO 140292449302336] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.955339620511\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:15 INFO 140292449302336] Epoch[18] Batch [5]#011Speed: 1057.30 samples/sec#011loss=0.955340\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.357976913452, \"sum\": 2459.357976913452, \"min\": 2459.357976913452}}, \"EndTime\": 1591924696.562257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924694.102847}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=242.737408479 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.949939209223\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:16 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_17ba3327-d592-4666-a339-9eabe3dd8460-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.546031951904297, \"sum\": 19.546031951904297, \"min\": 19.546031951904297}}, \"EndTime\": 1591924696.582379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924696.562317}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:17 INFO 140292449302336] Epoch[19] Batch[0] avg_epoch_loss=0.978234\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:17 INFO 140292449302336] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.978234052658\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:18 INFO 140292449302336] Epoch[19] Batch[5] avg_epoch_loss=0.952912\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.952912032604\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:18 INFO 140292449302336] Epoch[19] Batch [5]#011Speed: 1087.71 samples/sec#011loss=0.952912\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] Epoch[19] Batch[10] avg_epoch_loss=0.936772\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=0.917404294014\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] Epoch[19] Batch [10]#011Speed: 327.89 samples/sec#011loss=0.917404\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] processed a total of 698 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2557.2738647460938, \"sum\": 2557.2738647460938, \"min\": 2557.2738647460938}}, \"EndTime\": 1591924699.139811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924696.58244}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=272.930243294 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.936772151427\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:19 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_1d7d777d-af14-408d-a1c7-2706a573f053-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.306982040405273, \"sum\": 31.306982040405273, \"min\": 31.306982040405273}}, \"EndTime\": 1591924699.171689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924699.139893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:20 INFO 140292449302336] Epoch[20] Batch[0] avg_epoch_loss=0.932402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:20 INFO 140292449302336] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.93240237236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:20 INFO 140292449302336] Epoch[20] Batch[5] avg_epoch_loss=0.943730\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:20 INFO 140292449302336] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.943730324507\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:20 INFO 140292449302336] Epoch[20] Batch [5]#011Speed: 1022.02 samples/sec#011loss=0.943730\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] Epoch[20] Batch[10] avg_epoch_loss=0.969984\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=1.00148864985\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] Epoch[20] Batch [10]#011Speed: 338.56 samples/sec#011loss=1.001489\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2558.3879947662354, \"sum\": 2558.3879947662354, \"min\": 2558.3879947662354}}, \"EndTime\": 1591924701.730226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924699.171755}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.181721674 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.969984108751\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:21 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:22 INFO 140292449302336] Epoch[21] Batch[0] avg_epoch_loss=0.917326\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.917325973511\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:23 INFO 140292449302336] Epoch[21] Batch[5] avg_epoch_loss=0.933640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.93364049991\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:23 INFO 140292449302336] Epoch[21] Batch [5]#011Speed: 1089.54 samples/sec#011loss=0.933640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2443.3579444885254, \"sum\": 2443.3579444885254, \"min\": 2443.3579444885254}}, \"EndTime\": 1591924704.174109, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924701.730298}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=250.871828437 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.913753038645\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:24 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_08a787a0-72d7-4f7f-9015-ea8d17cf341f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.020931243896484, \"sum\": 27.020931243896484, \"min\": 27.020931243896484}}, \"EndTime\": 1591924704.201727, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924704.174192}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:25 INFO 140292449302336] Epoch[22] Batch[0] avg_epoch_loss=0.834601\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:25 INFO 140292449302336] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.834601044655\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:25 INFO 140292449302336] Epoch[22] Batch[5] avg_epoch_loss=0.885744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:25 INFO 140292449302336] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.885744293531\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:25 INFO 140292449302336] Epoch[22] Batch [5]#011Speed: 1008.47 samples/sec#011loss=0.885744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] Epoch[22] Batch[10] avg_epoch_loss=0.899343\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=0.915660607815\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] Epoch[22] Batch [10]#011Speed: 332.74 samples/sec#011loss=0.915661\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2576.505184173584, \"sum\": 2576.505184173584, \"min\": 2576.505184173584}}, \"EndTime\": 1591924706.778361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924704.201796}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.090045755 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.899342618205\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:26 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_2a2edbac-e5ef-4aa4-8d0b-18c5059110c5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.0889949798584, \"sum\": 32.0889949798584, \"min\": 32.0889949798584}}, \"EndTime\": 1591924706.811002, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924706.77844}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:28 INFO 140292449302336] Epoch[23] Batch[0] avg_epoch_loss=0.828153\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.828152894974\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:28 INFO 140292449302336] Epoch[23] Batch[5] avg_epoch_loss=0.896498\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.896497627099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:28 INFO 140292449302336] Epoch[23] Batch [5]#011Speed: 1101.26 samples/sec#011loss=0.896498\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] Epoch[23] Batch[10] avg_epoch_loss=0.917976\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.943750333786\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] Epoch[23] Batch [10]#011Speed: 324.73 samples/sec#011loss=0.943750\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2558.4568977355957, \"sum\": 2558.4568977355957, \"min\": 2558.4568977355957}}, \"EndTime\": 1591924709.369572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924706.81106}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.266175356 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.917976130139\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:29 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:30 INFO 140292449302336] Epoch[24] Batch[0] avg_epoch_loss=0.882858\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.882857739925\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:30 INFO 140292449302336] Epoch[24] Batch[5] avg_epoch_loss=0.890633\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.890633384387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:30 INFO 140292449302336] Epoch[24] Batch [5]#011Speed: 1036.65 samples/sec#011loss=0.890633\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] Epoch[24] Batch[10] avg_epoch_loss=0.902072\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.915797793865\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] Epoch[24] Batch [10]#011Speed: 333.89 samples/sec#011loss=0.915798\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2582.5321674346924, \"sum\": 2582.5321674346924, \"min\": 2582.5321674346924}}, \"EndTime\": 1591924711.952604, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924709.369651}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.64995086 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.902071752331\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:31 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:33 INFO 140292449302336] Epoch[25] Batch[0] avg_epoch_loss=0.936825\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.936824977398\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:33 INFO 140292449302336] Epoch[25] Batch[5] avg_epoch_loss=0.897195\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.897195349137\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:33 INFO 140292449302336] Epoch[25] Batch [5]#011Speed: 1075.44 samples/sec#011loss=0.897195\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] Epoch[25] Batch[10] avg_epoch_loss=0.865491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=0.827446746826\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] Epoch[25] Batch [10]#011Speed: 324.75 samples/sec#011loss=0.827447\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2565.7198429107666, \"sum\": 2565.7198429107666, \"min\": 2565.7198429107666}}, \"EndTime\": 1591924714.518861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924711.952681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=265.412007131 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.865491438996\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:34 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_47cc3744-4f4e-43c8-90ee-22b0f1a81319-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.425081253051758, \"sum\": 20.425081253051758, \"min\": 20.425081253051758}}, \"EndTime\": 1591924714.539841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924714.518923}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:35 INFO 140292449302336] Epoch[26] Batch[0] avg_epoch_loss=0.975780\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:35 INFO 140292449302336] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.975779950619\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] Epoch[26] Batch[5] avg_epoch_loss=0.896911\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.896910836299\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] Epoch[26] Batch [5]#011Speed: 1089.80 samples/sec#011loss=0.896911\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2435.506820678711, \"sum\": 2435.506820678711, \"min\": 2435.506820678711}}, \"EndTime\": 1591924716.975464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924714.5399}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=259.481542825 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.895721817017\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:36 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:38 INFO 140292449302336] Epoch[27] Batch[0] avg_epoch_loss=0.813599\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.813598811626\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:38 INFO 140292449302336] Epoch[27] Batch[5] avg_epoch_loss=0.866416\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.86641578873\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:38 INFO 140292449302336] Epoch[27] Batch [5]#011Speed: 1098.49 samples/sec#011loss=0.866416\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2465.891122817993, \"sum\": 2465.891122817993, \"min\": 2465.891122817993}}, \"EndTime\": 1591924719.441928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924716.975546}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=250.202942896 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.853688675165\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:39 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_4c5d3ac4-6e78-4754-9422-0c3aaa62df00-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.117998123168945, \"sum\": 20.117998123168945, \"min\": 20.117998123168945}}, \"EndTime\": 1591924719.462618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924719.441995}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:40 INFO 140292449302336] Epoch[28] Batch[0] avg_epoch_loss=0.895482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:40 INFO 140292449302336] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.895482301712\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] Epoch[28] Batch[5] avg_epoch_loss=0.848727\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.848726501067\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] Epoch[28] Batch [5]#011Speed: 1062.63 samples/sec#011loss=0.848727\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] Epoch[28] Batch[10] avg_epoch_loss=0.849552\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=0.850543117523\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] Epoch[28] Batch [10]#011Speed: 333.74 samples/sec#011loss=0.850543\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2533.8480472564697, \"sum\": 2533.8480472564697, \"min\": 2533.8480472564697}}, \"EndTime\": 1591924721.996578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924719.462677}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=260.856416085 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.84955223582\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:41 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:42 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_bd8d15b8-2dbe-4ce7-bbd1-6fb8a29192c3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.303955078125, \"sum\": 30.303955078125, \"min\": 30.303955078125}}, \"EndTime\": 1591924722.027417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924721.996656}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:43 INFO 140292449302336] Epoch[29] Batch[0] avg_epoch_loss=0.864233\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.864233195782\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:43 INFO 140292449302336] Epoch[29] Batch[5] avg_epoch_loss=0.857837\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.857836733262\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:43 INFO 140292449302336] Epoch[29] Batch [5]#011Speed: 1105.78 samples/sec#011loss=0.857837\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] Epoch[29] Batch[10] avg_epoch_loss=0.815288\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.764229500294\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] Epoch[29] Batch [10]#011Speed: 331.03 samples/sec#011loss=0.764230\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2572.4570751190186, \"sum\": 2572.4570751190186, \"min\": 2572.4570751190186}}, \"EndTime\": 1591924724.599986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924722.027475}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=252.276897586 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.815287991004\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:44 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_b5cd1045-48f9-4cb0-8aff-32f3e2c8d586-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.16305923461914, \"sum\": 20.16305923461914, \"min\": 20.16305923461914}}, \"EndTime\": 1591924724.620716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924724.600065}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:45 INFO 140292449302336] Epoch[30] Batch[0] avg_epoch_loss=0.828809\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:45 INFO 140292449302336] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.828809022903\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:46 INFO 140292449302336] Epoch[30] Batch[5] avg_epoch_loss=0.835973\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.835972905159\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:46 INFO 140292449302336] Epoch[30] Batch [5]#011Speed: 1076.08 samples/sec#011loss=0.835973\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] Epoch[30] Batch[10] avg_epoch_loss=0.816952\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=0.794127750397\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] Epoch[30] Batch [10]#011Speed: 328.66 samples/sec#011loss=0.794128\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2551.1629581451416, \"sum\": 2551.1629581451416, \"min\": 2551.1629581451416}}, \"EndTime\": 1591924727.17198, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924724.62076}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=267.31699542 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.816952380267\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:47 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:48 INFO 140292449302336] Epoch[31] Batch[0] avg_epoch_loss=0.729268\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.729268193245\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:48 INFO 140292449302336] Epoch[31] Batch[5] avg_epoch_loss=0.776986\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.776986350616\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:48 INFO 140292449302336] Epoch[31] Batch [5]#011Speed: 1088.18 samples/sec#011loss=0.776986\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2499.558925628662, \"sum\": 2499.558925628662, \"min\": 2499.558925628662}}, \"EndTime\": 1591924729.67204, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924727.172058}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=238.830475156 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.77123759985\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:49 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_a4403222-6314-4890-a334-054249aa7de1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.668973922729492, \"sum\": 30.668973922729492, \"min\": 30.668973922729492}}, \"EndTime\": 1591924729.703285, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924729.672124}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:50 INFO 140292449302336] Epoch[32] Batch[0] avg_epoch_loss=0.852751\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:50 INFO 140292449302336] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.852750897408\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:51 INFO 140292449302336] Epoch[32] Batch[5] avg_epoch_loss=0.792936\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.79293563962\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:51 INFO 140292449302336] Epoch[32] Batch [5]#011Speed: 1081.71 samples/sec#011loss=0.792936\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] Epoch[32] Batch[10] avg_epoch_loss=0.800031\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=0.808545315266\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] Epoch[32] Batch [10]#011Speed: 331.99 samples/sec#011loss=0.808545\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2529.2630195617676, \"sum\": 2529.2630195617676, \"min\": 2529.2630195617676}}, \"EndTime\": 1591924732.232663, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924729.703342}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=265.282617556 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.800030946732\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:52 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:53 INFO 140292449302336] Epoch[33] Batch[0] avg_epoch_loss=0.738542\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.738542020321\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:53 INFO 140292449302336] Epoch[33] Batch[5] avg_epoch_loss=0.718000\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.718000392119\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:53 INFO 140292449302336] Epoch[33] Batch [5]#011Speed: 1089.61 samples/sec#011loss=0.718000\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] Epoch[33] Batch[10] avg_epoch_loss=0.709369\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=0.699010467529\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] Epoch[33] Batch [10]#011Speed: 326.82 samples/sec#011loss=0.699010\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] processed a total of 705 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2629.399061203003, \"sum\": 2629.399061203003, \"min\": 2629.399061203003}}, \"EndTime\": 1591924734.862599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924732.232741}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=268.110869192 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.696818515658\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:54 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_86f425c3-1fce-4003-b247-ee71d7f9f0b6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.244121551513672, \"sum\": 20.244121551513672, \"min\": 20.244121551513672}}, \"EndTime\": 1591924734.88342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924734.862674}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:56 INFO 140292449302336] Epoch[34] Batch[0] avg_epoch_loss=0.724899\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.724898636341\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:56 INFO 140292449302336] Epoch[34] Batch[5] avg_epoch_loss=0.676633\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.67663299044\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:56 INFO 140292449302336] Epoch[34] Batch [5]#011Speed: 1086.15 samples/sec#011loss=0.676633\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] Epoch[34] Batch[10] avg_epoch_loss=0.694149\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=0.715168917179\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] Epoch[34] Batch [10]#011Speed: 341.64 samples/sec#011loss=0.715169\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2502.6350021362305, \"sum\": 2502.6350021362305, \"min\": 2502.6350021362305}}, \"EndTime\": 1591924737.386158, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924734.883464}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=275.296526284 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.694149320776\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:57 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_d6232b91-0aa3-4aad-9bca-bfe5e7627232-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.10512351989746, \"sum\": 20.10512351989746, \"min\": 20.10512351989746}}, \"EndTime\": 1591924737.406833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924737.38624}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:58 INFO 140292449302336] Epoch[35] Batch[0] avg_epoch_loss=0.742924\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.742924213409\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:58 INFO 140292449302336] Epoch[35] Batch[5] avg_epoch_loss=0.669527\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.669526626666\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:58 INFO 140292449302336] Epoch[35] Batch [5]#011Speed: 1036.45 samples/sec#011loss=0.669527\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] Epoch[35] Batch[10] avg_epoch_loss=0.709331\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.757097268105\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] Epoch[35] Batch [10]#011Speed: 337.28 samples/sec#011loss=0.757097\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2530.6899547576904, \"sum\": 2530.6899547576904, \"min\": 2530.6899547576904}}, \"EndTime\": 1591924739.937642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924737.406898}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.279584963 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.709331463684\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:18:59 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:01 INFO 140292449302336] Epoch[36] Batch[0] avg_epoch_loss=0.583678\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.583677589893\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:01 INFO 140292449302336] Epoch[36] Batch[5] avg_epoch_loss=0.615875\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.615875303745\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:01 INFO 140292449302336] Epoch[36] Batch [5]#011Speed: 1096.68 samples/sec#011loss=0.615875\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2466.9320583343506, \"sum\": 2466.9320583343506, \"min\": 2466.9320583343506}}, \"EndTime\": 1591924742.405145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924739.937721}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.690629201 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.679015982151\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:02 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_d8996d45-5222-437e-a87b-1602002ff609-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.652999877929688, \"sum\": 30.652999877929688, \"min\": 30.652999877929688}}, \"EndTime\": 1591924742.436379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924742.405229}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:03 INFO 140292449302336] Epoch[37] Batch[0] avg_epoch_loss=0.654796\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:03 INFO 140292449302336] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.654796183109\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] Epoch[37] Batch[5] avg_epoch_loss=0.683744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.683744420608\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] Epoch[37] Batch [5]#011Speed: 1087.62 samples/sec#011loss=0.683744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] Epoch[37] Batch[10] avg_epoch_loss=0.630816\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=0.567301595211\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] Epoch[37] Batch [10]#011Speed: 340.16 samples/sec#011loss=0.567302\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2516.324996948242, \"sum\": 2516.324996948242, \"min\": 2516.324996948242}}, \"EndTime\": 1591924744.952824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924742.43644}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=260.288549961 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.630815863609\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:04 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_14211879-2516-463e-aa26-489958f8b43f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.187854766845703, \"sum\": 20.187854766845703, \"min\": 20.187854766845703}}, \"EndTime\": 1591924744.97361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924744.952899}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:06 INFO 140292449302336] Epoch[38] Batch[0] avg_epoch_loss=0.665246\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.665246367455\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:06 INFO 140292449302336] Epoch[38] Batch[5] avg_epoch_loss=0.656901\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.656900862853\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:06 INFO 140292449302336] Epoch[38] Batch [5]#011Speed: 1089.43 samples/sec#011loss=0.656901\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:07 INFO 140292449302336] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2440.078020095825, \"sum\": 2440.078020095825, \"min\": 2440.078020095825}}, \"EndTime\": 1591924747.413808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924744.973671}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:07 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.176937193 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:07 INFO 140292449302336] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:07 INFO 140292449302336] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.661782038212\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:07 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:08 INFO 140292449302336] Epoch[39] Batch[0] avg_epoch_loss=0.671951\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:08 INFO 140292449302336] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.671951472759\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:08 INFO 140292449302336] Epoch[39] Batch[5] avg_epoch_loss=0.633387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:08 INFO 140292449302336] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.633386890093\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:08 INFO 140292449302336] Epoch[39] Batch [5]#011Speed: 1081.20 samples/sec#011loss=0.633387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.1680393218994, \"sum\": 2448.1680393218994, \"min\": 2448.1680393218994}}, \"EndTime\": 1591924749.862526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924747.413878}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=260.999675718 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.600209608674\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:09 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_5b88cb08-eda8-479f-8bc7-afce8f667a6c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.62702178955078, \"sum\": 20.62702178955078, \"min\": 20.62702178955078}}, \"EndTime\": 1591924749.883728, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924749.862602}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:11 INFO 140292449302336] Epoch[40] Batch[0] avg_epoch_loss=0.642673\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.642672836781\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:11 INFO 140292449302336] Epoch[40] Batch[5] avg_epoch_loss=0.585652\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.585652192434\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:11 INFO 140292449302336] Epoch[40] Batch [5]#011Speed: 960.75 samples/sec#011loss=0.585652\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:12 INFO 140292449302336] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2467.039108276367, \"sum\": 2467.039108276367, \"min\": 2467.039108276367}}, \"EndTime\": 1591924752.350895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924749.883803}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:12 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=237.926929696 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:12 INFO 140292449302336] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:12 INFO 140292449302336] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.66975351572\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:12 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:13 INFO 140292449302336] Epoch[41] Batch[0] avg_epoch_loss=0.586149\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:13 INFO 140292449302336] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.586148917675\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:13 INFO 140292449302336] Epoch[41] Batch[5] avg_epoch_loss=0.594039\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:13 INFO 140292449302336] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.594038685163\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:13 INFO 140292449302336] Epoch[41] Batch [5]#011Speed: 1094.68 samples/sec#011loss=0.594039\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:14 INFO 140292449302336] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2431.9121837615967, \"sum\": 2431.9121837615967, \"min\": 2431.9121837615967}}, \"EndTime\": 1591924754.783366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924752.350966}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:14 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=248.764187345 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:14 INFO 140292449302336] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:14 INFO 140292449302336] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.610267752409\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:14 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:16 INFO 140292449302336] Epoch[42] Batch[0] avg_epoch_loss=0.552449\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.552448749542\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:16 INFO 140292449302336] Epoch[42] Batch[5] avg_epoch_loss=0.609808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.609808375438\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:16 INFO 140292449302336] Epoch[42] Batch [5]#011Speed: 1043.64 samples/sec#011loss=0.609808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:17 INFO 140292449302336] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2470.453977584839, \"sum\": 2470.453977584839, \"min\": 2470.453977584839}}, \"EndTime\": 1591924757.254398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924754.783435}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:17 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=244.884263642 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:17 INFO 140292449302336] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:17 INFO 140292449302336] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.663845616579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:17 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:18 INFO 140292449302336] Epoch[43] Batch[0] avg_epoch_loss=0.502189\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.502189397812\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:18 INFO 140292449302336] Epoch[43] Batch[5] avg_epoch_loss=0.557770\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.557769834995\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:18 INFO 140292449302336] Epoch[43] Batch [5]#011Speed: 1095.62 samples/sec#011loss=0.557770\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] Epoch[43] Batch[10] avg_epoch_loss=0.410006\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.232690382004\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] Epoch[43] Batch [10]#011Speed: 340.28 samples/sec#011loss=0.232690\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2504.3468475341797, \"sum\": 2504.3468475341797, \"min\": 2504.3468475341797}}, \"EndTime\": 1591924759.759365, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924757.254463}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.340268953 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.410006447272\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:19 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_22984e1e-85d9-4261-b9da-f3ffa53ef99b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.056962966918945, \"sum\": 20.056962966918945, \"min\": 20.056962966918945}}, \"EndTime\": 1591924759.779959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924759.75943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:21 INFO 140292449302336] Epoch[44] Batch[0] avg_epoch_loss=0.553858\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.553857624531\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:21 INFO 140292449302336] Epoch[44] Batch[5] avg_epoch_loss=0.601938\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.601937969526\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:21 INFO 140292449302336] Epoch[44] Batch [5]#011Speed: 1051.59 samples/sec#011loss=0.601938\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] Epoch[44] Batch[10] avg_epoch_loss=0.567229\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.52557772994\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] Epoch[44] Batch [10]#011Speed: 341.03 samples/sec#011loss=0.525578\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2506.5371990203857, \"sum\": 2506.5371990203857, \"min\": 2506.5371990203857}}, \"EndTime\": 1591924762.286623, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924759.780027}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=266.093564544 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.567228769714\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:22 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:23 INFO 140292449302336] Epoch[45] Batch[0] avg_epoch_loss=0.540493\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.540493488312\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:23 INFO 140292449302336] Epoch[45] Batch[5] avg_epoch_loss=0.530617\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.530616765221\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:23 INFO 140292449302336] Epoch[45] Batch [5]#011Speed: 1095.66 samples/sec#011loss=0.530617\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:24 INFO 140292449302336] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2432.918071746826, \"sum\": 2432.918071746826, \"min\": 2432.918071746826}}, \"EndTime\": 1591924764.7201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924762.286684}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:24 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=260.168333871 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:24 INFO 140292449302336] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:24 INFO 140292449302336] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.520801085234\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:24 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:25 INFO 140292449302336] Epoch[46] Batch[0] avg_epoch_loss=0.482558\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:25 INFO 140292449302336] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.482557713985\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:26 INFO 140292449302336] Epoch[46] Batch[5] avg_epoch_loss=0.519860\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.519859741131\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:26 INFO 140292449302336] Epoch[46] Batch [5]#011Speed: 1096.04 samples/sec#011loss=0.519860\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:27 INFO 140292449302336] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2454.9219608306885, \"sum\": 2454.9219608306885, \"min\": 2454.9219608306885}}, \"EndTime\": 1591924767.175562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924764.720183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:27 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.577721012 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:27 INFO 140292449302336] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:27 INFO 140292449302336] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.504293294251\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:27 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:28 INFO 140292449302336] Epoch[47] Batch[0] avg_epoch_loss=0.553655\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.553654670715\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:28 INFO 140292449302336] Epoch[47] Batch[5] avg_epoch_loss=0.529581\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.529581492146\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:28 INFO 140292449302336] Epoch[47] Batch [5]#011Speed: 1093.42 samples/sec#011loss=0.529581\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:29 INFO 140292449302336] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.104166030884, \"sum\": 2474.104166030884, \"min\": 2474.104166030884}}, \"EndTime\": 1591924769.650281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924767.175647}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:29 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.262061538 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:29 INFO 140292449302336] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.524771109223\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:29 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:30 INFO 140292449302336] Epoch[48] Batch[0] avg_epoch_loss=0.443482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.443481564522\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:31 INFO 140292449302336] Epoch[48] Batch[5] avg_epoch_loss=0.485034\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.485033676028\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:31 INFO 140292449302336] Epoch[48] Batch [5]#011Speed: 1083.67 samples/sec#011loss=0.485034\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] Epoch[48] Batch[10] avg_epoch_loss=0.588171\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.711936336756\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] Epoch[48] Batch [10]#011Speed: 335.90 samples/sec#011loss=0.711936\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2509.4919204711914, \"sum\": 2509.4919204711914, \"min\": 2509.4919204711914}}, \"EndTime\": 1591924772.160373, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924769.650366}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=256.216454247 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.588171249086\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:32 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:33 INFO 140292449302336] Epoch[49] Batch[0] avg_epoch_loss=0.610795\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.610795259476\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:33 INFO 140292449302336] Epoch[49] Batch[5] avg_epoch_loss=0.577061\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.577061494191\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:33 INFO 140292449302336] Epoch[49] Batch [5]#011Speed: 1083.46 samples/sec#011loss=0.577061\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] Epoch[49] Batch[10] avg_epoch_loss=0.547022\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.51097394824\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] Epoch[49] Batch [10]#011Speed: 337.02 samples/sec#011loss=0.510974\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2511.043071746826, \"sum\": 2511.043071746826, \"min\": 2511.043071746826}}, \"EndTime\": 1591924774.671943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924772.160446}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=256.056558955 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.547021700577\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:34 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:35 INFO 140292449302336] Epoch[50] Batch[0] avg_epoch_loss=0.366457\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:35 INFO 140292449302336] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.366456568241\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:36 INFO 140292449302336] Epoch[50] Batch[5] avg_epoch_loss=0.477622\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.477622086803\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:36 INFO 140292449302336] Epoch[50] Batch [5]#011Speed: 1089.40 samples/sec#011loss=0.477622\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] Epoch[50] Batch[10] avg_epoch_loss=0.522040\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=0.575340950489\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] Epoch[50] Batch [10]#011Speed: 336.86 samples/sec#011loss=0.575341\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2508.796215057373, \"sum\": 2508.796215057373, \"min\": 2508.796215057373}}, \"EndTime\": 1591924777.181282, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924774.672026}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.278761676 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.522039752115\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:37 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:38 INFO 140292449302336] Epoch[51] Batch[0] avg_epoch_loss=0.504055\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.504055261612\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:38 INFO 140292449302336] Epoch[51] Batch[5] avg_epoch_loss=0.479436\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.479435577989\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:38 INFO 140292449302336] Epoch[51] Batch [5]#011Speed: 1071.55 samples/sec#011loss=0.479436\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] Epoch[51] Batch[10] avg_epoch_loss=0.510520\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.547821193933\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] Epoch[51] Batch [10]#011Speed: 329.09 samples/sec#011loss=0.547821\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2546.6060638427734, \"sum\": 2546.6060638427734, \"min\": 2546.6060638427734}}, \"EndTime\": 1591924779.728399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924777.181364}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=251.696202541 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.510519948873\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:39 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:41 INFO 140292449302336] Epoch[52] Batch[0] avg_epoch_loss=0.436722\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.436722040176\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:41 INFO 140292449302336] Epoch[52] Batch[5] avg_epoch_loss=0.480920\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.48092011114\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:41 INFO 140292449302336] Epoch[52] Batch [5]#011Speed: 1103.81 samples/sec#011loss=0.480920\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:42 INFO 140292449302336] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2482.7120304107666, \"sum\": 2482.7120304107666, \"min\": 2482.7120304107666}}, \"EndTime\": 1591924782.211605, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924779.728476}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:42 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.549381262 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:42 INFO 140292449302336] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:42 INFO 140292449302336] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.472001692653\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:42 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:43 INFO 140292449302336] Epoch[53] Batch[0] avg_epoch_loss=0.383544\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.383544117212\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:43 INFO 140292449302336] Epoch[53] Batch[5] avg_epoch_loss=0.472827\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.472826778889\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:43 INFO 140292449302336] Epoch[53] Batch [5]#011Speed: 1086.66 samples/sec#011loss=0.472827\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:44 INFO 140292449302336] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.830108642578, \"sum\": 2468.830108642578, \"min\": 2468.830108642578}}, \"EndTime\": 1591924784.681035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924782.211681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:44 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.548782347 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:44 INFO 140292449302336] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:44 INFO 140292449302336] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.446128803492\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:44 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:45 INFO 140292449302336] Epoch[54] Batch[0] avg_epoch_loss=0.402608\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:45 INFO 140292449302336] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.402608126402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:46 INFO 140292449302336] Epoch[54] Batch[5] avg_epoch_loss=0.405400\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.40540000548\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:46 INFO 140292449302336] Epoch[54] Batch [5]#011Speed: 1064.47 samples/sec#011loss=0.405400\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] Epoch[54] Batch[10] avg_epoch_loss=0.451092\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=54, batch=10 train loss <loss>=0.505922913551\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] Epoch[54] Batch [10]#011Speed: 328.90 samples/sec#011loss=0.505923\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2564.1698837280273, \"sum\": 2564.1698837280273, \"min\": 2564.1698837280273}}, \"EndTime\": 1591924787.245756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924784.681119}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.261716186 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.451092236421\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:47 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:48 INFO 140292449302336] Epoch[55] Batch[0] avg_epoch_loss=0.347263\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.347262859344\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:48 INFO 140292449302336] Epoch[55] Batch[5] avg_epoch_loss=0.420865\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.420865217845\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:48 INFO 140292449302336] Epoch[55] Batch [5]#011Speed: 1070.33 samples/sec#011loss=0.420865\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] Epoch[55] Batch[10] avg_epoch_loss=0.499881\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.594699025154\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] Epoch[55] Batch [10]#011Speed: 334.10 samples/sec#011loss=0.594699\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2560.448169708252, \"sum\": 2560.448169708252, \"min\": 2560.448169708252}}, \"EndTime\": 1591924789.806737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924787.245835}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=251.114091775 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.499880584803\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:49 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:51 INFO 140292449302336] Epoch[56] Batch[0] avg_epoch_loss=0.451668\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.451668262482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:51 INFO 140292449302336] Epoch[56] Batch[5] avg_epoch_loss=0.464639\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.46463872989\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:51 INFO 140292449302336] Epoch[56] Batch [5]#011Speed: 1088.57 samples/sec#011loss=0.464639\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:52 INFO 140292449302336] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2440.1659965515137, \"sum\": 2440.1659965515137, \"min\": 2440.1659965515137}}, \"EndTime\": 1591924792.247443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924789.806812}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:52 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=256.528369629 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:52 INFO 140292449302336] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.452061647177\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:52 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:53 INFO 140292449302336] Epoch[57] Batch[0] avg_epoch_loss=0.493613\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.493612974882\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:53 INFO 140292449302336] Epoch[57] Batch[5] avg_epoch_loss=0.454327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.454326967398\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:53 INFO 140292449302336] Epoch[57] Batch [5]#011Speed: 1052.26 samples/sec#011loss=0.454327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] Epoch[57] Batch[10] avg_epoch_loss=0.361659\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.250457036495\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] Epoch[57] Batch [10]#011Speed: 336.57 samples/sec#011loss=0.250457\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2534.6388816833496, \"sum\": 2534.6388816833496, \"min\": 2534.6388816833496}}, \"EndTime\": 1591924794.782631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924792.247521}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=263.141803539 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.361658816988\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:54 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_8f9b07c6-5d9a-4aec-a411-c960e60abb11-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.857086181640625, \"sum\": 30.857086181640625, \"min\": 30.857086181640625}}, \"EndTime\": 1591924794.814029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924794.782711}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:56 INFO 140292449302336] Epoch[58] Batch[0] avg_epoch_loss=0.312328\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.31232765317\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:56 INFO 140292449302336] Epoch[58] Batch[5] avg_epoch_loss=0.406748\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.406747937202\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:56 INFO 140292449302336] Epoch[58] Batch [5]#011Speed: 1055.53 samples/sec#011loss=0.406748\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:57 INFO 140292449302336] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2481.7728996276855, \"sum\": 2481.7728996276855, \"min\": 2481.7728996276855}}, \"EndTime\": 1591924797.295919, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924794.814089}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:57 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.809120655 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:57 INFO 140292449302336] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.376043888927\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:57 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:58 INFO 140292449302336] Epoch[59] Batch[0] avg_epoch_loss=0.574546\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.574545741081\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:58 INFO 140292449302336] Epoch[59] Batch[5] avg_epoch_loss=0.376653\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.376652732491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:58 INFO 140292449302336] Epoch[59] Batch [5]#011Speed: 1078.54 samples/sec#011loss=0.376653\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:59 INFO 140292449302336] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2511.9011402130127, \"sum\": 2511.9011402130127, \"min\": 2511.9011402130127}}, \"EndTime\": 1591924799.808432, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924797.296004}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:59 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=241.239740506 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:59 INFO 140292449302336] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.363480718434\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:19:59 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:01 INFO 140292449302336] Epoch[60] Batch[0] avg_epoch_loss=0.326663\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.326662808657\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:01 INFO 140292449302336] Epoch[60] Batch[5] avg_epoch_loss=0.396054\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.396053828299\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:01 INFO 140292449302336] Epoch[60] Batch [5]#011Speed: 1032.41 samples/sec#011loss=0.396054\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2544.3549156188965, \"sum\": 2544.3549156188965, \"min\": 2544.3549156188965}}, \"EndTime\": 1591924802.353339, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924799.808516}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=242.486458903 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.341280792654\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:02 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_1fa791ea-5325-4290-bed9-c9d7c3efa2fc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.894020080566406, \"sum\": 27.894020080566406, \"min\": 27.894020080566406}}, \"EndTime\": 1591924802.381807, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924802.35342}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:03 INFO 140292449302336] Epoch[61] Batch[0] avg_epoch_loss=0.425002\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:03 INFO 140292449302336] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.425001531839\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:03 INFO 140292449302336] Epoch[61] Batch[5] avg_epoch_loss=0.403424\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:03 INFO 140292449302336] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.403424041967\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:03 INFO 140292449302336] Epoch[61] Batch [5]#011Speed: 1068.78 samples/sec#011loss=0.403424\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:04 INFO 140292449302336] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2513.350009918213, \"sum\": 2513.350009918213, \"min\": 2513.350009918213}}, \"EndTime\": 1591924804.895292, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924802.38188}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:04 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.057987086 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:04 INFO 140292449302336] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.409673042595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:04 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:06 INFO 140292449302336] Epoch[62] Batch[0] avg_epoch_loss=0.410325\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.410324782133\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:06 INFO 140292449302336] Epoch[62] Batch[5] avg_epoch_loss=0.322484\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.32248355945\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:06 INFO 140292449302336] Epoch[62] Batch [5]#011Speed: 1082.74 samples/sec#011loss=0.322484\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2541.797161102295, \"sum\": 2541.797161102295, \"min\": 2541.797161102295}}, \"EndTime\": 1591924807.437667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924804.895375}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=236.43563274 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.313180568814\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:07 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_0a2f0967-db0e-47bb-855f-e91b157a266b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.64492416381836, \"sum\": 23.64492416381836, \"min\": 23.64492416381836}}, \"EndTime\": 1591924807.461965, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924807.437751}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:08 INFO 140292449302336] Epoch[63] Batch[0] avg_epoch_loss=0.294722\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:08 INFO 140292449302336] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.294721871614\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:09 INFO 140292449302336] Epoch[63] Batch[5] avg_epoch_loss=0.367623\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:09 INFO 140292449302336] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.367622586588\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:09 INFO 140292449302336] Epoch[63] Batch [5]#011Speed: 1095.72 samples/sec#011loss=0.367623\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] Epoch[63] Batch[10] avg_epoch_loss=0.360642\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.352264800668\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] Epoch[63] Batch [10]#011Speed: 331.58 samples/sec#011loss=0.352265\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2557.4679374694824, \"sum\": 2557.4679374694824, \"min\": 2557.4679374694824}}, \"EndTime\": 1591924810.019557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924807.462034}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=260.793406858 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.360641774806\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:10 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:11 INFO 140292449302336] Epoch[64] Batch[0] avg_epoch_loss=0.423034\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.423034340143\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:11 INFO 140292449302336] Epoch[64] Batch[5] avg_epoch_loss=0.381833\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.381832669179\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:11 INFO 140292449302336] Epoch[64] Batch [5]#011Speed: 1089.14 samples/sec#011loss=0.381833\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:12 INFO 140292449302336] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2435.811996459961, \"sum\": 2435.811996459961, \"min\": 2435.811996459961}}, \"EndTime\": 1591924812.455928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924810.019638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:12 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=262.734074046 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:12 INFO 140292449302336] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:12 INFO 140292449302336] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.351177845895\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:12 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:13 INFO 140292449302336] Epoch[65] Batch[0] avg_epoch_loss=0.347606\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:13 INFO 140292449302336] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.347605913877\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] Epoch[65] Batch[5] avg_epoch_loss=0.349316\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.349315548937\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] Epoch[65] Batch [5]#011Speed: 1081.27 samples/sec#011loss=0.349316\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2460.146903991699, \"sum\": 2460.146903991699, \"min\": 2460.146903991699}}, \"EndTime\": 1591924814.916643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924812.456005}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=255.663102925 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.344782933593\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:14 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:16 INFO 140292449302336] Epoch[66] Batch[0] avg_epoch_loss=0.498540\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.498540312052\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:16 INFO 140292449302336] Epoch[66] Batch[5] avg_epoch_loss=0.308917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.308917234341\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:16 INFO 140292449302336] Epoch[66] Batch [5]#011Speed: 1087.62 samples/sec#011loss=0.308917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.8639640808105, \"sum\": 2468.8639640808105, \"min\": 2468.8639640808105}}, \"EndTime\": 1591924817.386091, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924814.916725}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=247.875537786 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.270937729254\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:17 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_4d05cdf3-2849-4295-bd67-34e87b59bf2d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.289897918701172, \"sum\": 20.289897918701172, \"min\": 20.289897918701172}}, \"EndTime\": 1591924817.406997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924817.386173}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:18 INFO 140292449302336] Epoch[67] Batch[0] avg_epoch_loss=0.273264\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.273264318705\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:18 INFO 140292449302336] Epoch[67] Batch[5] avg_epoch_loss=0.265235\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.26523458461\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:18 INFO 140292449302336] Epoch[67] Batch [5]#011Speed: 1086.15 samples/sec#011loss=0.265235\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:19 INFO 140292449302336] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2438.2050037384033, \"sum\": 2438.2050037384033, \"min\": 2438.2050037384033}}, \"EndTime\": 1591924819.845318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924817.407059}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:19 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=261.245660453 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:19 INFO 140292449302336] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.306535103917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:19 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:21 INFO 140292449302336] Epoch[68] Batch[0] avg_epoch_loss=0.290519\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.290519237518\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] Epoch[68] Batch[5] avg_epoch_loss=0.323096\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.323096228143\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] Epoch[68] Batch [5]#011Speed: 323.48 samples/sec#011loss=0.323096\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] processed a total of 574 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2423.5119819641113, \"sum\": 2423.5119819641113, \"min\": 2423.5119819641113}}, \"EndTime\": 1591924822.269389, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924819.845397}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=236.834527607 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.323492666086\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:22 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:23 INFO 140292449302336] Epoch[69] Batch[0] avg_epoch_loss=0.428527\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.42852678895\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:23 INFO 140292449302336] Epoch[69] Batch[5] avg_epoch_loss=0.380539\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.380538716912\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:23 INFO 140292449302336] Epoch[69] Batch [5]#011Speed: 1076.45 samples/sec#011loss=0.380539\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:24 INFO 140292449302336] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2461.9081020355225, \"sum\": 2461.9081020355225, \"min\": 2461.9081020355225}}, \"EndTime\": 1591924824.731832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924822.269472}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:24 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.108128091 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:24 INFO 140292449302336] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:24 INFO 140292449302336] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.383202204108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:24 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:25 INFO 140292449302336] Epoch[70] Batch[0] avg_epoch_loss=0.465565\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:25 INFO 140292449302336] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.465565443039\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:26 INFO 140292449302336] Epoch[70] Batch[5] avg_epoch_loss=0.352766\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.352766404549\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:26 INFO 140292449302336] Epoch[70] Batch [5]#011Speed: 1066.82 samples/sec#011loss=0.352766\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:27 INFO 140292449302336] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2470.116138458252, \"sum\": 2470.116138458252, \"min\": 2470.116138458252}}, \"EndTime\": 1591924827.202418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924824.731893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:27 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=253.015040238 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:27 INFO 140292449302336] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:27 INFO 140292449302336] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.31913638562\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:27 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:28 INFO 140292449302336] Epoch[71] Batch[0] avg_epoch_loss=0.313596\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.313595563173\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:28 INFO 140292449302336] Epoch[71] Batch[5] avg_epoch_loss=0.324422\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.324421641727\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:28 INFO 140292449302336] Epoch[71] Batch [5]#011Speed: 1093.35 samples/sec#011loss=0.324422\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] Epoch[71] Batch[10] avg_epoch_loss=0.328673\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.333775123954\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] Epoch[71] Batch [10]#011Speed: 323.76 samples/sec#011loss=0.333775\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2562.5150203704834, \"sum\": 2562.5150203704834, \"min\": 2562.5150203704834}}, \"EndTime\": 1591924829.765472, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924827.202479}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=256.76794553 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.328673224558\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:29 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:31 INFO 140292449302336] Epoch[72] Batch[0] avg_epoch_loss=0.193299\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.193299338222\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:31 INFO 140292449302336] Epoch[72] Batch[5] avg_epoch_loss=0.348196\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.348196290433\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:31 INFO 140292449302336] Epoch[72] Batch [5]#011Speed: 1018.54 samples/sec#011loss=0.348196\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] Epoch[72] Batch[10] avg_epoch_loss=0.182878\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=-0.0155045360327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] Epoch[72] Batch [10]#011Speed: 333.03 samples/sec#011loss=-0.015505\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2539.605140686035, \"sum\": 2539.605140686035, \"min\": 2539.605140686035}}, \"EndTime\": 1591924832.305562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924829.765536}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=257.509010082 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.182877732949\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:32 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_3e315ebc-e072-421a-a0cc-537a22a9f282-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.238876342773438, \"sum\": 20.238876342773438, \"min\": 20.238876342773438}}, \"EndTime\": 1591924832.326374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924832.305634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:33 INFO 140292449302336] Epoch[73] Batch[0] avg_epoch_loss=0.276924\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.276923507452\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:33 INFO 140292449302336] Epoch[73] Batch[5] avg_epoch_loss=0.394647\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.394646664461\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:33 INFO 140292449302336] Epoch[73] Batch [5]#011Speed: 1082.61 samples/sec#011loss=0.394647\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:34 INFO 140292449302336] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.139108657837, \"sum\": 2459.139108657837, \"min\": 2459.139108657837}}, \"EndTime\": 1591924834.785621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924832.32643}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:34 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=239.505472102 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:34 INFO 140292449302336] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.40796841979\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:34 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:36 INFO 140292449302336] Epoch[74] Batch[0] avg_epoch_loss=0.242585\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.242584526539\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:36 INFO 140292449302336] Epoch[74] Batch[5] avg_epoch_loss=0.357473\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.357472866774\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:36 INFO 140292449302336] Epoch[74] Batch [5]#011Speed: 1019.82 samples/sec#011loss=0.357473\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:37 INFO 140292449302336] processed a total of 581 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2472.440004348755, \"sum\": 2472.440004348755, \"min\": 2472.440004348755}}, \"EndTime\": 1591924837.258596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924834.785683}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:37 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=234.978186064 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:37 INFO 140292449302336] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.358910445869\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:37 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:38 INFO 140292449302336] Epoch[75] Batch[0] avg_epoch_loss=0.127103\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.127103418112\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:38 INFO 140292449302336] Epoch[75] Batch[5] avg_epoch_loss=0.316448\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.316448047757\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:38 INFO 140292449302336] Epoch[75] Batch [5]#011Speed: 1084.38 samples/sec#011loss=0.316448\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] Epoch[75] Batch[10] avg_epoch_loss=0.369625\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.43343680799\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] Epoch[75] Batch [10]#011Speed: 335.78 samples/sec#011loss=0.433437\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2541.0540103912354, \"sum\": 2541.0540103912354, \"min\": 2541.0540103912354}}, \"EndTime\": 1591924839.800205, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924837.258687}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.607428282 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.369624756954\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:39 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:41 INFO 140292449302336] Epoch[76] Batch[0] avg_epoch_loss=0.448019\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.448018878698\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:41 INFO 140292449302336] Epoch[76] Batch[5] avg_epoch_loss=0.360931\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:41 INFO 140292449302336] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.360931128263\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:41 INFO 140292449302336] Epoch[76] Batch [5]#011Speed: 1068.28 samples/sec#011loss=0.360931\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] Epoch[76] Batch[10] avg_epoch_loss=0.401493\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=0.450166851282\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] Epoch[76] Batch [10]#011Speed: 335.38 samples/sec#011loss=0.450167\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2527.049779891968, \"sum\": 2527.049779891968, \"min\": 2527.049779891968}}, \"EndTime\": 1591924842.327823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924839.800283}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.434688299 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.401492820545\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:42 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:43 INFO 140292449302336] Epoch[77] Batch[0] avg_epoch_loss=0.326066\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.32606613636\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:43 INFO 140292449302336] Epoch[77] Batch[5] avg_epoch_loss=0.358759\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:43 INFO 140292449302336] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.358758700391\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:43 INFO 140292449302336] Epoch[77] Batch [5]#011Speed: 1077.74 samples/sec#011loss=0.358759\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:44 INFO 140292449302336] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.1620693206787, \"sum\": 2458.1620693206787, \"min\": 2458.1620693206787}}, \"EndTime\": 1591924844.786507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924842.327907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:44 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.719475527 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:44 INFO 140292449302336] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:44 INFO 140292449302336] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.365862624347\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:44 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:46 INFO 140292449302336] Epoch[78] Batch[0] avg_epoch_loss=0.476734\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.476733654737\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:46 INFO 140292449302336] Epoch[78] Batch[5] avg_epoch_loss=0.365683\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:46 INFO 140292449302336] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.365683337053\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:46 INFO 140292449302336] Epoch[78] Batch [5]#011Speed: 1039.74 samples/sec#011loss=0.365683\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] Epoch[78] Batch[10] avg_epoch_loss=0.294655\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.209421429038\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] Epoch[78] Batch [10]#011Speed: 338.90 samples/sec#011loss=0.209421\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2536.7369651794434, \"sum\": 2536.7369651794434, \"min\": 2536.7369651794434}}, \"EndTime\": 1591924847.323808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924844.786574}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=254.645292349 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.294655197046\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:47 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:48 INFO 140292449302336] Epoch[79] Batch[0] avg_epoch_loss=0.447808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.44780755043\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:48 INFO 140292449302336] Epoch[79] Batch[5] avg_epoch_loss=0.321240\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:48 INFO 140292449302336] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.321239843965\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:48 INFO 140292449302336] Epoch[79] Batch [5]#011Speed: 1074.29 samples/sec#011loss=0.321240\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] Epoch[79] Batch[10] avg_epoch_loss=0.345397\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=0.374385386705\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] Epoch[79] Batch [10]#011Speed: 328.42 samples/sec#011loss=0.374385\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2550.0900745391846, \"sum\": 2550.0900745391846, \"min\": 2550.0900745391846}}, \"EndTime\": 1591924849.874458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924847.323894}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=264.292479246 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.345396908847\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:49 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:51 INFO 140292449302336] Epoch[80] Batch[0] avg_epoch_loss=0.105689\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.10568882525\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:51 INFO 140292449302336] Epoch[80] Batch[5] avg_epoch_loss=0.268780\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:51 INFO 140292449302336] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.268780387938\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:51 INFO 140292449302336] Epoch[80] Batch [5]#011Speed: 1089.53 samples/sec#011loss=0.268780\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] Epoch[80] Batch[10] avg_epoch_loss=0.253180\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.234458464384\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] Epoch[80] Batch [10]#011Speed: 321.48 samples/sec#011loss=0.234458\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2592.924118041992, \"sum\": 2592.924118041992, \"min\": 2592.924118041992}}, \"EndTime\": 1591924852.467885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924849.874537}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=258.383887756 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.253179513595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:52 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:53 INFO 140292449302336] Epoch[81] Batch[0] avg_epoch_loss=0.135228\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:53 INFO 140292449302336] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.135228440166\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] Epoch[81] Batch[5] avg_epoch_loss=0.210980\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.210979697605\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] Epoch[81] Batch [5]#011Speed: 1105.32 samples/sec#011loss=0.210980\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2482.9471111297607, \"sum\": 2482.9471111297607, \"min\": 2482.9471111297607}}, \"EndTime\": 1591924854.95136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924852.467964}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=248.080008065 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.223501449078\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:54 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:56 INFO 140292449302336] Epoch[82] Batch[0] avg_epoch_loss=0.225879\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.225878641009\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:56 INFO 140292449302336] Epoch[82] Batch[5] avg_epoch_loss=0.306486\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:56 INFO 140292449302336] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.306485662858\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:56 INFO 140292449302336] Epoch[82] Batch [5]#011Speed: 1085.51 samples/sec#011loss=0.306486\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] Epoch[82] Batch[10] avg_epoch_loss=0.321320\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.339120185375\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] Epoch[82] Batch [10]#011Speed: 339.38 samples/sec#011loss=0.339120\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2532.611131668091, \"sum\": 2532.611131668091, \"min\": 2532.611131668091}}, \"EndTime\": 1591924857.484524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924854.951444}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=261.773795392 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.321319536729\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:57 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:58 INFO 140292449302336] Epoch[83] Batch[0] avg_epoch_loss=0.491398\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:58 INFO 140292449302336] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.491398245096\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] Epoch[83] Batch[5] avg_epoch_loss=0.329017\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.329016613464\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] Epoch[83] Batch [5]#011Speed: 1091.66 samples/sec#011loss=0.329017\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2475.5699634552, \"sum\": 2475.5699634552, \"min\": 2475.5699634552}}, \"EndTime\": 1591924859.960616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924857.484595}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=255.686083473 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.264715834893\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:20:59 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:01 INFO 140292449302336] Epoch[84] Batch[0] avg_epoch_loss=0.424093\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.424093395472\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:01 INFO 140292449302336] Epoch[84] Batch[5] avg_epoch_loss=0.374004\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:01 INFO 140292449302336] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.374003519615\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:01 INFO 140292449302336] Epoch[84] Batch [5]#011Speed: 993.06 samples/sec#011loss=0.374004\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:02 INFO 140292449302336] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2499.7270107269287, \"sum\": 2499.7270107269287, \"min\": 2499.7270107269287}}, \"EndTime\": 1591924862.460924, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924859.960698}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:02 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=245.21461593 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:02 INFO 140292449302336] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:02 INFO 140292449302336] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.400180569291\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:02 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:03 INFO 140292449302336] Epoch[85] Batch[0] avg_epoch_loss=0.255276\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:03 INFO 140292449302336] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.255276054144\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:04 INFO 140292449302336] Epoch[85] Batch[5] avg_epoch_loss=0.286258\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:04 INFO 140292449302336] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.286257813374\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:04 INFO 140292449302336] Epoch[85] Batch [5]#011Speed: 1093.23 samples/sec#011loss=0.286258\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] Epoch[85] Batch[10] avg_epoch_loss=0.374464\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.48031193018\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] Epoch[85] Batch [10]#011Speed: 323.97 samples/sec#011loss=0.480312\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2578.5369873046875, \"sum\": 2578.5369873046875, \"min\": 2578.5369873046875}}, \"EndTime\": 1591924865.040014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924862.461007}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=248.577752802 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.374464230104\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:05 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:06 INFO 140292449302336] Epoch[86] Batch[0] avg_epoch_loss=0.425643\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.425643116236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:06 INFO 140292449302336] Epoch[86] Batch[5] avg_epoch_loss=0.261358\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:06 INFO 140292449302336] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.261357893546\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:06 INFO 140292449302336] Epoch[86] Batch [5]#011Speed: 1072.05 samples/sec#011loss=0.261358\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:07 INFO 140292449302336] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.8770065307617, \"sum\": 2492.8770065307617, \"min\": 2492.8770065307617}}, \"EndTime\": 1591924867.533423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924865.04011}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:07 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=249.097469875 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:07 INFO 140292449302336] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:07 INFO 140292449302336] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.25509685576\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:07 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:08 INFO 140292449302336] Epoch[87] Batch[0] avg_epoch_loss=0.378273\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:08 INFO 140292449302336] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.378272771835\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:09 INFO 140292449302336] Epoch[87] Batch[5] avg_epoch_loss=0.282121\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:09 INFO 140292449302336] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.282120740662\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:09 INFO 140292449302336] Epoch[87] Batch [5]#011Speed: 1061.65 samples/sec#011loss=0.282121\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:10 INFO 140292449302336] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2541.222095489502, \"sum\": 2541.222095489502, \"min\": 2541.222095489502}}, \"EndTime\": 1591924870.075178, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924867.533508}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:10 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=250.657130374 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:10 INFO 140292449302336] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:10 INFO 140292449302336] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.269011240453\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:10 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:11 INFO 140292449302336] Epoch[88] Batch[0] avg_epoch_loss=0.369104\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.369103997946\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:11 INFO 140292449302336] Epoch[88] Batch[5] avg_epoch_loss=0.272726\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:11 INFO 140292449302336] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.272725909327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:11 INFO 140292449302336] Epoch[88] Batch [5]#011Speed: 1095.69 samples/sec#011loss=0.272726\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:12 INFO 140292449302336] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2439.465045928955, \"sum\": 2439.465045928955, \"min\": 2439.465045928955}}, \"EndTime\": 1591924872.515189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924870.075241}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:12 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=261.111467086 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:12 INFO 140292449302336] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:12 INFO 140292449302336] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.276842404529\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:12 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:13 INFO 140292449302336] Epoch[89] Batch[0] avg_epoch_loss=0.349044\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:13 INFO 140292449302336] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.349043756723\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:14 INFO 140292449302336] Epoch[89] Batch[5] avg_epoch_loss=0.282730\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:14 INFO 140292449302336] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.282729854186\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:14 INFO 140292449302336] Epoch[89] Batch [5]#011Speed: 1030.21 samples/sec#011loss=0.282730\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] Epoch[89] Batch[10] avg_epoch_loss=0.306254\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.334482261539\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] Epoch[89] Batch [10]#011Speed: 335.36 samples/sec#011loss=0.334482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] processed a total of 694 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2544.9399948120117, \"sum\": 2544.9399948120117, \"min\": 2544.9399948120117}}, \"EndTime\": 1591924875.060642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924872.515263}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=272.686179321 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.30625367571\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:15 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:16 INFO 140292449302336] Epoch[90] Batch[0] avg_epoch_loss=0.204060\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.204059973359\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:16 INFO 140292449302336] Epoch[90] Batch[5] avg_epoch_loss=0.310011\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:16 INFO 140292449302336] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.310010783374\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:16 INFO 140292449302336] Epoch[90] Batch [5]#011Speed: 1093.29 samples/sec#011loss=0.310011\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:17 INFO 140292449302336] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2448.7199783325195, \"sum\": 2448.7199783325195, \"min\": 2448.7199783325195}}, \"EndTime\": 1591924877.509909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924875.060713}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:17 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=256.858818436 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:17 INFO 140292449302336] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:17 INFO 140292449302336] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.320254980028\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:17 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:18 INFO 140292449302336] Epoch[91] Batch[0] avg_epoch_loss=0.121100\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:18 INFO 140292449302336] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.121100150049\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:19 INFO 140292449302336] Epoch[91] Batch[5] avg_epoch_loss=0.184201\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:19 INFO 140292449302336] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.184200568125\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:19 INFO 140292449302336] Epoch[91] Batch [5]#011Speed: 1105.10 samples/sec#011loss=0.184201\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] Epoch[91] Batch[10] avg_epoch_loss=0.203158\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.225906331092\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] Epoch[91] Batch [10]#011Speed: 332.44 samples/sec#011loss=0.225906\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2532.5088500976562, \"sum\": 2532.5088500976562, \"min\": 2532.5088500976562}}, \"EndTime\": 1591924880.042966, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924877.509972}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=264.153307403 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.20315773311\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:20 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:21 INFO 140292449302336] Epoch[92] Batch[0] avg_epoch_loss=0.272553\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:21 INFO 140292449302336] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.272552609444\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] Epoch[92] Batch[5] avg_epoch_loss=0.251051\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.251051177581\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] Epoch[92] Batch [5]#011Speed: 336.99 samples/sec#011loss=0.251051\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2459.7809314727783, \"sum\": 2459.7809314727783, \"min\": 2459.7809314727783}}, \"EndTime\": 1591924882.503278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924880.043038}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=247.977545873 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.228296019882\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:22 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:23 INFO 140292449302336] Epoch[93] Batch[0] avg_epoch_loss=0.244982\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:23 INFO 140292449302336] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.244982257485\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:24 INFO 140292449302336] Epoch[93] Batch[5] avg_epoch_loss=0.270376\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:24 INFO 140292449302336] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.270376148323\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:24 INFO 140292449302336] Epoch[93] Batch [5]#011Speed: 338.98 samples/sec#011loss=0.270376\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:25 INFO 140292449302336] processed a total of 588 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2550.5831241607666, \"sum\": 2550.5831241607666, \"min\": 2550.5831241607666}}, \"EndTime\": 1591924885.054398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924882.503359}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:25 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=230.524333641 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:25 INFO 140292449302336] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:25 INFO 140292449302336] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.292005458474\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:25 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:26 INFO 140292449302336] Epoch[94] Batch[0] avg_epoch_loss=0.307239\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.307238817215\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:26 INFO 140292449302336] Epoch[94] Batch[5] avg_epoch_loss=0.288569\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:26 INFO 140292449302336] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.288569112619\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:26 INFO 140292449302336] Epoch[94] Batch [5]#011Speed: 1103.13 samples/sec#011loss=0.288569\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] Epoch[94] Batch[10] avg_epoch_loss=0.264210\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=0.234978784621\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] Epoch[94] Batch [10]#011Speed: 336.95 samples/sec#011loss=0.234979\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2527.8480052948, \"sum\": 2527.8480052948, \"min\": 2527.8480052948}}, \"EndTime\": 1591924887.582799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924885.054483}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=271.760212369 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.26420987262\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:27 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:28 INFO 140292449302336] Epoch[95] Batch[0] avg_epoch_loss=0.291060\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:28 INFO 140292449302336] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.291060209274\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:29 INFO 140292449302336] Epoch[95] Batch[5] avg_epoch_loss=0.231366\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:29 INFO 140292449302336] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.231366273016\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:29 INFO 140292449302336] Epoch[95] Batch [5]#011Speed: 1057.77 samples/sec#011loss=0.231366\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] Epoch[95] Batch[10] avg_epoch_loss=0.209340\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=0.182908175886\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] Epoch[95] Batch [10]#011Speed: 324.58 samples/sec#011loss=0.182908\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2564.90421295166, \"sum\": 2564.90421295166, \"min\": 2564.90421295166}}, \"EndTime\": 1591924890.148213, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924887.582876}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=267.444299628 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.209339865229\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:30 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:31 INFO 140292449302336] Epoch[96] Batch[0] avg_epoch_loss=0.339382\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.339382082224\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:31 INFO 140292449302336] Epoch[96] Batch[5] avg_epoch_loss=0.278155\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:31 INFO 140292449302336] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.278154954314\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:31 INFO 140292449302336] Epoch[96] Batch [5]#011Speed: 1095.48 samples/sec#011loss=0.278155\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:32 INFO 140292449302336] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2488.077163696289, \"sum\": 2488.077163696289, \"min\": 2488.077163696289}}, \"EndTime\": 1591924892.636789, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924890.148292}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:32 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=247.568314981 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:32 INFO 140292449302336] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:32 INFO 140292449302336] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.246469247341\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:32 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:33 INFO 140292449302336] Epoch[97] Batch[0] avg_epoch_loss=0.440286\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:33 INFO 140292449302336] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.440286159515\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:34 INFO 140292449302336] Epoch[97] Batch[5] avg_epoch_loss=0.337482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:34 INFO 140292449302336] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.337481568257\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:34 INFO 140292449302336] Epoch[97] Batch [5]#011Speed: 1088.99 samples/sec#011loss=0.337482\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] Epoch[97] Batch[10] avg_epoch_loss=0.257585\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=0.161709518731\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] Epoch[97] Batch [10]#011Speed: 334.30 samples/sec#011loss=0.161710\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2565.2599334716797, \"sum\": 2565.2599334716797, \"min\": 2565.2599334716797}}, \"EndTime\": 1591924895.2026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924892.636876}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=252.98425953 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.257585182109\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:35 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:36 INFO 140292449302336] Epoch[98] Batch[0] avg_epoch_loss=0.332874\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.332873553038\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:36 INFO 140292449302336] Epoch[98] Batch[5] avg_epoch_loss=0.292239\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:36 INFO 140292449302336] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.292239400248\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:36 INFO 140292449302336] Epoch[98] Batch [5]#011Speed: 1092.71 samples/sec#011loss=0.292239\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:37 INFO 140292449302336] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2481.0709953308105, \"sum\": 2481.0709953308105, \"min\": 2481.0709953308105}}, \"EndTime\": 1591924897.684163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924895.202681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:37 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=245.043688657 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:37 INFO 140292449302336] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:37 INFO 140292449302336] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.305733127892\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:37 INFO 140292449302336] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:38 INFO 140292449302336] Epoch[99] Batch[0] avg_epoch_loss=0.427930\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:38 INFO 140292449302336] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.427930146456\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:39 INFO 140292449302336] Epoch[99] Batch[5] avg_epoch_loss=0.239368\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:39 INFO 140292449302336] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.239367770652\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:39 INFO 140292449302336] Epoch[99] Batch [5]#011Speed: 327.53 samples/sec#011loss=0.239368\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2508.3601474761963, \"sum\": 2508.3601474761963, \"min\": 2508.3601474761963}}, \"EndTime\": 1591924900.193037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924897.684244}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] #throughput_metric: host=algo-1, train throughput=245.16819537 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.160360588133\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/state_19eef0b4-e5cd-4a35-8972-a5b331e75ffa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.098915100097656, \"sum\": 30.098915100097656, \"min\": 30.098915100097656}}, \"EndTime\": 1591924900.223724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.193121}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Final loss: 0.160360588133 (occurred at epoch 99)\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] #quality_metric: host=algo-1, train final_loss <loss>=0.160360588133\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 WARNING 140292449302336] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 253.7519931793213, \"sum\": 253.7519931793213, \"min\": 253.7519931793213}}, \"EndTime\": 1591924900.478118, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.223805}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 307.6357841491699, \"sum\": 307.6357841491699, \"min\": 307.6357841491699}}, \"EndTime\": 1591924900.531961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.478172}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 8.369922637939453, \"sum\": 8.369922637939453, \"min\": 8.369922637939453}}, \"EndTime\": 1591924900.540435, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.532029}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:40 INFO 140292449302336] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.025033950805664062, \"sum\": 0.025033950805664062, \"min\": 0.025033950805664062}}, \"EndTime\": 1591924900.541051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.540479}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:44 INFO 140292449302336] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:47 INFO 140292449302336] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:51 INFO 140292449302336] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:55 INFO 140292449302336] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:21:58 INFO 140292449302336] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:02 INFO 140292449302336] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:06 INFO 140292449302336] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 28003.785133361816, \"sum\": 28003.785133361816, \"min\": 28003.785133361816}}, \"EndTime\": 1591924928.544814, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924900.541092}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, RMSE): 0.720601993766\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, mean_absolute_QuantileLoss): 1719.8826215610595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, mean_wQuantileLoss): 0.45499540252938075\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.1]): 0.5620253604310984\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.2]): 0.5445894795031874\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.3]): 0.5184471452520131\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.4]): 0.48837462467989007\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.5]): 0.4571150920458696\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.6]): 0.42750314773914094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.7]): 0.39878602883132985\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.8]): 0.3676335869888681\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #test_score (algo-1, wQuantileLoss[0.9]): 0.3304841572930297\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.454995402529\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:22:08 INFO 140292449302336] #quality_metric: host=algo-1, test RMSE <loss>=0.720601993766\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 295013.60392570496, \"sum\": 295013.60392570496, \"min\": 295013.60392570496}, \"setuptime\": {\"count\": 1, \"max\": 8.87298583984375, \"sum\": 8.87298583984375, \"min\": 8.87298583984375}}, \"EndTime\": 1591924928.561948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591924928.544881}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-12 01:22:45 Uploading - Uploading generated training model\n",
      "2020-06-12 01:22:45 Completed - Training job completed\n",
      "Training seconds: 396\n",
      "Billable seconds: 396\n",
      "CPU times: user 1.27 s, sys: 82.6 ms, total: 1.35 s\n",
      "Wall time: 8min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator_indicator.fit(inputs=data_channels, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor_indicator = estimator_indicator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_predictor = sagemaker.predictor.RealTimePredictor(endpoint='deep-ar-indicator-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>0.07433</td>\n",
       "      <td>0.285545</td>\n",
       "      <td>0.364432</td>\n",
       "      <td>0.436737</td>\n",
       "      <td>0.372038</td>\n",
       "      <td>0.292267</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume      PC1       PC2       PC3       PC4  \\\n",
       "Date       Ticker                                                               \n",
       "2019-01-02 AAPL      0.03987  0.043087  0.07433  0.285545  0.364432  0.436737   \n",
       "\n",
       "                        PC5       PC6  target  prediction  \n",
       "Date       Ticker                                          \n",
       "2019-01-02 AAPL    0.372038  0.292267      -1          -1  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "date = '2019-01-02'\n",
    "ticker = 'AAPL'\n",
    "df = stock_indicator_data\n",
    "predictor = indicator_predictor\n",
    "def get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat):\n",
    "\n",
    "    date_pred = pd.Timestamp(date, freq='D')\n",
    "    date_start = date_pred-timedelta(days=50)\n",
    "    pred_df = stock_indicator_data.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "    result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "    pred = {\n",
    "            \"start\": str(date_pred),\n",
    "            \"target\": pred_df['target'][date_start:date_pred-timedelta(days=1)].tolist(),\n",
    "            \"dynamic_feat\": pred_df[dynamic_feat][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "    req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "    res = indicator_predictor.predict(req)\n",
    "    prediction_data = json.loads(res.decode('utf-8'))\n",
    "    pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "    result_df['prediction'] = pred\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_dynamic_feat_accuracy(ticker):\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "    df = stock_indicator_data\n",
    "    for date in date_index:\n",
    "        target.append(get_dynamic_feat_prediction(ticker, date,df,indicator_predictor,dynamic_feat)['target'].values[0])\n",
    "        prediction.append(int(get_dynamic_feat_prediction(ticker, date,df,indicator_predictor,dynamic_feat)['prediction'].values[0]))\n",
    "\n",
    "    return accuracy_score(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.746031746031746"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dynamic_feat_accuracy(ticker='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HWM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-0495d7dae550>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mticker\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtickers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dynamic_feat_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-b54269317327>\u001b[0m in \u001b[0;36mget_dynamic_feat_accuracy\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstock_indicator_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dynamic_feat_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicator_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdynamic_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dynamic_feat_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicator_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdynamic_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-8d70d8f27a0e>\u001b[0m in \u001b[0;36mget_dynamic_feat_prediction\u001b[0;34m(ticker, date, df, predictor, dynamic_feat)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdate_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     pred = {\n\u001b[1;32m     13\u001b[0m             \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0mcurrent_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m                 \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_locs\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m   3060\u001b[0m                 indexer = _update_indexer(\n\u001b[1;32m   3061\u001b[0m                     _convert_to_indexer(\n\u001b[0;32m-> 3062\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3063\u001b[0m                     ),\n\u001b[1;32m   3064\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   2844\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   2934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m                     \u001b[0;31m# The label is present in self.levels[level] but unused:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2936\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2937\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HWM'"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    acc[ticker] = get_dynamic_feat_accuracy(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6612781323480014"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(acc.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.6730769230769231,\n",
       " 'AAL': 0.6346153846153846,\n",
       " 'AAP': 0.7403846153846154,\n",
       " 'AAPL': 0.6826923076923077,\n",
       " 'ABBV': 0.7307692307692307,\n",
       " 'ABC': 0.6153846153846154,\n",
       " 'ABMD': 0.7307692307692307,\n",
       " 'ABT': 0.7019230769230769,\n",
       " 'ACN': 0.5769230769230769,\n",
       " 'ADBE': 0.5961538461538461,\n",
       " 'ADI': 0.5673076923076923,\n",
       " 'ADM': 0.6442307692307693,\n",
       " 'ADP': 0.7019230769230769,\n",
       " 'ADS': 0.6634615384615384,\n",
       " 'ADSK': 0.6346153846153846,\n",
       " 'AEE': 0.6826923076923077,\n",
       " 'AEP': 0.6826923076923077,\n",
       " 'AES': 0.6346153846153846,\n",
       " 'AFL': 0.6730769230769231,\n",
       " 'AIG': 0.6826923076923077,\n",
       " 'AIV': 0.6346153846153846,\n",
       " 'AIZ': 0.6923076923076923,\n",
       " 'AJG': 0.6923076923076923,\n",
       " 'AKAM': 0.7307692307692307,\n",
       " 'ALB': 0.7211538461538461,\n",
       " 'ALGN': 0.625,\n",
       " 'ALK': 0.6538461538461539,\n",
       " 'ALL': 0.7211538461538461,\n",
       " 'ALLE': 0.6826923076923077,\n",
       " 'ALXN': 0.7403846153846154,\n",
       " 'AMAT': 0.4807692307692308,\n",
       " 'AMCR': 0.7019230769230769,\n",
       " 'AMD': 0.6538461538461539,\n",
       " 'AME': 0.6730769230769231,\n",
       " 'AMGN': 0.7019230769230769,\n",
       " 'AMP': 0.6153846153846154,\n",
       " 'AMT': 0.6634615384615384,\n",
       " 'AMZN': 0.33653846153846156,\n",
       " 'ANET': 0.6730769230769231,\n",
       " 'ANSS': 0.625,\n",
       " 'ANTM': 0.625,\n",
       " 'AON': 0.6923076923076923,\n",
       " 'AOS': 0.6057692307692307,\n",
       " 'APA': 0.6538461538461539,\n",
       " 'APD': 0.625,\n",
       " 'APH': 0.75,\n",
       " 'APTV': 0.6634615384615384,\n",
       " 'ARE': 0.7211538461538461,\n",
       " 'ATO': 0.7403846153846154,\n",
       " 'ATVI': 0.6730769230769231,\n",
       " 'AVB': 0.6442307692307693,\n",
       " 'AVGO': 0.5576923076923077,\n",
       " 'AVY': 0.6923076923076923,\n",
       " 'AWK': 0.7019230769230769,\n",
       " 'AXP': 0.6826923076923077,\n",
       " 'AZO': 0.40384615384615385,\n",
       " 'BA': 0.5961538461538461,\n",
       " 'BAC': 0.7019230769230769,\n",
       " 'BAX': 0.75,\n",
       " 'BBY': 0.6538461538461539,\n",
       " 'BDX': 0.6538461538461539,\n",
       " 'BEN': 0.6538461538461539,\n",
       " 'BIIB': 0.6826923076923077,\n",
       " 'BK': 0.7403846153846154,\n",
       " 'BKNG': 0.4423076923076923,\n",
       " 'BKR': 0.7307692307692307,\n",
       " 'BLK': 0.5576923076923077,\n",
       " 'BLL': 0.7788461538461539,\n",
       " 'BMY': 0.75,\n",
       " 'BR': 0.6442307692307693,\n",
       " 'BSX': 0.5865384615384616,\n",
       " 'BWA': 0.75,\n",
       " 'BXP': 0.6538461538461539,\n",
       " 'C': 0.6057692307692307,\n",
       " 'CAG': 0.6923076923076923,\n",
       " 'CAH': 0.6538461538461539,\n",
       " 'CAT': 0.6923076923076923,\n",
       " 'CB': 0.6634615384615384,\n",
       " 'CBOE': 0.6923076923076923,\n",
       " 'CBRE': 0.7019230769230769,\n",
       " 'CCI': 0.6346153846153846,\n",
       " 'CCL': 0.6346153846153846,\n",
       " 'CDNS': 0.6153846153846154,\n",
       " 'CDW': 0.6346153846153846,\n",
       " 'CE': 0.6826923076923077,\n",
       " 'CERN': 0.7788461538461539,\n",
       " 'CF': 0.6730769230769231,\n",
       " 'CFG': 0.6153846153846154,\n",
       " 'CHD': 0.7019230769230769,\n",
       " 'CHRW': 0.7115384615384616,\n",
       " 'CHTR': 0.625,\n",
       " 'CI': 0.5865384615384616,\n",
       " 'CINF': 0.6923076923076923,\n",
       " 'CL': 0.7115384615384616,\n",
       " 'CLX': 0.7596153846153846,\n",
       " 'CMA': 0.625,\n",
       " 'CMCSA': 0.6826923076923077,\n",
       " 'CME': 0.5673076923076923,\n",
       " 'CMG': 0.5480769230769231,\n",
       " 'CMI': 0.6442307692307693,\n",
       " 'CMS': 0.7019230769230769,\n",
       " 'CNC': 0.6057692307692307,\n",
       " 'CNP': 0.625,\n",
       " 'COF': 0.6826923076923077,\n",
       " 'COG': 0.6826923076923077,\n",
       " 'COO': 0.6538461538461539,\n",
       " 'COP': 0.6442307692307693,\n",
       " 'COST': 0.6826923076923077,\n",
       " 'COTY': 0.6538461538461539,\n",
       " 'CPB': 0.8461538461538461,\n",
       " 'CPRT': 0.7211538461538461,\n",
       " 'CRM': 0.6442307692307693,\n",
       " 'CSCO': 0.6153846153846154,\n",
       " 'CSX': 0.625,\n",
       " 'CTAS': 0.625,\n",
       " 'CTL': 0.7211538461538461,\n",
       " 'CTSH': 0.6923076923076923,\n",
       " 'CTVA': 0.5865384615384616,\n",
       " 'CTXS': 0.7403846153846154,\n",
       " 'CVS': 0.7980769230769231,\n",
       " 'CVX': 0.6057692307692307,\n",
       " 'CXO': 0.6346153846153846,\n",
       " 'D': 0.7115384615384616,\n",
       " 'DAL': 0.6923076923076923,\n",
       " 'DD': 0.6634615384615384,\n",
       " 'DE': 0.5673076923076923,\n",
       " 'DFS': 0.5769230769230769,\n",
       " 'DG': 0.7019230769230769,\n",
       " 'DGX': 0.7788461538461539,\n",
       " 'DHI': 0.75,\n",
       " 'DHR': 0.7692307692307693,\n",
       " 'DIS': 0.6730769230769231,\n",
       " 'DISCA': 0.6634615384615384,\n",
       " 'DISCK': 0.6923076923076923,\n",
       " 'DISH': 0.6730769230769231,\n",
       " 'DLR': 0.6730769230769231,\n",
       " 'DLTR': 0.6442307692307693,\n",
       " 'DOV': 0.6826923076923077,\n",
       " 'DOW': 0.6538461538461539,\n",
       " 'DRE': 0.6923076923076923,\n",
       " 'DRI': 0.6538461538461539,\n",
       " 'DTE': 0.7211538461538461,\n",
       " 'DUK': 0.7115384615384616,\n",
       " 'DVA': 0.7307692307692307,\n",
       " 'DVN': 0.6538461538461539,\n",
       " 'DXC': 0.6538461538461539,\n",
       " 'EA': 0.6923076923076923,\n",
       " 'EBAY': 0.7692307692307693,\n",
       " 'ECL': 0.6923076923076923,\n",
       " 'ED': 0.7596153846153846,\n",
       " 'EFX': 0.7115384615384616,\n",
       " 'EIX': 0.6634615384615384,\n",
       " 'EL': 0.6923076923076923,\n",
       " 'EMN': 0.7211538461538461,\n",
       " 'EMR': 0.7115384615384616,\n",
       " 'EOG': 0.6442307692307693,\n",
       " 'EQIX': 0.6153846153846154,\n",
       " 'EQR': 0.5865384615384616,\n",
       " 'ES': 0.7019230769230769,\n",
       " 'ESS': 0.5480769230769231,\n",
       " 'ETFC': 0.6826923076923077,\n",
       " 'ETN': 0.6634615384615384,\n",
       " 'ETR': 0.6634615384615384,\n",
       " 'EVRG': 0.6826923076923077,\n",
       " 'EW': 0.6346153846153846,\n",
       " 'EXC': 0.6442307692307693,\n",
       " 'EXPD': 0.7403846153846154,\n",
       " 'EXPE': 0.6538461538461539,\n",
       " 'EXR': 0.7692307692307693,\n",
       " 'F': 0.7211538461538461,\n",
       " 'FANG': 0.6730769230769231,\n",
       " 'FAST': 0.5961538461538461,\n",
       " 'FB': 0.5961538461538461,\n",
       " 'FBHS': 0.7211538461538461,\n",
       " 'FCX': 0.6442307692307693,\n",
       " 'FDX': 0.6923076923076923,\n",
       " 'FE': 0.7019230769230769,\n",
       " 'FFIV': 0.6538461538461539,\n",
       " 'FIS': 0.6346153846153846,\n",
       " 'FISV': 0.5480769230769231,\n",
       " 'FITB': 0.5865384615384616,\n",
       " 'FLIR': 0.6057692307692307,\n",
       " 'FLS': 0.6826923076923077,\n",
       " 'FLT': 0.6634615384615384,\n",
       " 'FMC': 0.7019230769230769,\n",
       " 'FOX': 0.6923076923076923,\n",
       " 'FOXA': 0.7019230769230769,\n",
       " 'FRC': 0.5865384615384616,\n",
       " 'FRT': 0.6730769230769231,\n",
       " 'FTI': 0.6442307692307693,\n",
       " 'FTNT': 0.6923076923076923,\n",
       " 'FTV': 0.7019230769230769,\n",
       " 'GD': 0.6634615384615384,\n",
       " 'GE': 0.6153846153846154,\n",
       " 'GILD': 0.6538461538461539,\n",
       " 'GIS': 0.7403846153846154,\n",
       " 'GL': 0.6442307692307693,\n",
       " 'GLW': 0.6730769230769231,\n",
       " 'GM': 0.625,\n",
       " 'GOOG': 0.33653846153846156,\n",
       " 'GOOGL': 0.2692307692307692,\n",
       " 'GPC': 0.6634615384615384,\n",
       " 'GPN': 0.625,\n",
       " 'GPS': 0.6826923076923077,\n",
       " 'GRMN': 0.6538461538461539,\n",
       " 'GS': 0.625,\n",
       " 'GWW': 0.6923076923076923,\n",
       " 'HAL': 0.6442307692307693,\n",
       " 'HAS': 0.6730769230769231,\n",
       " 'HBAN': 0.6826923076923077,\n",
       " 'HBI': 0.6346153846153846,\n",
       " 'HCA': 0.5961538461538461,\n",
       " 'HD': 0.6634615384615384,\n",
       " 'HES': 0.5673076923076923,\n",
       " 'HFC': 0.625,\n",
       " 'HIG': 0.6153846153846154,\n",
       " 'HII': 0.6442307692307693,\n",
       " 'HLT': 0.7596153846153846,\n",
       " 'HOG': 0.6538461538461539,\n",
       " 'HOLX': 0.6634615384615384,\n",
       " 'HON': 0.6634615384615384,\n",
       " 'HPE': 0.75,\n",
       " 'HPQ': 0.6634615384615384,\n",
       " 'HRB': 0.7019230769230769,\n",
       " 'HRL': 0.7211538461538461,\n",
       " 'HSIC': 0.6826923076923077,\n",
       " 'HST': 0.6057692307692307,\n",
       " 'HSY': 0.7019230769230769,\n",
       " 'HUM': 0.5673076923076923}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "acc['A'] = get_dynamic_feat_accuracy('A')\n",
    "acc['F'] = get_dynamic_feat_accuracy('F')\n",
    "acc['GE'] = get_dynamic_feat_accuracy('GE')\n",
    "acc['DAL'] = get_dynamic_feat_accuracy('DAL')\n",
    "acc['UAL'] = get_dynamic_feat_accuracy('UAL')\n",
    "acc['ABC'] = get_dynamic_feat_accuracy('ABC')\n",
    "acc['CAT'] = get_dynamic_feat_accuracy('CAT')\n",
    "acc['DE'] = get_dynamic_feat_accuracy('DE')\n",
    "acc['D'] = get_dynamic_feat_accuracy('D')\n",
    "acc['PEP'] = get_dynamic_feat_accuracy('PEP')\n",
    "acc['IBM'] = get_dynamic_feat_accuracy('IBM')\n",
    "acc['PXD'] = get_dynamic_feat_accuracy('PXD')\n",
    "acc['VLO'] = get_dynamic_feat_accuracy('VLO')\n",
    "acc['YUM'] = get_dynamic_feat_accuracy('YUM')\n",
    "acc['AIG'] = get_dynamic_feat_accuracy('AIG')\n",
    "acc['BWA'] = get_dynamic_feat_accuracy('BWA')\n",
    "acc['HLT'] = get_dynamic_feat_accuracy('HLT')\n",
    "acc['INTU'] = get_dynamic_feat_accuracy('INTU')\n",
    "acc['L'] = get_dynamic_feat_accuracy('L')\n",
    "acc['ZTS'] = get_dynamic_feat_accuracy('ZTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.746626984126984"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(acc.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.7301587301587301,\n",
       " 'F': 0.7063492063492064,\n",
       " 'GE': 0.7301587301587301,\n",
       " 'DAL': 0.7182539682539683,\n",
       " 'UAL': 0.7777777777777778,\n",
       " 'ABC': 0.7142857142857143,\n",
       " 'CAT': 0.7341269841269841,\n",
       " 'DE': 0.7301587301587301,\n",
       " 'D': 0.7619047619047619,\n",
       " 'PEP': 0.7857142857142857,\n",
       " 'IBM': 0.7579365079365079,\n",
       " 'PXD': 0.75,\n",
       " 'VLO': 0.7103174603174603,\n",
       " 'YUM': 0.8253968253968254,\n",
       " 'AIG': 0.7182539682539683,\n",
       " 'BWA': 0.7896825396825397,\n",
       " 'HLT': 0.7420634920634921,\n",
       " 'INTU': 0.7301587301587301,\n",
       " 'L': 0.7738095238095238,\n",
       " 'ZTS': 0.746031746031746}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('stock_test_data.csv',parse_dates=True, index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2020-01-02</th>\n",
       "      <th>A</th>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.004206</td>\n",
       "      <td>-0.923785</td>\n",
       "      <td>2.71684</td>\n",
       "      <td>-3.987023</td>\n",
       "      <td>-1.518074</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>0.38385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>-0.923785</td>\n",
       "      <td>2.71684</td>\n",
       "      <td>-3.987023</td>\n",
       "      <td>-1.518074</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>0.38385</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP</th>\n",
       "      <td>0.038836</td>\n",
       "      <td>0.002817</td>\n",
       "      <td>-0.923785</td>\n",
       "      <td>2.71684</td>\n",
       "      <td>-3.987023</td>\n",
       "      <td>-1.518074</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>0.38385</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.073563</td>\n",
       "      <td>0.100990</td>\n",
       "      <td>-0.923785</td>\n",
       "      <td>2.71684</td>\n",
       "      <td>-3.987023</td>\n",
       "      <td>-1.518074</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>0.38385</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABBV</th>\n",
       "      <td>0.021090</td>\n",
       "      <td>0.016814</td>\n",
       "      <td>-0.923785</td>\n",
       "      <td>2.71684</td>\n",
       "      <td>-3.987023</td>\n",
       "      <td>-1.518074</td>\n",
       "      <td>-0.115581</td>\n",
       "      <td>0.38385</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1      PC2       PC3       PC4  \\\n",
       "Date       Ticker                                                               \n",
       "2020-01-02 A        0.020762  0.004206 -0.923785  2.71684 -3.987023 -1.518074   \n",
       "           AAL      0.006700  0.019235 -0.923785  2.71684 -3.987023 -1.518074   \n",
       "           AAP      0.038836  0.002817 -0.923785  2.71684 -3.987023 -1.518074   \n",
       "           AAPL     0.073563  0.100990 -0.923785  2.71684 -3.987023 -1.518074   \n",
       "           ABBV     0.021090  0.016814 -0.923785  2.71684 -3.987023 -1.518074   \n",
       "\n",
       "                        PC5      PC6  target  \n",
       "Date       Ticker                             \n",
       "2020-01-02 A      -0.115581  0.38385       0  \n",
       "           AAL    -0.115581  0.38385      -1  \n",
       "           AAP    -0.115581  0.38385      -1  \n",
       "           AAPL   -0.115581  0.38385       1  \n",
       "           ABBV   -0.115581  0.38385       0  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-01</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.059051</td>\n",
       "      <td>0.131357</td>\n",
       "      <td>4.21148</td>\n",
       "      <td>0.224202</td>\n",
       "      <td>-0.21806</td>\n",
       "      <td>-1.052568</td>\n",
       "      <td>0.15746</td>\n",
       "      <td>-0.346668</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume      PC1       PC2      PC3       PC4  \\\n",
       "Date       Ticker                                                              \n",
       "2020-04-01 AAPL     0.059051  0.131357  4.21148  0.224202 -0.21806 -1.052568   \n",
       "\n",
       "                       PC5       PC6  target  prediction  \n",
       "Date       Ticker                                         \n",
       "2020-04-01 AAPL    0.15746 -0.346668       1          -1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "date = '2020-04-01'\n",
    "ticker = 'AAPL'\n",
    "df = test_df\n",
    "predictor = indicator_predictor\n",
    "def get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat):\n",
    "\n",
    "    date_pred = pd.Timestamp(date, freq='D')\n",
    "    date_start = date_pred-timedelta(days=50)\n",
    "    pred_df = test_df.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "    result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "    pred = {\n",
    "            \"start\": str(date_pred),\n",
    "            \"target\": pred_df['target'][date_start:date_pred-timedelta(days=1)].tolist(),\n",
    "            \"dynamic_feat\": pred_df[dynamic_feat][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "    req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "    res = indicator_predictor.predict(req)\n",
    "    prediction_data = json.loads(res.decode('utf-8'))\n",
    "    pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "    result_df['prediction'] = pred\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAL\n",
      "AAP\n",
      "AAPL\n",
      "ABBV\n",
      "ABC\n",
      "ABMD\n",
      "ABT\n",
      "ACN\n",
      "ADBE\n",
      "ADI\n",
      "ADM\n",
      "ADP\n",
      "ADS\n",
      "ADSK\n",
      "AEE\n",
      "AEP\n",
      "AES\n",
      "AFL\n",
      "AIG\n",
      "AIV\n",
      "AIZ\n",
      "AJG\n",
      "AKAM\n",
      "ALB\n",
      "ALGN\n",
      "ALK\n",
      "ALL\n",
      "ALLE\n",
      "ALXN\n",
      "AMAT\n",
      "AMCR\n",
      "AMD\n",
      "AME\n",
      "AMGN\n",
      "AMP\n",
      "AMT\n",
      "AMZN\n",
      "ANET\n",
      "ANSS\n",
      "ANTM\n",
      "AON\n",
      "AOS\n",
      "APA\n",
      "APD\n",
      "APH\n",
      "APTV\n",
      "ARE\n",
      "ATO\n",
      "ATVI\n",
      "AVB\n",
      "AVGO\n",
      "AVY\n",
      "AWK\n",
      "AXP\n",
      "AZO\n",
      "BA\n",
      "BAC\n",
      "BAX\n",
      "BBY\n",
      "BDX\n",
      "BEN\n",
      "BIIB\n",
      "BK\n",
      "BKNG\n",
      "BKR\n",
      "BLK\n",
      "BLL\n",
      "BMY\n",
      "BR\n",
      "BSX\n",
      "BWA\n",
      "BXP\n",
      "C\n",
      "CAG\n",
      "CAH\n",
      "CAT\n",
      "CB\n",
      "CBOE\n",
      "CBRE\n",
      "CCI\n",
      "CCL\n",
      "CDNS\n",
      "CDW\n",
      "CE\n",
      "CERN\n",
      "CF\n",
      "CFG\n",
      "CHD\n",
      "CHRW\n",
      "CHTR\n",
      "CI\n",
      "CINF\n",
      "CL\n",
      "CLX\n",
      "CMA\n",
      "CMCSA\n",
      "CME\n",
      "CMG\n",
      "CMI\n",
      "CMS\n",
      "CNC\n",
      "CNP\n",
      "COF\n",
      "COG\n",
      "COO\n",
      "COP\n",
      "COST\n",
      "COTY\n",
      "CPB\n",
      "CPRT\n",
      "CRM\n",
      "CSCO\n",
      "CSX\n",
      "CTAS\n",
      "CTL\n",
      "CTSH\n",
      "CTVA\n",
      "CTXS\n",
      "CVS\n",
      "CVX\n",
      "CXO\n",
      "D\n",
      "DAL\n",
      "DD\n",
      "DE\n",
      "DFS\n",
      "DG\n",
      "DGX\n",
      "DHI\n",
      "DHR\n",
      "DIS\n",
      "DISCA\n",
      "DISCK\n",
      "DISH\n",
      "DLR\n",
      "DLTR\n",
      "DOV\n",
      "DOW\n",
      "DRE\n",
      "DRI\n",
      "DTE\n",
      "DUK\n",
      "DVA\n",
      "DVN\n",
      "DXC\n",
      "EA\n",
      "EBAY\n",
      "ECL\n",
      "ED\n",
      "EFX\n",
      "EIX\n",
      "EL\n",
      "EMN\n",
      "EMR\n",
      "EOG\n",
      "EQIX\n",
      "EQR\n",
      "ES\n",
      "ESS\n",
      "ETFC\n",
      "ETN\n",
      "ETR\n",
      "EVRG\n",
      "EW\n",
      "EXC\n",
      "EXPD\n",
      "EXPE\n",
      "EXR\n",
      "F\n",
      "FANG\n",
      "FAST\n",
      "FB\n",
      "FBHS\n",
      "FCX\n",
      "FDX\n",
      "FE\n",
      "FFIV\n",
      "FIS\n",
      "FISV\n",
      "FITB\n",
      "FLIR\n",
      "FLS\n",
      "FLT\n",
      "FMC\n",
      "FOX\n",
      "FOXA\n",
      "FRC\n",
      "FRT\n",
      "FTI\n",
      "FTNT\n",
      "FTV\n",
      "GD\n",
      "GE\n",
      "GILD\n",
      "GIS\n",
      "GL\n",
      "GLW\n",
      "GM\n",
      "GOOG\n",
      "GOOGL\n",
      "GPC\n",
      "GPN\n",
      "GPS\n",
      "GRMN\n",
      "GS\n",
      "GWW\n",
      "HAL\n",
      "HAS\n",
      "HBAN\n",
      "HBI\n",
      "HCA\n",
      "HD\n",
      "HES\n",
      "HFC\n",
      "HIG\n",
      "HII\n",
      "HLT\n",
      "HOG\n",
      "HOLX\n",
      "HON\n",
      "HPE\n",
      "HPQ\n",
      "HRB\n",
      "HRL\n",
      "HSIC\n",
      "HST\n",
      "HSY\n",
      "HUM\n",
      "HWM\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'HWM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-35ee39405d13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdate_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dynamic_feat_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicator_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdynamic_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_dynamic_feat_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindicator_predictor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdynamic_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-64-8d70d8f27a0e>\u001b[0m in \u001b[0;36mget_dynamic_feat_prediction\u001b[0;34m(ticker, date, df, predictor, dynamic_feat)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdate_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     pred = {\n\u001b[1;32m     13\u001b[0m             \u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_lowerdim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;31m# we may have a nested tuples indexer here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_nested_tuple_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;31m# we maybe be using a tuple to represent multiple dimensions here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_nested_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1452\u001b[0m             \u001b[0mcurrent_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1453\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1454\u001b[0m             \u001b[0maxis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1956\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_nested_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m                 \u001b[0mlocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_locs\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m   3060\u001b[0m                 indexer = _update_indexer(\n\u001b[1;32m   3061\u001b[0m                     _convert_to_indexer(\n\u001b[0;32m-> 3062\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3063\u001b[0m                     ),\n\u001b[1;32m   3064\u001b[0m                     \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36mget_loc_level\u001b[0;34m(self, key, level, drop_level)\u001b[0m\n\u001b[1;32m   2844\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0milevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_level_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaybe_mi_droplevels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/indexes/multi.py\u001b[0m in \u001b[0;36m_get_level_indexer\u001b[0;34m(self, key, level, indexer)\u001b[0m\n\u001b[1;32m   2934\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2935\u001b[0m                     \u001b[0;31m# The label is present in self.levels[level] but unused:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2936\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2937\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlocs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'HWM'"
     ]
    }
   ],
   "source": [
    "date_index = pd.read_csv('final_test_date_index.csv')\n",
    "date_index = date_index.values.reshape(104).tolist()\n",
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "df = test_df\n",
    "target = []\n",
    "prediction = []\n",
    "predictor = indicator_predictor\n",
    "acc = {}\n",
    "for ticker in tickers:\n",
    "    print(ticker)\n",
    "    for date in date_index:\n",
    "        target.append(get_dynamic_feat_prediction(ticker, date,df,indicator_predictor,dynamic_feat)['target'].values[0])\n",
    "        prediction.append(get_dynamic_feat_prediction(ticker, date,df,indicator_predictor,dynamic_feat)['prediction'].values[0])\n",
    "        acc[ticker] = accuracy_score(target, prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0.6730769230769231,\n",
       " 'AAL': 0.6538461538461539,\n",
       " 'AAP': 0.6826923076923077,\n",
       " 'AAPL': 0.6826923076923077,\n",
       " 'ABBV': 0.6923076923076923,\n",
       " 'ABC': 0.6794871794871795,\n",
       " 'ABMD': 0.6868131868131868,\n",
       " 'ABT': 0.6887019230769231,\n",
       " 'ACN': 0.6762820512820513,\n",
       " 'ADBE': 0.6682692307692307,\n",
       " 'ADI': 0.6590909090909091,\n",
       " 'ADM': 0.6578525641025641,\n",
       " 'ADP': 0.6612426035502958,\n",
       " 'ADS': 0.6614010989010989,\n",
       " 'ADSK': 0.6596153846153846,\n",
       " 'AEE': 0.6610576923076923,\n",
       " 'AEP': 0.6623303167420814,\n",
       " 'AES': 0.6607905982905983,\n",
       " 'AFL': 0.6614372469635628,\n",
       " 'AIG': 0.6625,\n",
       " 'AIV': 0.6611721611721612,\n",
       " 'AIZ': 0.6625874125874126,\n",
       " 'AJG': 0.6638795986622074,\n",
       " 'AKAM': 0.6666666666666666,\n",
       " 'ALB': 0.6688461538461539,\n",
       " 'ALGN': 0.6671597633136095,\n",
       " 'ALK': 0.6666666666666666,\n",
       " 'ALL': 0.6686126373626373,\n",
       " 'ALLE': 0.6690981432360743,\n",
       " 'ALXN': 0.6714743589743589,\n",
       " 'AMAT': 0.6653225806451613,\n",
       " 'AMCR': 0.6664663461538461,\n",
       " 'AMD': 0.666083916083916,\n",
       " 'AME': 0.666289592760181,\n",
       " 'AMGN': 0.6673076923076923,\n",
       " 'AMP': 0.6658653846153846,\n",
       " 'AMT': 0.6658004158004158,\n",
       " 'AMZN': 0.6571356275303644,\n",
       " 'ANET': 0.6575443786982249,\n",
       " 'ANSS': 0.6567307692307692,\n",
       " 'ANTM': 0.6561913696060038,\n",
       " 'AON': 0.657051282051282,\n",
       " 'AOS': 0.6558586762075134,\n",
       " 'APA': 0.6558129370629371,\n",
       " 'APD': 0.6551282051282051,\n",
       " 'APH': 0.657190635451505,\n",
       " 'APTV': 0.6573240589198036,\n",
       " 'ARE': 0.6586538461538461,\n",
       " 'ATO': 0.6603218210361067,\n",
       " 'ATVI': 0.6605769230769231,\n",
       " 'AVB': 0.6604449472096531,\n",
       " 'AVGO': 0.6584689349112426,\n",
       " 'AVY': 0.6591074020319303,\n",
       " 'AWK': 0.6599002849002849,\n",
       " 'AXP': 0.6603146853146853,\n",
       " 'AZO': 0.6550480769230769,\n",
       " 'BA': 0.6540148448043185,\n",
       " 'BAC': 0.6548408488063661,\n",
       " 'BAX': 0.6564537157757496,\n",
       " 'BBY': 0.6564102564102564,\n",
       " 'BDX': 0.6563682219419924,\n",
       " 'BEN': 0.6563275434243176,\n",
       " 'BIIB': 0.6567460317460317,\n",
       " 'BK': 0.6580528846153846,\n",
       " 'BKNG': 0.6545857988165681,\n",
       " 'BKR': 0.6557400932400932,\n",
       " 'BLK': 0.6541331802525833,\n",
       " 'BLL': 0.6559671945701357,\n",
       " 'BMY': 0.657329988851728,\n",
       " 'BR': 0.6571428571428571,\n",
       " 'BSX': 0.656148429035753,\n",
       " 'BWA': 0.6574519230769231,\n",
       " 'BXP': 0.6574025289778714,\n",
       " 'C': 0.6567047817047817,\n",
       " 'CAG': 0.6571794871794872,\n",
       " 'CAH': 0.6571356275303644,\n",
       " 'CAT': 0.6575924075924076,\n",
       " 'CB': 0.6576676528599605,\n",
       " 'CBOE': 0.6581061343719572,\n",
       " 'CBRE': 0.6586538461538461,\n",
       " 'CCI': 0.6583570750237417,\n",
       " 'CCL': 0.6580675422138836,\n",
       " 'CDNS': 0.6575532900834106,\n",
       " 'CDW': 0.6572802197802198,\n",
       " 'CE': 0.657579185520362,\n",
       " 'CERN': 0.6589892665474061,\n",
       " 'CF': 0.6591511936339522,\n",
       " 'CFG': 0.6586538461538461,\n",
       " 'CHD': 0.6591400172860847,\n",
       " 'CHRW': 0.6597222222222222,\n",
       " 'CHTR': 0.6597633136094675,\n",
       " 'CI': 0.6589673913043478,\n",
       " 'CINF': 0.6593258891645989,\n",
       " 'CL': 0.6598813420621932,\n",
       " 'CLX': 0.6609311740890689,\n",
       " 'CMA': 0.6605568910256411,\n",
       " 'CMCSA': 0.6607850911974623,\n",
       " 'CME': 0.659831240188383,\n",
       " 'CMG': 0.6590909090909091,\n",
       " 'CMI': 0.6589423076923077,\n",
       " 'CMS': 0.6593678598629094,\n",
       " 'CNC': 0.658842383107089,\n",
       " 'CNP': 0.6585138162808065,\n",
       " 'COF': 0.6587463017751479,\n",
       " 'COG': 0.658974358974359,\n",
       " 'COO': 0.6589259796806967,\n",
       " 'COP': 0.6587886412652768,\n",
       " 'COST': 0.6590099715099715,\n",
       " 'COTY': 0.6589625970359916,\n",
       " 'CPB': 0.6606643356643357,\n",
       " 'CPRT': 0.6612092862092862,\n",
       " 'CRM': 0.6610576923076923,\n",
       " 'CSCO': 0.6606535057862492,\n",
       " 'CSX': 0.6603407557354926,\n",
       " 'CTAS': 0.6600334448160535,\n",
       " 'CTL': 0.6605603448275862,\n",
       " 'CTSH': 0.6608316896778436,\n",
       " 'CTVA': 0.6602020860495437,\n",
       " 'CTXS': 0.6608758888170653,\n",
       " 'CVS': 0.6620192307692307,\n",
       " 'CVX': 0.661554354736173,\n",
       " 'CXO': 0.6613335435056746,\n",
       " 'D': 0.6617417135709819,\n",
       " 'DAL': 0.6619882133995038,\n",
       " 'DD': 0.662,\n",
       " 'DE': 0.6612484737484737,\n",
       " 'DFS': 0.6605844942459116,\n",
       " 'DG': 0.6609074519230769,\n",
       " 'DGX': 0.6618217054263565,\n",
       " 'DHI': 0.6625,\n",
       " 'DHR': 0.6633147386964181,\n",
       " 'DIS': 0.6633886946386947,\n",
       " 'DISCA': 0.6633892423366108,\n",
       " 'DISCK': 0.6636050516647531,\n",
       " 'DISH': 0.6636752136752136,\n",
       " 'DLR': 0.6637443438914027,\n",
       " 'DLTR': 0.6636019090398653,\n",
       " 'DOV': 0.6637402452619844,\n",
       " 'DOW': 0.6636690647482014,\n",
       " 'DRE': 0.6638736263736263,\n",
       " 'DRI': 0.6638025095471904,\n",
       " 'DTE': 0.66420639219935,\n",
       " 'DUK': 0.6645373856912319,\n",
       " 'DVA': 0.6649973290598291,\n",
       " 'DVN': 0.664920424403183,\n",
       " 'DXC': 0.6648445732349842,\n",
       " 'EA': 0.6650313971742543,\n",
       " 'EBAY': 0.665735446985447,\n",
       " 'ECL': 0.6659137842023748,\n",
       " 'ED': 0.6665384615384615,\n",
       " 'EFX': 0.6668364747834946,\n",
       " 'EIX': 0.6668142712550608,\n",
       " 'EL': 0.6669808949220714,\n",
       " 'EMN': 0.6673326673326674,\n",
       " 'EMR': 0.6676178660049628,\n",
       " 'EOG': 0.6674679487179487,\n",
       " 'EQIX': 0.6672586967172954,\n",
       " 'EQR': 0.6667478091528725,\n",
       " 'ES': 0.6669690372520561,\n",
       " 'ESS': 0.6662259615384616,\n",
       " 'ETFC': 0.6663282369804109,\n",
       " 'ETN': 0.6663105413105413,\n",
       " 'ETR': 0.6662930627654554,\n",
       " 'EVRG': 0.6663930581613509,\n",
       " 'EW': 0.6662004662004662,\n",
       " 'EXC': 0.6660681186283596,\n",
       " 'EXPD': 0.6665131275909719,\n",
       " 'EXPE': 0.6664377289377289,\n",
       " 'EXR': 0.6670459717796996,\n",
       " 'F': 0.6673642533936651,\n",
       " 'FANG': 0.6673976608187134,\n",
       " 'FAST': 0.6669834525939177,\n",
       " 'FB': 0.6665740329035127,\n",
       " 'FBHS': 0.6668877099911583,\n",
       " 'FCX': 0.6667582417582417,\n",
       " 'FDX': 0.6669034090909091,\n",
       " 'FE': 0.6671012603215993,\n",
       " 'FFIV': 0.6670267934312878,\n",
       " 'FIS': 0.6668457241082939,\n",
       " 'FISV': 0.6661858974358974,\n",
       " 'FITB': 0.6657458563535912,\n",
       " 'FLIR': 0.665416314454776,\n",
       " 'FLS': 0.6655107187894073,\n",
       " 'FLT': 0.6654995819397993,\n",
       " 'FMC': 0.6656964656964657,\n",
       " 'FOX': 0.6658395368072787,\n",
       " 'FOXA': 0.6660324969148499,\n",
       " 'FRC': 0.6656096563011457,\n",
       " 'FRT': 0.6656491656491657,\n",
       " 'FTI': 0.6655364372469635,\n",
       " 'FTNT': 0.665676600886025,\n",
       " 'FTV': 0.6658653846153846,\n",
       " 'GD': 0.6658529294539657,\n",
       " 'GE': 0.6655927835051546,\n",
       " 'GILD': 0.6655325443786982,\n",
       " 'GIS': 0.665914442700157,\n",
       " 'GL': 0.665804373291683,\n",
       " 'GLW': 0.6658411033411034,\n",
       " 'GM': 0.6656358716660224,\n",
       " 'GOOG': 0.6639423076923077,\n",
       " 'GOOGL': 0.6621699196326062,\n",
       " 'GPC': 0.6621763137852247,\n",
       " 'GPN': 0.6619931792345586,\n",
       " 'GPS': 0.6620946455505279,\n",
       " 'GRMN': 0.6620544090056285,\n",
       " 'GS': 0.6618745332337566,\n",
       " 'GWW': 0.6620215533259012,\n",
       " 'HAL': 0.6619360207100592,\n",
       " 'HAS': 0.6619893264630107,\n",
       " 'HBAN': 0.6620879120879121,\n",
       " 'HBI': 0.6619577105359096,\n",
       " 'HCA': 0.6616473149492017,\n",
       " 'HD': 0.6616558324304803,\n",
       " 'HES': 0.6612149532710281,\n",
       " 'HFC': 0.661046511627907,\n",
       " 'HIG': 0.660835113960114,\n",
       " 'HII': 0.6607585962424672,\n",
       " 'HLT': 0.661212067748765,\n",
       " 'HOG': 0.6611784334387074,\n",
       " 'HOLX': 0.6611888111888112,\n",
       " 'HON': 0.6611990950226244,\n",
       " 'HPE': 0.6615990990990991,\n",
       " 'HPQ': 0.661607450845119,\n",
       " 'HRB': 0.6617874313186813,\n",
       " 'HRL': 0.662051282051282,\n",
       " 'HSIC': 0.662142614023145,\n",
       " 'HST': 0.6618942731277533,\n",
       " 'HSY': 0.6620698380566802,\n",
       " 'HUM': 0.6616560295599597}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
