{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  363122\n",
      " 0:  357809\n",
      " 1:  456392\n"
     ]
    }
   ],
   "source": [
    "stock_data_preprocessed = pd.read_csv('stock_indicator_data.csv',parse_dates=True, index_col=[0,1])\n",
    "stock_data_preprocessed = stock_data_preprocessed[['Adj Close','Volume','target']]\n",
    "get_target_distribution(stock_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-03-16</th>\n",
       "      <th>A</th>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP</th>\n",
       "      <td>0.010572</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.129969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABC</th>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-12-31</th>\n",
       "      <th>YUM</th>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBH</th>\n",
       "      <td>0.038353</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBRA</th>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZION</th>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTS</th>\n",
       "      <td>0.033893</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1177323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target\n",
       "Date       Ticker                             \n",
       "2010-03-16 A        0.005604  0.003822       1\n",
       "           AAL      0.001880  0.006881      -1\n",
       "           AAP      0.010572  0.000981       1\n",
       "           AAPL     0.007148  0.129969       0\n",
       "           ABC      0.006258  0.004898       1\n",
       "...                      ...       ...     ...\n",
       "2019-12-31 YUM      0.025759  0.001479       0\n",
       "           ZBH      0.038353  0.000712       0\n",
       "           ZBRA     0.065616  0.000423       0\n",
       "           ZION     0.013238  0.001450       0\n",
       "           ZTS      0.033893  0.001313       0\n",
       "\n",
       "[1177323 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 860 ms, total: 19.3 s\n",
      "Wall time: 22.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"price_train.json\", training_data)\n",
    "write_json_dataset(\"price_test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "CPU times: user 146 ms, sys: 3.91 ms, total: 150 ms\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"price_train.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"price_test.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "price_estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"30\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-09 04:10:49 Starting - Starting the training job...\n",
      "2020-06-09 04:10:51 Starting - Launching requested ML instances......\n",
      "2020-06-09 04:11:53 Starting - Preparing the instances for training...\n",
      "2020-06-09 04:12:40 Downloading - Downloading input data...\n",
      "2020-06-09 04:13:20 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'30', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'30', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Real time series\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:23 INFO 140436758837056] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] Real time series\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] number of observations: 10562640\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] mean target length: 2151\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] min/mean/max target: -1.0/0.0763878159248/1.0\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] mean abs(target): 0.697724527202\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] nvidia-smi took: 0.0251979827881 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 235.3370189666748, \"sum\": 235.3370189666748, \"min\": 235.3370189666748}}, \"EndTime\": 1591676007.396361, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676007.160082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:27 INFO 140436758837056] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 449.9058723449707, \"sum\": 449.9058723449707, \"min\": 449.9058723449707}}, \"EndTime\": 1591676007.610128, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676007.396453}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:28 INFO 140436758837056] Epoch[0] Batch[0] avg_epoch_loss=1.365301\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:28 INFO 140436758837056] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.36530137062\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:28 INFO 140436758837056] Epoch[0] Batch[5] avg_epoch_loss=1.377711\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:28 INFO 140436758837056] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.37771083911\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:28 INFO 140436758837056] Epoch[0] Batch [5]#011Speed: 1022.82 samples/sec#011loss=1.377711\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 30, \"sum\": 30.0, \"min\": 30}, \"update.time\": {\"count\": 1, \"max\": 1567.6360130310059, \"sum\": 1567.6360130310059, \"min\": 1567.6360130310059}}, \"EndTime\": 1591676009.177972, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676007.610231}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=408.220784818 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.35542753935\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_d6768934-0ce4-4147-81c1-1527c96326b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.426916122436523, \"sum\": 21.426916122436523, \"min\": 21.426916122436523}}, \"EndTime\": 1591676009.200137, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676009.178064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] Epoch[1] Batch[0] avg_epoch_loss=1.333315\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:29 INFO 140436758837056] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.33331549168\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] Epoch[1] Batch[5] avg_epoch_loss=1.273547\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.27354709307\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] Epoch[1] Batch [5]#011Speed: 994.74 samples/sec#011loss=1.273547\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1474.0970134735107, \"sum\": 1474.0970134735107, \"min\": 1474.0970134735107}}, \"EndTime\": 1591676010.674379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676009.200212}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=420.555725724 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.27223032713\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:30 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_109dd1f7-a5c5-487a-ae18-9f3eac27843c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.43598747253418, \"sum\": 34.43598747253418, \"min\": 34.43598747253418}}, \"EndTime\": 1591676010.709478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676010.674479}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:31 INFO 140436758837056] Epoch[2] Batch[0] avg_epoch_loss=1.235049\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:31 INFO 140436758837056] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.2350487709\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:31 INFO 140436758837056] Epoch[2] Batch[5] avg_epoch_loss=1.267619\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:31 INFO 140436758837056] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.26761935155\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:31 INFO 140436758837056] Epoch[2] Batch [5]#011Speed: 732.43 samples/sec#011loss=1.267619\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] Epoch[2] Batch[10] avg_epoch_loss=1.263706\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.25900995731\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] Epoch[2] Batch [10]#011Speed: 484.35 samples/sec#011loss=1.259010\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1743.323802947998, \"sum\": 1743.323802947998, \"min\": 1743.323802947998}}, \"EndTime\": 1591676012.452975, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676010.709566}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=375.689733379 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.26370599053\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:32 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_1c3364f8-f485-466f-ad41-9c10549470f8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.81410789489746, \"sum\": 21.81410789489746, \"min\": 21.81410789489746}}, \"EndTime\": 1591676012.475373, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676012.453063}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] Epoch[3] Batch[0] avg_epoch_loss=1.194736\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.19473600388\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] Epoch[3] Batch[5] avg_epoch_loss=1.212061\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.21206118663\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] Epoch[3] Batch [5]#011Speed: 950.28 samples/sec#011loss=1.212061\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1461.6751670837402, \"sum\": 1461.6751670837402, \"min\": 1461.6751670837402}}, \"EndTime\": 1591676013.937187, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676012.47544}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=409.079058043 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.20652127266\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:33 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_6c23beda-65f6-472b-9a0a-f4110578cc21-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.862102508544922, \"sum\": 20.862102508544922, \"min\": 20.862102508544922}}, \"EndTime\": 1591676013.958698, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676013.937286}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:34 INFO 140436758837056] Epoch[4] Batch[0] avg_epoch_loss=1.218506\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:34 INFO 140436758837056] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.21850609779\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:34 INFO 140436758837056] Epoch[4] Batch[5] avg_epoch_loss=1.195354\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:34 INFO 140436758837056] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.19535404444\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:34 INFO 140436758837056] Epoch[4] Batch [5]#011Speed: 985.89 samples/sec#011loss=1.195354\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1506.1450004577637, \"sum\": 1506.1450004577637, \"min\": 1506.1450004577637}}, \"EndTime\": 1591676015.464988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676013.958775}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=408.288763934 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.18953652382\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:35 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_692d1ab0-bc93-4a7e-aab0-57cdcc0cbdef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.100967407226562, \"sum\": 28.100967407226562, \"min\": 28.100967407226562}}, \"EndTime\": 1591676015.49375, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676015.465086}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:36 INFO 140436758837056] Epoch[5] Batch[0] avg_epoch_loss=1.236566\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:36 INFO 140436758837056] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.23656606674\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:36 INFO 140436758837056] Epoch[5] Batch[5] avg_epoch_loss=1.153999\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:36 INFO 140436758837056] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.15399948756\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:36 INFO 140436758837056] Epoch[5] Batch [5]#011Speed: 1008.43 samples/sec#011loss=1.153999\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] Epoch[5] Batch[10] avg_epoch_loss=1.156046\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.15850076675\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] Epoch[5] Batch [10]#011Speed: 491.13 samples/sec#011loss=1.158501\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1566.4629936218262, \"sum\": 1566.4629936218262, \"min\": 1566.4629936218262}}, \"EndTime\": 1591676017.060385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676015.493837}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=427.04131024 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.15604552356\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_c18a072a-cdff-4397-8595-64193c7dcf3a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.614002227783203, \"sum\": 22.614002227783203, \"min\": 22.614002227783203}}, \"EndTime\": 1591676017.083621, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676017.060466}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] Epoch[6] Batch[0] avg_epoch_loss=1.169981\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:37 INFO 140436758837056] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.1699808836\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] Epoch[6] Batch[5] avg_epoch_loss=1.157919\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.15791944663\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] Epoch[6] Batch [5]#011Speed: 945.81 samples/sec#011loss=1.157919\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] Epoch[6] Batch[10] avg_epoch_loss=1.133826\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.10491307974\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] Epoch[6] Batch [10]#011Speed: 522.62 samples/sec#011loss=1.104913\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1542.9418087005615, \"sum\": 1542.9418087005615, \"min\": 1542.9418087005615}}, \"EndTime\": 1591676018.626716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676017.083696}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=420.587956561 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.1338256435\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:38 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_b78c74e6-811b-4478-aede-ede2a9a0b9c5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.81712532043457, \"sum\": 32.81712532043457, \"min\": 32.81712532043457}}, \"EndTime\": 1591676018.660145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676018.626807}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:39 INFO 140436758837056] Epoch[7] Batch[0] avg_epoch_loss=1.159488\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:39 INFO 140436758837056] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.15948796272\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:39 INFO 140436758837056] Epoch[7] Batch[5] avg_epoch_loss=1.128898\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:39 INFO 140436758837056] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.12889804443\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:39 INFO 140436758837056] Epoch[7] Batch [5]#011Speed: 963.28 samples/sec#011loss=1.128898\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] Epoch[7] Batch[10] avg_epoch_loss=1.146082\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.16670281887\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] Epoch[7] Batch [10]#011Speed: 524.47 samples/sec#011loss=1.166703\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1548.0339527130127, \"sum\": 1548.0339527130127, \"min\": 1548.0339527130127}}, \"EndTime\": 1591676020.208314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676018.660215}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=425.022874233 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.14608203281\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] Epoch[8] Batch[0] avg_epoch_loss=1.020067\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:40 INFO 140436758837056] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.02006697655\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] Epoch[8] Batch[5] avg_epoch_loss=1.098475\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.09847531716\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] Epoch[8] Batch [5]#011Speed: 904.59 samples/sec#011loss=1.098475\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] Epoch[8] Batch[10] avg_epoch_loss=1.095309\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.09150862694\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] Epoch[8] Batch [10]#011Speed: 533.15 samples/sec#011loss=1.091509\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1540.0810241699219, \"sum\": 1540.0810241699219, \"min\": 1540.0810241699219}}, \"EndTime\": 1591676021.748964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676020.208394}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=418.771338317 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.09530863979\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:41 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_338bb97a-c52c-4b0e-a99f-aa290e8086c3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.085905075073242, \"sum\": 22.085905075073242, \"min\": 22.085905075073242}}, \"EndTime\": 1591676021.771724, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676021.749059}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:42 INFO 140436758837056] Epoch[9] Batch[0] avg_epoch_loss=1.115062\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:42 INFO 140436758837056] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.11506223679\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:42 INFO 140436758837056] Epoch[9] Batch[5] avg_epoch_loss=1.071749\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:42 INFO 140436758837056] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.07174911102\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:42 INFO 140436758837056] Epoch[9] Batch [5]#011Speed: 1008.05 samples/sec#011loss=1.071749\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] Epoch[9] Batch[10] avg_epoch_loss=1.067720\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.06288399696\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] Epoch[9] Batch [10]#011Speed: 520.76 samples/sec#011loss=1.062884\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.5018939971924, \"sum\": 1526.5018939971924, \"min\": 1526.5018939971924}}, \"EndTime\": 1591676023.298388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676021.771808}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=442.803065475 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.06771951372\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_3acd3a80-782b-4f96-ab93-bd0533d089d8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.24699401855469, \"sum\": 33.24699401855469, \"min\": 33.24699401855469}}, \"EndTime\": 1591676023.33222, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676023.298478}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] Epoch[10] Batch[0] avg_epoch_loss=1.053786\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:43 INFO 140436758837056] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.05378615856\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] Epoch[10] Batch[5] avg_epoch_loss=1.068732\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.06873156627\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] Epoch[10] Batch [5]#011Speed: 1012.61 samples/sec#011loss=1.068732\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1447.192907333374, \"sum\": 1447.192907333374, \"min\": 1447.192907333374}}, \"EndTime\": 1591676024.779575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676023.332302}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=426.99167639 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.07208999395\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:44 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:45 INFO 140436758837056] Epoch[11] Batch[0] avg_epoch_loss=1.001351\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:45 INFO 140436758837056] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.00135099888\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:45 INFO 140436758837056] Epoch[11] Batch[5] avg_epoch_loss=1.049928\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:45 INFO 140436758837056] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.04992787043\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:45 INFO 140436758837056] Epoch[11] Batch [5]#011Speed: 993.76 samples/sec#011loss=1.049928\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1477.2570133209229, \"sum\": 1477.2570133209229, \"min\": 1477.2570133209229}}, \"EndTime\": 1591676026.257496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676024.77967}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=424.39336567 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.05118554831\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_effa5d02-328e-4525-9ac9-9f2b94d49752-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.42683410644531, \"sum\": 32.42683410644531, \"min\": 32.42683410644531}}, \"EndTime\": 1591676026.290588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676026.257595}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] Epoch[12] Batch[0] avg_epoch_loss=1.025814\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:46 INFO 140436758837056] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.02581369877\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] Epoch[12] Batch[5] avg_epoch_loss=1.023065\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.02306477229\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] Epoch[12] Batch [5]#011Speed: 943.28 samples/sec#011loss=1.023065\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] Epoch[12] Batch[10] avg_epoch_loss=1.045780\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.07303781509\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] Epoch[12] Batch [10]#011Speed: 518.77 samples/sec#011loss=1.073038\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1554.3689727783203, \"sum\": 1554.3689727783203, \"min\": 1554.3689727783203}}, \"EndTime\": 1591676027.845115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676026.29067}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=417.496023202 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.04577979175\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:47 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_b94cd39a-5d3d-465f-bb3b-60ca26e193c5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.40792655944824, \"sum\": 33.40792655944824, \"min\": 33.40792655944824}}, \"EndTime\": 1591676027.879145, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676027.845206}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:48 INFO 140436758837056] Epoch[13] Batch[0] avg_epoch_loss=1.038801\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:48 INFO 140436758837056] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.0388007164\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:48 INFO 140436758837056] Epoch[13] Batch[5] avg_epoch_loss=1.026907\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:48 INFO 140436758837056] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.02690693736\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:48 INFO 140436758837056] Epoch[13] Batch [5]#011Speed: 1014.26 samples/sec#011loss=1.026907\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] Epoch[13] Batch[10] avg_epoch_loss=1.022603\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=1.01743906736\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] Epoch[13] Batch [10]#011Speed: 546.42 samples/sec#011loss=1.017439\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1480.1568984985352, \"sum\": 1480.1568984985352, \"min\": 1480.1568984985352}}, \"EndTime\": 1591676029.35947, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676027.879232}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=435.726293146 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.02260336009\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_5c41bd3b-d662-4242-85aa-af93e2e84b78-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.487951278686523, \"sum\": 21.487951278686523, \"min\": 21.487951278686523}}, \"EndTime\": 1591676029.381522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676029.359554}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] Epoch[14] Batch[0] avg_epoch_loss=0.996914\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:49 INFO 140436758837056] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.996913611889\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] Epoch[14] Batch[5] avg_epoch_loss=1.001675\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.00167475144\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] Epoch[14] Batch [5]#011Speed: 968.33 samples/sec#011loss=1.001675\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1441.7951107025146, \"sum\": 1441.7951107025146, \"min\": 1441.7951107025146}}, \"EndTime\": 1591676030.823456, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676029.381593}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=417.499154327 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] #quality_metric: host=algo-1, epoch=14, train loss <loss>=0.995080149174\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:50 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_807467b4-e098-4fb2-9d73-1dfd126d00e2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.676063537597656, \"sum\": 21.676063537597656, \"min\": 21.676063537597656}}, \"EndTime\": 1591676030.845782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676030.823544}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:51 INFO 140436758837056] Epoch[15] Batch[0] avg_epoch_loss=0.920876\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:51 INFO 140436758837056] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=0.920875668526\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:51 INFO 140436758837056] Epoch[15] Batch[5] avg_epoch_loss=0.981554\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:51 INFO 140436758837056] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.981553673744\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:51 INFO 140436758837056] Epoch[15] Batch [5]#011Speed: 1015.56 samples/sec#011loss=0.981554\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1432.737112045288, \"sum\": 1432.737112045288, \"min\": 1432.737112045288}}, \"EndTime\": 1591676032.278675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676030.845866}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=425.725838281 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.976937013865\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_659d5801-4955-4558-9450-f48d129047ec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.821022033691406, \"sum\": 21.821022033691406, \"min\": 21.821022033691406}}, \"EndTime\": 1591676032.301144, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676032.278747}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] Epoch[16] Batch[0] avg_epoch_loss=0.999606\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:52 INFO 140436758837056] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.999605953693\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] Epoch[16] Batch[5] avg_epoch_loss=0.997071\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.99707103769\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] Epoch[16] Batch [5]#011Speed: 964.31 samples/sec#011loss=0.997071\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] Epoch[16] Batch[10] avg_epoch_loss=0.994462\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=0.99133143425\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] Epoch[16] Batch [10]#011Speed: 543.89 samples/sec#011loss=0.991331\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] processed a total of 687 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1500.946044921875, \"sum\": 1500.946044921875, \"min\": 1500.946044921875}}, \"EndTime\": 1591676033.802254, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676032.301229}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=457.670829693 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.994462127035\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:53 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:54 INFO 140436758837056] Epoch[17] Batch[0] avg_epoch_loss=1.027278\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:54 INFO 140436758837056] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.02727770805\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:54 INFO 140436758837056] Epoch[17] Batch[5] avg_epoch_loss=1.001413\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:54 INFO 140436758837056] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.00141341488\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:54 INFO 140436758837056] Epoch[17] Batch [5]#011Speed: 1013.08 samples/sec#011loss=1.001413\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] Epoch[17] Batch[10] avg_epoch_loss=0.984836\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.96494294405\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] Epoch[17] Batch [10]#011Speed: 543.97 samples/sec#011loss=0.964943\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.2829494476318, \"sum\": 1481.2829494476318, \"min\": 1481.2829494476318}}, \"EndTime\": 1591676035.284127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676033.802341}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=462.396334946 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.984835928137\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] Epoch[18] Batch[0] avg_epoch_loss=0.997304\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:55 INFO 140436758837056] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.997304022312\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] Epoch[18] Batch[5] avg_epoch_loss=0.985512\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.98551150163\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] Epoch[18] Batch [5]#011Speed: 988.92 samples/sec#011loss=0.985512\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1437.941074371338, \"sum\": 1437.941074371338, \"min\": 1437.941074371338}}, \"EndTime\": 1591676036.722606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676035.284211}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=442.961605276 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.987996649742\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:56 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:57 INFO 140436758837056] Epoch[19] Batch[0] avg_epoch_loss=0.990491\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:57 INFO 140436758837056] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.990491330624\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:57 INFO 140436758837056] Epoch[19] Batch[5] avg_epoch_loss=0.966517\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:57 INFO 140436758837056] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.966517070929\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:57 INFO 140436758837056] Epoch[19] Batch [5]#011Speed: 1013.12 samples/sec#011loss=0.966517\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1436.7320537567139, \"sum\": 1436.7320537567139, \"min\": 1436.7320537567139}}, \"EndTime\": 1591676038.159927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676036.722676}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=425.924354496 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.95399159193\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_76268ea9-81e3-4dcc-a1b7-3abc2081b485-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.109031677246094, \"sum\": 22.109031677246094, \"min\": 22.109031677246094}}, \"EndTime\": 1591676038.182722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676038.160003}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] Epoch[20] Batch[0] avg_epoch_loss=0.925555\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:58 INFO 140436758837056] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.925555229187\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] Epoch[20] Batch[5] avg_epoch_loss=0.934583\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.934582769871\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] Epoch[20] Batch [5]#011Speed: 1022.70 samples/sec#011loss=0.934583\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] Epoch[20] Batch[10] avg_epoch_loss=0.937270\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=0.940495252609\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] Epoch[20] Batch [10]#011Speed: 541.85 samples/sec#011loss=0.940495\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] processed a total of 712 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1555.9890270233154, \"sum\": 1555.9890270233154, \"min\": 1555.9890270233154}}, \"EndTime\": 1591676039.738867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676038.182795}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=457.550878738 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.963542203108\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:13:59 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:00 INFO 140436758837056] Epoch[21] Batch[0] avg_epoch_loss=0.956257\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:00 INFO 140436758837056] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.956256628036\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:00 INFO 140436758837056] Epoch[21] Batch[5] avg_epoch_loss=0.945094\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:00 INFO 140436758837056] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.945093760888\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:00 INFO 140436758837056] Epoch[21] Batch [5]#011Speed: 1007.09 samples/sec#011loss=0.945094\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] Epoch[21] Batch[10] avg_epoch_loss=0.987166\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=1.03765271902\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] Epoch[21] Batch [10]#011Speed: 526.50 samples/sec#011loss=1.037653\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1498.6789226531982, \"sum\": 1498.6789226531982, \"min\": 1498.6789226531982}}, \"EndTime\": 1591676041.238186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676039.738944}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=428.338165867 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.987166014585\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] Epoch[22] Batch[0] avg_epoch_loss=0.986267\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:01 INFO 140436758837056] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.986267030239\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] Epoch[22] Batch[5] avg_epoch_loss=0.944724\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.944724371036\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] Epoch[22] Batch [5]#011Speed: 924.14 samples/sec#011loss=0.944724\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] Epoch[22] Batch[10] avg_epoch_loss=0.977694\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=1.01725748777\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] Epoch[22] Batch [10]#011Speed: 529.86 samples/sec#011loss=1.017257\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] processed a total of 662 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1538.5150909423828, \"sum\": 1538.5150909423828, \"min\": 1538.5150909423828}}, \"EndTime\": 1591676042.77726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676041.238257}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=430.247884694 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.977693969553\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:02 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:03 INFO 140436758837056] Epoch[23] Batch[0] avg_epoch_loss=0.983600\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:03 INFO 140436758837056] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.983600020409\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:03 INFO 140436758837056] Epoch[23] Batch[5] avg_epoch_loss=0.939344\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:03 INFO 140436758837056] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.939344326655\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:03 INFO 140436758837056] Epoch[23] Batch [5]#011Speed: 1013.89 samples/sec#011loss=0.939344\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] Epoch[23] Batch[10] avg_epoch_loss=0.913744\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.883023440838\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] Epoch[23] Batch [10]#011Speed: 498.35 samples/sec#011loss=0.883023\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1563.7872219085693, \"sum\": 1563.7872219085693, \"min\": 1563.7872219085693}}, \"EndTime\": 1591676044.341576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676042.77735}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=418.178613651 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.913743924011\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_7ac58b4b-dc54-4375-8a50-ce544cf711e6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.72710609436035, \"sum\": 24.72710609436035, \"min\": 24.72710609436035}}, \"EndTime\": 1591676044.366935, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676044.341669}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] Epoch[24] Batch[0] avg_epoch_loss=0.934686\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:04 INFO 140436758837056] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.934686124325\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] Epoch[24] Batch[5] avg_epoch_loss=0.955710\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.95570965608\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] Epoch[24] Batch [5]#011Speed: 1005.01 samples/sec#011loss=0.955710\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] Epoch[24] Batch[10] avg_epoch_loss=0.961657\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.968793618679\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] Epoch[24] Batch [10]#011Speed: 523.46 samples/sec#011loss=0.968794\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1514.7919654846191, \"sum\": 1514.7919654846191, \"min\": 1514.7919654846191}}, \"EndTime\": 1591676045.881885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676044.367015}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=430.386102678 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.961656911807\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:05 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:06 INFO 140436758837056] Epoch[25] Batch[0] avg_epoch_loss=0.966143\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:06 INFO 140436758837056] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.966143012047\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:06 INFO 140436758837056] Epoch[25] Batch[5] avg_epoch_loss=0.910949\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:06 INFO 140436758837056] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.910949488481\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:06 INFO 140436758837056] Epoch[25] Batch [5]#011Speed: 995.13 samples/sec#011loss=0.910949\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] Epoch[25] Batch[10] avg_epoch_loss=0.907419\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=0.903181874752\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] Epoch[25] Batch [10]#011Speed: 516.55 samples/sec#011loss=0.903182\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1533.9579582214355, \"sum\": 1533.9579582214355, \"min\": 1533.9579582214355}}, \"EndTime\": 1591676047.416443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676045.881965}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=442.607382394 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.907418754968\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:07 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_a8ccca75-f3f7-460f-8cf2-a254ad5ae137-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.4320068359375, \"sum\": 33.4320068359375, \"min\": 33.4320068359375}}, \"EndTime\": 1591676047.450487, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676047.416533}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] Epoch[26] Batch[0] avg_epoch_loss=0.863176\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.863176167011\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] Epoch[26] Batch[5] avg_epoch_loss=0.895536\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.895535637935\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] Epoch[26] Batch [5]#011Speed: 1020.29 samples/sec#011loss=0.895536\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] Epoch[26] Batch[10] avg_epoch_loss=0.900038\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=0.905439770222\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] Epoch[26] Batch [10]#011Speed: 533.86 samples/sec#011loss=0.905440\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1546.1490154266357, \"sum\": 1546.1490154266357, \"min\": 1546.1490154266357}}, \"EndTime\": 1591676048.996805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676047.450576}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=441.705825249 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.900037516247\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:08 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_5e95a73e-eddc-460f-9889-5965d25074d2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.44598960876465, \"sum\": 21.44598960876465, \"min\": 21.44598960876465}}, \"EndTime\": 1591676049.018852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676048.996892}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] Epoch[27] Batch[0] avg_epoch_loss=0.872659\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.872659444809\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] Epoch[27] Batch[5] avg_epoch_loss=0.902233\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.902232706547\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:09 INFO 140436758837056] Epoch[27] Batch [5]#011Speed: 954.50 samples/sec#011loss=0.902233\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1471.4369773864746, \"sum\": 1471.4369773864746, \"min\": 1471.4369773864746}}, \"EndTime\": 1591676050.490435, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676049.018921}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=421.314696001 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.886910289526\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:10 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_db579037-49ea-459a-881c-8dc3322a9ba7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.77484130859375, \"sum\": 20.77484130859375, \"min\": 20.77484130859375}}, \"EndTime\": 1591676050.51188, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676050.490534}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:11 INFO 140436758837056] Epoch[28] Batch[0] avg_epoch_loss=0.920363\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:11 INFO 140436758837056] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.920362770557\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:11 INFO 140436758837056] Epoch[28] Batch[5] avg_epoch_loss=0.887572\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:11 INFO 140436758837056] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.887572179238\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:11 INFO 140436758837056] Epoch[28] Batch [5]#011Speed: 905.80 samples/sec#011loss=0.887572\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.0821514129639, \"sum\": 1508.0821514129639, \"min\": 1508.0821514129639}}, \"EndTime\": 1591676052.020122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676050.511964}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=417.709833306 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.892062944174\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] Epoch[29] Batch[0] avg_epoch_loss=0.880159\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.880159139633\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] Epoch[29] Batch[5] avg_epoch_loss=0.895688\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.895688275496\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:12 INFO 140436758837056] Epoch[29] Batch [5]#011Speed: 1006.62 samples/sec#011loss=0.895688\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Epoch[29] Batch[10] avg_epoch_loss=0.883966\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.869898271561\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Epoch[29] Batch [10]#011Speed: 536.07 samples/sec#011loss=0.869898\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.3590812683105, \"sum\": 1526.3590812683105, \"min\": 1526.3590812683105}}, \"EndTime\": 1591676053.54706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676052.020218}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] #throughput_metric: host=algo-1, train throughput=430.39828469 records/second\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.883965546435\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/state_ea7d1f61-4f6c-4132-ba99-b04177b646a3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.47786521911621, \"sum\": 22.47786521911621, \"min\": 22.47786521911621}}, \"EndTime\": 1591676053.570117, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.547151}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Final loss: 0.883965546435 (occurred at epoch 29)\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] #quality_metric: host=algo-1, train final_loss <loss>=0.883965546435\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 WARNING 140436758837056] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 254.12702560424805, \"sum\": 254.12702560424805, \"min\": 254.12702560424805}}, \"EndTime\": 1591676053.824906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.570205}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 315.71507453918457, \"sum\": 315.71507453918457, \"min\": 315.71507453918457}}, \"EndTime\": 1591676053.886436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.825001}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 8.288860321044922, \"sum\": 8.288860321044922, \"min\": 8.288860321044922}}, \"EndTime\": 1591676053.894844, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.886518}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:13 INFO 140436758837056] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.026941299438476562, \"sum\": 0.026941299438476562, \"min\": 0.026941299438476562}}, \"EndTime\": 1591676053.895554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.894887}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:16 INFO 140436758837056] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:19 INFO 140436758837056] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:22 INFO 140436758837056] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:25 INFO 140436758837056] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:28 INFO 140436758837056] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:31 INFO 140436758837056] Number of test batches scored: 60\u001b[0m\n",
      "\n",
      "2020-06-09 04:14:48 Uploading - Uploading generated training model\n",
      "2020-06-09 04:14:48 Completed - Training job completed\n",
      "\u001b[34m[06/09/2020 04:14:34 INFO 140436758837056] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 22757.272005081177, \"sum\": 22757.272005081177, \"min\": 22757.272005081177}}, \"EndTime\": 1591676076.652805, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676053.895593}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, RMSE): 0.701861048992\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, mean_absolute_QuantileLoss): 1914.4572180701523\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, mean_wQuantileLoss): 0.4836930818772492\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.1]): 0.4137331479709519\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.2]): 0.5440215817096862\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.3]): 0.5804502881914678\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.4]): 0.5701427159190908\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.5]): 0.537787166320916\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.6]): 0.5030144772540821\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.7]): 0.4736905604917152\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.8]): 0.4239509073711513\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #test_score (algo-1, wQuantileLoss[0.9]): 0.3064468916661815\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.483693081877\u001b[0m\n",
      "\u001b[34m[06/09/2020 04:14:36 INFO 140436758837056] #quality_metric: host=algo-1, test RMSE <loss>=0.701861048992\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 73640.16795158386, \"sum\": 73640.16795158386, \"min\": 73640.16795158386}, \"setuptime\": {\"count\": 1, \"max\": 8.894920349121094, \"sum\": 8.894920349121094, \"min\": 8.894920349121094}}, \"EndTime\": 1591676076.67377, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591676076.652883}\n",
      "\u001b[0m\n",
      "Training seconds: 128\n",
      "Billable seconds: 128\n",
      "CPU times: user 620 ms, sys: 39.6 ms, total: 660 ms\n",
      "Wall time: 4min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "price_estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#price_predictor = price_estimator.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type='ml.m4.xlarge',\n",
    "#    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predictor = sagemaker.predictor.RealTimePredictor(endpoint='price-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target  prediction\n",
       "Date       Ticker                                         \n",
       "2019-01-02 AAPL      0.03987  0.043087      -1          -1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume']\n",
    "date = '2019-01-02'\n",
    "ticker = 'AAPL'\n",
    "df = stock_data_preprocessed\n",
    "predictor = price_predictor\n",
    "def get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat):\n",
    "\n",
    "    date_pred = pd.Timestamp(date, freq='D')\n",
    "    date_start = date_pred-timedelta(days=50)\n",
    "    pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "    result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "    pred = {\n",
    "            \"start\": str(date_pred),\n",
    "            \"target\": pred_df['target'][date_start:date_pred-timedelta(days=1)].tolist(),\n",
    "            \"dynamic_feat\": pred_df[dynamic_feat][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "    req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "    res = price_predictor.predict(req)\n",
    "    prediction_data = json.loads(res.decode('utf-8'))\n",
    "    pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "    result_df['prediction'] = pred\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_dynamic_feat_accuracy(ticker):\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "    df = stock_data_preprocessed\n",
    "    for date in date_index:\n",
    "        target.append(get_dynamic_feat_prediction(ticker, date,df,price_predictor,dynamic_feat)['target'].values[0])\n",
    "        prediction.append(int(get_dynamic_feat_prediction(ticker, date,df,price_predictor,dynamic_feat)['prediction'].values[0]))\n",
    "    target = list(np.array(target).reshape(252))\n",
    "    prediction = list(np.array(prediction).reshape(252))\n",
    "    data = {'target': list(target), 'prediction': list(prediction)}\n",
    "    prediction_df = pd.DataFrame(data=data,index=date_index, columns=['target','prediction'])\n",
    "\n",
    "    return accuracy_score(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7063492063492064"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dynamic_feat_accuracy(ticker='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "acc['A'] = get_dynamic_feat_accuracy('A')\n",
    "acc['F'] = get_dynamic_feat_accuracy('F')\n",
    "acc['GE'] = get_dynamic_feat_accuracy('GE')\n",
    "acc['DAL'] = get_dynamic_feat_accuracy('DAL')\n",
    "acc['UAL'] = get_dynamic_feat_accuracy('UAL')\n",
    "acc['ABC'] = get_dynamic_feat_accuracy('ABC')\n",
    "acc['CAT'] = get_dynamic_feat_accuracy('CAT')\n",
    "acc['DE'] = get_dynamic_feat_accuracy('DE')\n",
    "acc['D'] = get_dynamic_feat_accuracy('D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(list(acc.values())).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
