{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  1109910\n",
      " 0:  1087758\n",
      " 1:  1400007\n"
     ]
    }
   ],
   "source": [
    "stock_data_preprocessed = pd.read_csv('stock_data_preprocessed.csv',parse_dates=True, index_col=[0,1])\n",
    "print(\"-1: \", stock_data_preprocessed[stock_data_preprocessed['target']==-1].size)\n",
    "print(\" 0: \", stock_data_preprocessed[stock_data_preprocessed['target']==0].size)\n",
    "print(\" 1: \", stock_data_preprocessed[stock_data_preprocessed['target']==1].size)\n",
    "#stock_data_preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 1\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp(\"2018-12-31 00:00:00\", freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data_2 = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + (2*k * prediction_length)].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training + (2*k * prediction_length)].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_dataset(filename, data): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for d in data:\n",
    "            json_line = json.dumps(d) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 21s, sys: 1.35 s, total: 2min 23s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train.json\", training_data)\n",
    "write_json_dataset(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/test/test.json\n",
      "CPU times: user 4 s, sys: 1.36 s, total: 5.35 s\n",
      "Wall time: 23.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-01-04 00:00:00\", \"target\": [-1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"10\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-01 00:27:34 Starting - Starting the training job...\n",
      "2020-06-01 00:27:37 Starting - Launching requested ML instances......\n",
      "2020-06-01 00:28:40 Starting - Preparing the instances for training......\n",
      "2020-06-01 00:30:00 Downloading - Downloading input data\n",
      "2020-06-01 00:30:00 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'10', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'10', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] number of observations: 1074589\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] mean target length: 2188\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] min/mean/max target: -1.0/0.0764683055568/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] mean abs(target): 0.698935127756\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:16 INFO 140489765541696] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] number of observations: 10781660\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] mean target length: 2195\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] min/mean/max target: -1.0/0.078016464997/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] mean abs(target): 0.699434317165\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] nvidia-smi took: 0.0251760482788 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 235.60380935668945, \"sum\": 235.60380935668945, \"min\": 235.60380935668945}}, \"EndTime\": 1590971420.712562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971420.476019}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:20 INFO 140489765541696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 434.7951412200928, \"sum\": 434.7951412200928, \"min\": 434.7951412200928}}, \"EndTime\": 1590971420.910954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971420.71264}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:21 INFO 140489765541696] Epoch[0] Batch[0] avg_epoch_loss=1.383354\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:21 INFO 140489765541696] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.38335394859\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:21 INFO 140489765541696] Epoch[0] Batch[5] avg_epoch_loss=1.366484\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:21 INFO 140489765541696] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.36648400625\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:21 INFO 140489765541696] Epoch[0] Batch [5]#011Speed: 1040.86 samples/sec#011loss=1.366484\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 10, \"sum\": 10.0, \"min\": 10}, \"update.time\": {\"count\": 1, \"max\": 1565.2968883514404, \"sum\": 1565.2968883514404, \"min\": 1565.2968883514404}}, \"EndTime\": 1590971422.476388, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971420.911016}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=394.779006263 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.32967811823\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:22 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_f27a5ee4-c22d-4d20-a4ec-6e9d281403d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.49002456665039, \"sum\": 22.49002456665039, \"min\": 22.49002456665039}}, \"EndTime\": 1590971422.49954, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971422.476487}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:23 INFO 140489765541696] Epoch[1] Batch[0] avg_epoch_loss=1.278607\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:23 INFO 140489765541696] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.27860713005\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:23 INFO 140489765541696] Epoch[1] Batch[5] avg_epoch_loss=1.277999\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:23 INFO 140489765541696] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.27799910307\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:23 INFO 140489765541696] Epoch[1] Batch [5]#011Speed: 969.81 samples/sec#011loss=1.277999\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1641.2019729614258, \"sum\": 1641.2019729614258, \"min\": 1641.2019729614258}}, \"EndTime\": 1590971424.140868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971422.499606}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=383.227189401 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.28214641809\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_e062d3e2-b080-4ed6-884b-5946056d80a7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.065080642700195, \"sum\": 33.065080642700195, \"min\": 33.065080642700195}}, \"EndTime\": 1590971424.174632, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971424.140951}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-01 00:30:13 Training - Training image download completed. Training in progress.\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] Epoch[2] Batch[0] avg_epoch_loss=1.196939\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:24 INFO 140489765541696] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.19693863392\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] Epoch[2] Batch[5] avg_epoch_loss=1.236816\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.23681626717\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] Epoch[2] Batch [5]#011Speed: 1076.66 samples/sec#011loss=1.236816\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] Epoch[2] Batch[10] avg_epoch_loss=1.253147\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.27274377346\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] Epoch[2] Batch [10]#011Speed: 511.29 samples/sec#011loss=1.272744\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1592.419147491455, \"sum\": 1592.419147491455, \"min\": 1592.419147491455}}, \"EndTime\": 1590971425.767243, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971424.174705}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=405.014641648 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.25314695185\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:25 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_97a53781-6e98-4fd2-aa8b-3d7949e16008-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.435012817382812, \"sum\": 31.435012817382812, \"min\": 31.435012817382812}}, \"EndTime\": 1590971425.799225, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971425.76732}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:26 INFO 140489765541696] Epoch[3] Batch[0] avg_epoch_loss=1.212687\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:26 INFO 140489765541696] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.21268689632\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:26 INFO 140489765541696] Epoch[3] Batch[5] avg_epoch_loss=1.187655\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:26 INFO 140489765541696] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.1876552701\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:26 INFO 140489765541696] Epoch[3] Batch [5]#011Speed: 1059.96 samples/sec#011loss=1.187655\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1439.2459392547607, \"sum\": 1439.2459392547607, \"min\": 1439.2459392547607}}, \"EndTime\": 1590971427.238585, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971425.799285}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=432.135603597 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.1831389308\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_972a4d6c-3839-45b7-aaa0-60c839733a12-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.62205696105957, \"sum\": 26.62205696105957, \"min\": 26.62205696105957}}, \"EndTime\": 1590971427.265792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971427.238667}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] Epoch[4] Batch[0] avg_epoch_loss=1.288647\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:27 INFO 140489765541696] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.28864729404\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] Epoch[4] Batch[5] avg_epoch_loss=1.236900\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.23690025012\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] Epoch[4] Batch [5]#011Speed: 1059.27 samples/sec#011loss=1.236900\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] Epoch[4] Batch[10] avg_epoch_loss=1.229623\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.22089054585\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] Epoch[4] Batch [10]#011Speed: 523.62 samples/sec#011loss=1.220891\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1523.6918926239014, \"sum\": 1523.6918926239014, \"min\": 1523.6918926239014}}, \"EndTime\": 1590971428.789602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971427.265855}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=449.532613157 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.22962311181\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:28 INFO 140489765541696] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:29 INFO 140489765541696] Epoch[5] Batch[0] avg_epoch_loss=1.218588\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:29 INFO 140489765541696] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.21858847141\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:29 INFO 140489765541696] Epoch[5] Batch[5] avg_epoch_loss=1.167140\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:29 INFO 140489765541696] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.16714048386\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:29 INFO 140489765541696] Epoch[5] Batch [5]#011Speed: 1086.89 samples/sec#011loss=1.167140\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1428.2729625701904, \"sum\": 1428.2729625701904, \"min\": 1428.2729625701904}}, \"EndTime\": 1590971430.218374, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971428.78968}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=443.163078012 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.18002935648\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_13e95174-88cc-4e94-9ffd-abf43ee33770-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.687103271484375, \"sum\": 20.687103271484375, \"min\": 20.687103271484375}}, \"EndTime\": 1590971430.239659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971430.218436}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] Epoch[6] Batch[0] avg_epoch_loss=1.158017\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:30 INFO 140489765541696] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.15801656246\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] Epoch[6] Batch[5] avg_epoch_loss=1.147999\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.14799910784\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] Epoch[6] Batch [5]#011Speed: 1027.57 samples/sec#011loss=1.147999\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1399.6858596801758, \"sum\": 1399.6858596801758, \"min\": 1399.6858596801758}}, \"EndTime\": 1590971431.639458, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971430.239717}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=447.204210369 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.13407012224\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:31 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_46336356-0e51-49c5-8947-fd7bbab35f93-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.54882049560547, \"sum\": 20.54882049560547, \"min\": 20.54882049560547}}, \"EndTime\": 1590971431.660634, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971431.639542}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:32 INFO 140489765541696] Epoch[7] Batch[0] avg_epoch_loss=1.145430\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:32 INFO 140489765541696] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.14542984962\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:32 INFO 140489765541696] Epoch[7] Batch[5] avg_epoch_loss=1.156469\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:32 INFO 140489765541696] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.15646912654\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:32 INFO 140489765541696] Epoch[7] Batch [5]#011Speed: 1081.71 samples/sec#011loss=1.156469\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1433.2330226898193, \"sum\": 1433.2330226898193, \"min\": 1433.2330226898193}}, \"EndTime\": 1590971433.09398, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971431.660691}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=443.016898756 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.13372193575\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_f38a5ec5-5c13-497b-ba5c-ebec4f4f8758-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.358966827392578, \"sum\": 21.358966827392578, \"min\": 21.358966827392578}}, \"EndTime\": 1590971433.115927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971433.094064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] Epoch[8] Batch[0] avg_epoch_loss=1.134721\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:33 INFO 140489765541696] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.1347206831\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] Epoch[8] Batch[5] avg_epoch_loss=1.103386\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.10338600477\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] Epoch[8] Batch [5]#011Speed: 1078.23 samples/sec#011loss=1.103386\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1436.5911483764648, \"sum\": 1436.5911483764648, \"min\": 1436.5911483764648}}, \"EndTime\": 1590971434.552631, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971433.115989}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=435.030452829 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.07571862936\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:34 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/state_56f80ecb-3e20-4571-83a2-94bdbef4ae6d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.507097244262695, \"sum\": 20.507097244262695, \"min\": 20.507097244262695}}, \"EndTime\": 1590971434.57377, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971434.552692}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] Epoch[9] Batch[0] avg_epoch_loss=1.177100\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.17709994316\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] Epoch[9] Batch[5] avg_epoch_loss=1.077137\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.07713704308\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] Epoch[9] Batch [5]#011Speed: 566.12 samples/sec#011loss=1.077137\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] processed a total of 576 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1316.6558742523193, \"sum\": 1316.6558742523193, \"min\": 1316.6558742523193}}, \"EndTime\": 1590971435.890558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971434.573836}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #throughput_metric: host=algo-1, train throughput=437.436057961 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.09236691395\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] Final loss: 1.07571862936 (occurred at epoch 8)\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] #quality_metric: host=algo-1, train final_loss <loss>=1.07571862936\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 WARNING 140489765541696] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:35 INFO 140489765541696] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 244.62294578552246, \"sum\": 244.62294578552246, \"min\": 244.62294578552246}}, \"EndTime\": 1590971436.13618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971435.890635}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:36 INFO 140489765541696] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 304.04019355773926, \"sum\": 304.04019355773926, \"min\": 304.04019355773926}}, \"EndTime\": 1590971436.195562, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971436.136246}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:36 INFO 140489765541696] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:36 INFO 140489765541696] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 8.455991744995117, \"sum\": 8.455991744995117, \"min\": 8.455991744995117}}, \"EndTime\": 1590971436.204136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971436.195638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:36 INFO 140489765541696] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:36 INFO 140489765541696] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03790855407714844, \"sum\": 0.03790855407714844, \"min\": 0.03790855407714844}}, \"EndTime\": 1590971436.204852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971436.20418}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:39 INFO 140489765541696] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:42 INFO 140489765541696] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:45 INFO 140489765541696] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:48 INFO 140489765541696] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:51 INFO 140489765541696] Number of test batches scored: 50\u001b[0m\n",
      "\n",
      "2020-06-01 00:31:06 Uploading - Uploading generated training model\n",
      "2020-06-01 00:31:06 Completed - Training job completed\n",
      "\u001b[34m[06/01/2020 00:30:53 INFO 140489765541696] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:56 INFO 140489765541696] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 22685.595989227295, \"sum\": 22685.595989227295, \"min\": 22685.595989227295}}, \"EndTime\": 1590971458.890409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971436.204912}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, RMSE): 0.744281273417\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, mean_absolute_QuantileLoss): 2263.5308479734795\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, mean_wQuantileLoss): 0.5718875310696008\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.1]): 0.3738502437445538\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.2]): 0.5793189969416398\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.3]): 0.7023141129540035\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.4]): 0.757961069836181\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.5]): 0.7392427612968098\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.6]): 0.663396136370827\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.7]): 0.5706282853888194\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.8]): 0.4579769653539696\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #test_score (algo-1, wQuantileLoss[0.9]): 0.3022992077396034\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.57188753107\u001b[0m\n",
      "\u001b[34m[06/01/2020 00:30:58 INFO 140489765541696] #quality_metric: host=algo-1, test RMSE <loss>=0.744281273417\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 42569.748878479004, \"sum\": 42569.748878479004, \"min\": 42569.748878479004}, \"setuptime\": {\"count\": 1, \"max\": 9.142875671386719, \"sum\": 9.142875671386719, \"min\": 9.142875671386719}}, \"EndTime\": 1590971458.907418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1590971458.890474}\n",
      "\u001b[0m\n",
      "Training seconds: 89\n",
      "Billable seconds: 89\n",
      "CPU times: user 1.25 s, sys: 90.4 ms, total: 1.34 s\n",
      "Wall time: 3min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = estimator.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type='ml.m4.xlarge',\n",
    "#    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_request(instance, num_samples, quantiles):\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = sagemaker.predictor.RealTimePredictor(endpoint='MLEND-Capstone-Project-2020-05-31-23-12-35-007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prediction(ticker,date):\n",
    "    try:\n",
    "        date_pred = pd.Timestamp(date, freq='D')\n",
    "        date_start = date_pred-50\n",
    "        pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "        result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "\n",
    "        pred = {\n",
    "                \"start\": str(date_pred),\n",
    "                \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
    "                \"dynamic_feat\": pred_df[['Adj Close','Volume']][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "        req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "        res = predictor.predict(req)\n",
    "\n",
    "        prediction_data = json.loads(res.decode('utf-8'))\n",
    "        pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "        result_df['prediction'] = pred\n",
    "        return result_df\n",
    "    except:\n",
    "        print('{} did not trade today.'.format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_prediction('AAPL', '2019-01-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL did not trade today.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages/ipykernel/__main__.py:7: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages/ipykernel/__main__.py:4: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-415-27a5e769e137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#for i in range(200):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stock_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_stock_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AAPL'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "date = pd.Timestamp(\"2019-01-19\", freq='D')\n",
    "i = 0\n",
    "target = []\n",
    "prediction = []\n",
    "\n",
    "#for i in range(200):\n",
    "d = date+i\n",
    "print(get_stock_prediction('AAPL', str(d))['target'].values[0])\n",
    "print(int(get_stock_prediction('AAPL', str(d))['prediction'].values[0]))\n",
    "\n",
    "   # prediction.append(get_stock_prediction('AAPL', str(d))['prediction'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Date        Ticker\n",
       " 2019-01-16  AAPL      0\n",
       " Name: target, dtype: int64, Date        Ticker\n",
       " 2019-01-17  AAPL      0\n",
       " Name: target, dtype: int64, Date        Ticker\n",
       " 2019-01-18  AAPL     -1\n",
       " Name: target, dtype: int64]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
