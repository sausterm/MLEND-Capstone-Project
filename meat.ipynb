{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_distribution(df):\n",
    "    print(\"-1: \", df[df['target']==-1].size)\n",
    "    print(\" 0: \", df[df['target']==0].size)\n",
    "    print(\" 1: \", df[df['target']==1].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_timeseries(freq='D', end_training, context_length = 50, prediction_length=1, tickers)\n",
    "\n",
    "    # we use 2 hour frequency for the time series\n",
    "    #freq = freq\n",
    "\n",
    "    # we predict for 7 days\n",
    "    #prediction_length = prediction_length\n",
    "\n",
    "    # we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "    #context_length = context_length\n",
    "\n",
    "    end_training = pd.Timestamp(end_training, freq=freq)\n",
    "\n",
    "    timeseries = []\n",
    "    \n",
    "    for ID,ticker in list(enumerate(tickers)):\n",
    "        ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "        if ticker.index[0][0]<end_training:\n",
    "            timeseries.append(ticker)\n",
    "    return timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_dataset(filename, data): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for d in data:\n",
    "            json_line = json.dumps(d) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_request(instance, num_samples, quantiles):\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prediction(ticker,date):\n",
    "    try:\n",
    "        date_pred = pd.Timestamp(date, freq='D')\n",
    "        date_start = date_pred-50\n",
    "        pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "        result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "\n",
    "        pred = {\n",
    "                \"start\": str(date_pred),\n",
    "                \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
    "                \"dynamic_feat\": pred_df[['Adj Close','Volume']][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "        req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "        res = predictor.predict(req)\n",
    "\n",
    "        prediction_data = json.loads(res.decode('utf-8'))\n",
    "        pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "        result_df['prediction'] = pred\n",
    "        return result_df\n",
    "    except:\n",
    "        print('{} did not trade today.'.format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_accuracy(ticker, date_range):\n",
    "    ticker = str(ticker)\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "\n",
    "    for date in date_range:\n",
    "        target.append(get_stock_prediction(ticker, date)['target'].values[0])\n",
    "        prediction.append(int(get_stock_prediction(ticker, date)['prediction'].values[0]))\n",
    "    target = list(np.array(target).reshape(252))\n",
    "    prediction = list(np.array(prediction).reshape(252))\n",
    "    data = {'target': list(target), 'prediction': list(prediction)}\n",
    "    prediction_df = pd.DataFrame(data=data,index=date_index, columns=['target','prediction'])\n",
    "    \n",
    "    return accuracy_score(target, prediction), prediction_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
