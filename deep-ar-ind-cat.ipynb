{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_ind_cat\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_ind_cat\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  3268098\n",
      " 0:  3220281\n",
      " 1:  4107528\n"
     ]
    }
   ],
   "source": [
    "stock_ind_cat_data = pd.read_csv('stock_indicator_data.csv',parse_dates=True, index_col=[0,1])\n",
    "get_target_distribution(stock_ind_cat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_ind_cat_data.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = []\n",
    "for ts in timeseries:\n",
    "    tickers.append(ts.index[1][1])\n",
    "cat = {}\n",
    "for ticker in enumerate(tickers):\n",
    "    cat[ticker[1]] = ticker[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Day>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_training.freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "        \"cat\" : cat[ts.index[1][1]],\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training].values.T.tolist()\n",
    "        \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"cat\" : cat[ts.index[1][1]], # input stock ticker id\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 10s, sys: 3.63 s, total: 1min 13s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train_ind_cat.json\", training_data)\n",
    "write_json_dataset(\"test_ind_cat.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_ind_cat/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_ind_cat/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train_ind_cat.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"test_ind_cat.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_ind_cat = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_ind_cat.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-05 04:42:59 Starting - Starting the training job...\n",
      "2020-06-05 04:43:00 Starting - Launching requested ML instances......\n",
      "2020-06-05 04:44:05 Starting - Preparing the instances for training...\n",
      "2020-06-05 04:44:49 Downloading - Downloading input data......\n",
      "2020-06-05 04:45:45 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:48 INFO 140024126494528] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] [cardinality=auto] Inferred value of cardinality=[491] from dataset.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] Real time series\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:45:50 INFO 140024126494528] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] Real time series\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] number of observations: 10562640\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] mean target length: 2151\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] min/mean/max target: -1.0/0.0763878159248/1.0\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] mean abs(target): 0.697724527202\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] nvidia-smi took: 0.0252130031586 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:02 INFO 140024126494528] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 236.63806915283203, \"sum\": 236.63806915283203, \"min\": 236.63806915283203}}, \"EndTime\": 1591332363.021089, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332362.783553}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:03 INFO 140024126494528] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 445.6448554992676, \"sum\": 445.6448554992676, \"min\": 445.6448554992676}}, \"EndTime\": 1591332363.229329, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332363.021172}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:04 INFO 140024126494528] Epoch[0] Batch[0] avg_epoch_loss=1.379662\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.37966179848\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:04 INFO 140024126494528] Epoch[0] Batch[5] avg_epoch_loss=1.392185\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.39218477408\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:04 INFO 140024126494528] Epoch[0] Batch [5]#011Speed: 1008.75 samples/sec#011loss=1.392185\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 2850.9929180145264, \"sum\": 2850.9929180145264, \"min\": 2850.9929180145264}}, \"EndTime\": 1591332366.080477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332363.229389}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=222.718636352 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.37895396948\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:06 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_d8e39b82-a9d0-466e-8406-15d1ccc86341-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 34.277915954589844, \"sum\": 34.277915954589844, \"min\": 34.277915954589844}}, \"EndTime\": 1591332366.115393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332366.080574}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:07 INFO 140024126494528] Epoch[1] Batch[0] avg_epoch_loss=1.320424\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.32042360306\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:07 INFO 140024126494528] Epoch[1] Batch[5] avg_epoch_loss=1.305307\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.30530736844\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:07 INFO 140024126494528] Epoch[1] Batch [5]#011Speed: 1080.36 samples/sec#011loss=1.305307\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] Epoch[1] Batch[10] avg_epoch_loss=1.314248\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=1.32497706413\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] Epoch[1] Batch [10]#011Speed: 321.32 samples/sec#011loss=1.324977\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2637.460947036743, \"sum\": 2637.460947036743, \"min\": 2637.460947036743}}, \"EndTime\": 1591332368.752996, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332366.115469}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=254.779493529 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.31424813921\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:08 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_4938f624-215d-4207-883a-af799d97a45f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 28.189897537231445, \"sum\": 28.189897537231445, \"min\": 28.189897537231445}}, \"EndTime\": 1591332368.781754, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332368.753073}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:10 INFO 140024126494528] Epoch[2] Batch[0] avg_epoch_loss=1.229884\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:10 INFO 140024126494528] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.22988438606\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:10 INFO 140024126494528] Epoch[2] Batch[5] avg_epoch_loss=1.285030\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:10 INFO 140024126494528] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.28503040473\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:10 INFO 140024126494528] Epoch[2] Batch [5]#011Speed: 1052.28 samples/sec#011loss=1.285030\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] Epoch[2] Batch[10] avg_epoch_loss=1.278300\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.27022294998\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] Epoch[2] Batch [10]#011Speed: 323.10 samples/sec#011loss=1.270223\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2595.2892303466797, \"sum\": 2595.2892303466797, \"min\": 2595.2892303466797}}, \"EndTime\": 1591332371.377185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332368.781832}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=254.681256425 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.27829974348\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:11 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_27d59ada-c569-48b6-8f78-bf6406e22e17-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.546913146972656, \"sum\": 20.546913146972656, \"min\": 20.546913146972656}}, \"EndTime\": 1591332371.398321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332371.377259}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:12 INFO 140024126494528] Epoch[3] Batch[0] avg_epoch_loss=1.257934\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.25793373585\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:12 INFO 140024126494528] Epoch[3] Batch[5] avg_epoch_loss=1.232842\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.23284200827\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:12 INFO 140024126494528] Epoch[3] Batch [5]#011Speed: 1088.04 samples/sec#011loss=1.232842\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2458.383083343506, \"sum\": 2458.383083343506, \"min\": 2458.383083343506}}, \"EndTime\": 1591332373.856822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332371.398391}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=246.899913282 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.25013591051\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:13 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_89608479-895a-4f64-83b0-88a0565139bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.56598663330078, \"sum\": 20.56598663330078, \"min\": 20.56598663330078}}, \"EndTime\": 1591332373.877889, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332373.856895}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:15 INFO 140024126494528] Epoch[4] Batch[0] avg_epoch_loss=1.265180\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:15 INFO 140024126494528] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.26517999172\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:15 INFO 140024126494528] Epoch[4] Batch[5] avg_epoch_loss=1.237171\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:15 INFO 140024126494528] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.23717127244\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:15 INFO 140024126494528] Epoch[4] Batch [5]#011Speed: 1007.32 samples/sec#011loss=1.237171\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] Epoch[4] Batch[10] avg_epoch_loss=1.203439\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.16296062469\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] Epoch[4] Batch [10]#011Speed: 318.53 samples/sec#011loss=1.162961\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2593.381881713867, \"sum\": 2593.381881713867, \"min\": 2593.381881713867}}, \"EndTime\": 1591332376.471379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332373.877939}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=247.157293614 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.20343915983\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:16 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_73158160-15db-40ec-8625-8d08fbb3f62a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.322956085205078, \"sum\": 31.322956085205078, \"min\": 31.322956085205078}}, \"EndTime\": 1591332376.503221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332376.471452}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:17 INFO 140024126494528] Epoch[5] Batch[0] avg_epoch_loss=1.153351\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.15335130692\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:18 INFO 140024126494528] Epoch[5] Batch[5] avg_epoch_loss=1.210193\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:18 INFO 140024126494528] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.21019285917\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:18 INFO 140024126494528] Epoch[5] Batch [5]#011Speed: 1069.77 samples/sec#011loss=1.210193\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] Epoch[5] Batch[10] avg_epoch_loss=1.191216\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.16844360828\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] Epoch[5] Batch [10]#011Speed: 331.42 samples/sec#011loss=1.168444\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2575.1750469207764, \"sum\": 2575.1750469207764, \"min\": 2575.1750469207764}}, \"EndTime\": 1591332379.078533, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332376.503295}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.234031144 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.19121592695\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:19 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_71616ad9-2c32-4e24-bede-1cb8e3985971-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.33209800720215, \"sum\": 20.33209800720215, \"min\": 20.33209800720215}}, \"EndTime\": 1591332379.099455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332379.078608}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:20 INFO 140024126494528] Epoch[6] Batch[0] avg_epoch_loss=1.187339\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:20 INFO 140024126494528] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.18733942509\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:20 INFO 140024126494528] Epoch[6] Batch[5] avg_epoch_loss=1.188264\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:20 INFO 140024126494528] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.18826442957\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:20 INFO 140024126494528] Epoch[6] Batch [5]#011Speed: 961.07 samples/sec#011loss=1.188264\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2563.159942626953, \"sum\": 2563.159942626953, \"min\": 2563.159942626953}}, \"EndTime\": 1591332381.662741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332379.099525}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.779910184 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.18799339533\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:21 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_681bf219-4827-4eef-bf21-183f6c2dc337-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.380062103271484, \"sum\": 26.380062103271484, \"min\": 26.380062103271484}}, \"EndTime\": 1591332381.689717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332381.662817}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:22 INFO 140024126494528] Epoch[7] Batch[0] avg_epoch_loss=1.223866\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.2238663435\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:23 INFO 140024126494528] Epoch[7] Batch[5] avg_epoch_loss=1.165555\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:23 INFO 140024126494528] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.16555527846\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:23 INFO 140024126494528] Epoch[7] Batch [5]#011Speed: 1012.20 samples/sec#011loss=1.165555\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] Epoch[7] Batch[10] avg_epoch_loss=1.202212\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.24620108604\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] Epoch[7] Batch [10]#011Speed: 320.66 samples/sec#011loss=1.246201\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2624.171018600464, \"sum\": 2624.171018600464, \"min\": 2624.171018600464}}, \"EndTime\": 1591332384.314022, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332381.68979}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=246.924581457 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.20221246373\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:24 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:25 INFO 140024126494528] Epoch[8] Batch[0] avg_epoch_loss=1.175556\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.17555630207\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:25 INFO 140024126494528] Epoch[8] Batch[5] avg_epoch_loss=1.148737\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.14873687426\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:25 INFO 140024126494528] Epoch[8] Batch [5]#011Speed: 973.30 samples/sec#011loss=1.148737\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] Epoch[8] Batch[10] avg_epoch_loss=1.133998\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.11631071568\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] Epoch[8] Batch [10]#011Speed: 319.17 samples/sec#011loss=1.116311\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2629.8911571502686, \"sum\": 2629.8911571502686, \"min\": 2629.8911571502686}}, \"EndTime\": 1591332386.944393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332384.314098}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=261.976889923 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.13399771127\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:26 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_db0da49b-499b-4ff1-9846-5a559d05d5ec-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.22308921813965, \"sum\": 24.22308921813965, \"min\": 24.22308921813965}}, \"EndTime\": 1591332386.96916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332386.94447}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:28 INFO 140024126494528] Epoch[9] Batch[0] avg_epoch_loss=1.068402\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:28 INFO 140024126494528] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.06840229034\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:28 INFO 140024126494528] Epoch[9] Batch[5] avg_epoch_loss=1.110421\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:28 INFO 140024126494528] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.11042145888\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:28 INFO 140024126494528] Epoch[9] Batch [5]#011Speed: 1052.65 samples/sec#011loss=1.110421\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2489.6390438079834, \"sum\": 2489.6390438079834, \"min\": 2489.6390438079834}}, \"EndTime\": 1591332389.458925, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332386.969227}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.231729805 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.11687064171\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:29 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_20563f63-b832-4c6c-81bd-0b81875a4dac-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.677961349487305, \"sum\": 31.677961349487305, \"min\": 31.677961349487305}}, \"EndTime\": 1591332389.4912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332389.459024}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:30 INFO 140024126494528] Epoch[10] Batch[0] avg_epoch_loss=1.135082\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:30 INFO 140024126494528] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.13508176804\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:31 INFO 140024126494528] Epoch[10] Batch[5] avg_epoch_loss=1.079419\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.07941909631\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:31 INFO 140024126494528] Epoch[10] Batch [5]#011Speed: 1065.91 samples/sec#011loss=1.079419\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2510.6070041656494, \"sum\": 2510.6070041656494, \"min\": 2510.6070041656494}}, \"EndTime\": 1591332392.00195, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332389.491277}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=250.127333267 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.10077879429\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:32 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_e0509080-1885-4d7c-8e95-3ff42cfcc465-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.016908645629883, \"sum\": 20.016908645629883, \"min\": 20.016908645629883}}, \"EndTime\": 1591332392.022578, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332392.002025}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:33 INFO 140024126494528] Epoch[11] Batch[0] avg_epoch_loss=1.091075\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:33 INFO 140024126494528] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.09107494354\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:33 INFO 140024126494528] Epoch[11] Batch[5] avg_epoch_loss=1.093806\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:33 INFO 140024126494528] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.09380580982\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:33 INFO 140024126494528] Epoch[11] Batch [5]#011Speed: 1053.09 samples/sec#011loss=1.093806\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2468.123197555542, \"sum\": 2468.123197555542, \"min\": 2468.123197555542}}, \"EndTime\": 1591332394.490825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332392.022643}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.458337893 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.07827167511\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:34 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_d8374e1c-5ddd-4908-9bfc-8c5e5ad5e469-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.39209747314453, \"sum\": 31.39209747314453, \"min\": 31.39209747314453}}, \"EndTime\": 1591332394.522769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332394.490903}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:35 INFO 140024126494528] Epoch[12] Batch[0] avg_epoch_loss=1.017559\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.01755857468\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:36 INFO 140024126494528] Epoch[12] Batch[5] avg_epoch_loss=1.049165\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.04916477203\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:36 INFO 140024126494528] Epoch[12] Batch [5]#011Speed: 1068.70 samples/sec#011loss=1.049165\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2514.219045639038, \"sum\": 2514.219045639038, \"min\": 2514.219045639038}}, \"EndTime\": 1591332397.037121, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332394.522844}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=243.803110592 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.05489053726\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:37 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_c504a299-476b-4f41-a96d-7d35f6dd9447-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.051002502441406, \"sum\": 20.051002502441406, \"min\": 20.051002502441406}}, \"EndTime\": 1591332397.057779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332397.037195}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:38 INFO 140024126494528] Epoch[13] Batch[0] avg_epoch_loss=1.043758\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:38 INFO 140024126494528] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.04375767708\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:38 INFO 140024126494528] Epoch[13] Batch[5] avg_epoch_loss=1.031885\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:38 INFO 140024126494528] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.03188541532\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:38 INFO 140024126494528] Epoch[13] Batch [5]#011Speed: 1073.93 samples/sec#011loss=1.031885\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2456.418991088867, \"sum\": 2456.418991088867, \"min\": 2456.418991088867}}, \"EndTime\": 1591332399.514332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332397.057852}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=259.715325864 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.02044565678\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:39 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_c40a6c22-148d-4234-8aeb-9a6a706b7944-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.9200382232666, \"sum\": 20.9200382232666, \"min\": 20.9200382232666}}, \"EndTime\": 1591332399.535828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332399.514413}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:40 INFO 140024126494528] Epoch[14] Batch[0] avg_epoch_loss=0.937033\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.937032580376\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:41 INFO 140024126494528] Epoch[14] Batch[5] avg_epoch_loss=0.976863\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=0.976862559716\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:41 INFO 140024126494528] Epoch[14] Batch [5]#011Speed: 988.13 samples/sec#011loss=0.976863\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2535.8290672302246, \"sum\": 2535.8290672302246, \"min\": 2535.8290672302246}}, \"EndTime\": 1591332402.071781, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332399.535885}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.963909303 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] #quality_metric: host=algo-1, epoch=14, train loss <loss>=0.992963880301\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:42 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_06af3f82-3fe8-43e8-9b77-bc86a34f2529-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.688844680786133, \"sum\": 19.688844680786133, \"min\": 19.688844680786133}}, \"EndTime\": 1591332402.092054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332402.071863}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:43 INFO 140024126494528] Epoch[15] Batch[0] avg_epoch_loss=1.012780\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:43 INFO 140024126494528] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.01278042793\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] Epoch[15] Batch[5] avg_epoch_loss=1.000059\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.00005930662\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] Epoch[15] Batch [5]#011Speed: 332.72 samples/sec#011loss=1.000059\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.079132080078, \"sum\": 2474.079132080078, \"min\": 2474.079132080078}}, \"EndTime\": 1591332404.566255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332402.092123}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=242.90855268 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.97235455513\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:44 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_7fd55bdd-9f6d-486b-a1a1-f1dd2f6af8f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.885944366455078, \"sum\": 20.885944366455078, \"min\": 20.885944366455078}}, \"EndTime\": 1591332404.587723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332404.566329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:45 INFO 140024126494528] Epoch[16] Batch[0] avg_epoch_loss=0.972637\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.972637057304\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:46 INFO 140024126494528] Epoch[16] Batch[5] avg_epoch_loss=0.970075\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:46 INFO 140024126494528] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.970075001319\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:46 INFO 140024126494528] Epoch[16] Batch [5]#011Speed: 1021.17 samples/sec#011loss=0.970075\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:47 INFO 140024126494528] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2524.3310928344727, \"sum\": 2524.3310928344727, \"min\": 2524.3310928344727}}, \"EndTime\": 1591332407.112171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332404.587789}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:47 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.996424326 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:47 INFO 140024126494528] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:47 INFO 140024126494528] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.972366577387\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:47 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:48 INFO 140024126494528] Epoch[17] Batch[0] avg_epoch_loss=0.903411\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.903411030769\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:48 INFO 140024126494528] Epoch[17] Batch[5] avg_epoch_loss=0.934429\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.934429208438\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:48 INFO 140024126494528] Epoch[17] Batch [5]#011Speed: 1051.86 samples/sec#011loss=0.934429\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2476.472854614258, \"sum\": 2476.472854614258, \"min\": 2476.472854614258}}, \"EndTime\": 1591332409.589182, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332407.112236}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.977722847 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.932896226645\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:49 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_d283ea53-8142-4103-8c47-8df2b4985824-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.321130752563477, \"sum\": 20.321130752563477, \"min\": 20.321130752563477}}, \"EndTime\": 1591332409.61016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332409.589265}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:50 INFO 140024126494528] Epoch[18] Batch[0] avg_epoch_loss=0.906745\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.906745374203\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:51 INFO 140024126494528] Epoch[18] Batch[5] avg_epoch_loss=0.911816\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:51 INFO 140024126494528] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.911816308896\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:51 INFO 140024126494528] Epoch[18] Batch [5]#011Speed: 1058.50 samples/sec#011loss=0.911816\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2524.599075317383, \"sum\": 2524.599075317383, \"min\": 2524.599075317383}}, \"EndTime\": 1591332412.134891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332409.610228}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.137624739 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.920039725304\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:52 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_ee9990da-7513-47f9-8e54-333d500d78b7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.27416229248047, \"sum\": 20.27416229248047, \"min\": 20.27416229248047}}, \"EndTime\": 1591332412.155776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332412.134961}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:53 INFO 140024126494528] Epoch[19] Batch[0] avg_epoch_loss=0.885141\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.88514149189\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:53 INFO 140024126494528] Epoch[19] Batch[5] avg_epoch_loss=0.879582\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.879581530889\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:53 INFO 140024126494528] Epoch[19] Batch [5]#011Speed: 1053.39 samples/sec#011loss=0.879582\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2485.16583442688, \"sum\": 2485.16583442688, \"min\": 2485.16583442688}}, \"EndTime\": 1591332414.641058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332412.155837}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=246.249419779 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.858762615919\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:54 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_5fe721d9-94d4-4279-a48c-5d0a1cfd06a5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.32017707824707, \"sum\": 20.32017707824707, \"min\": 20.32017707824707}}, \"EndTime\": 1591332414.662045, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332414.64114}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:55 INFO 140024126494528] Epoch[20] Batch[0] avg_epoch_loss=0.924957\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:55 INFO 140024126494528] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.924956560135\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:56 INFO 140024126494528] Epoch[20] Batch[5] avg_epoch_loss=0.904456\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:56 INFO 140024126494528] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.904456267754\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:56 INFO 140024126494528] Epoch[20] Batch [5]#011Speed: 1062.10 samples/sec#011loss=0.904456\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:57 INFO 140024126494528] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2491.5668964385986, \"sum\": 2491.5668964385986, \"min\": 2491.5668964385986}}, \"EndTime\": 1591332417.153735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332414.662109}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:57 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.21726147 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:57 INFO 140024126494528] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:57 INFO 140024126494528] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.905786246061\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:57 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:58 INFO 140024126494528] Epoch[21] Batch[0] avg_epoch_loss=0.864444\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.864443600178\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:58 INFO 140024126494528] Epoch[21] Batch[5] avg_epoch_loss=0.863075\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.863075405359\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:58 INFO 140024126494528] Epoch[21] Batch [5]#011Speed: 1052.25 samples/sec#011loss=0.863075\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] Epoch[21] Batch[10] avg_epoch_loss=0.849096\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=0.832320713997\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] Epoch[21] Batch [10]#011Speed: 332.99 samples/sec#011loss=0.832321\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2552.1769523620605, \"sum\": 2552.1769523620605, \"min\": 2552.1769523620605}}, \"EndTime\": 1591332419.706453, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332417.153799}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.240477979 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.849096000195\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:46:59 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_9f339ee9-a4fa-454a-a08d-77e1bf9cc366-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.8601131439209, \"sum\": 31.8601131439209, \"min\": 31.8601131439209}}, \"EndTime\": 1591332419.738868, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332419.70653}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:01 INFO 140024126494528] Epoch[22] Batch[0] avg_epoch_loss=0.928535\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.928535401821\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:01 INFO 140024126494528] Epoch[22] Batch[5] avg_epoch_loss=0.865536\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.865536252658\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:01 INFO 140024126494528] Epoch[22] Batch [5]#011Speed: 1017.68 samples/sec#011loss=0.865536\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2515.0320529937744, \"sum\": 2515.0320529937744, \"min\": 2515.0320529937744}}, \"EndTime\": 1591332422.254034, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332419.738943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.314672921 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.843407523632\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:02 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_f88e923d-bf8b-431c-8023-d8a2eecc52b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.354032516479492, \"sum\": 20.354032516479492, \"min\": 20.354032516479492}}, \"EndTime\": 1591332422.274987, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332422.254106}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:03 INFO 140024126494528] Epoch[23] Batch[0] avg_epoch_loss=0.857991\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.857990503311\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:03 INFO 140024126494528] Epoch[23] Batch[5] avg_epoch_loss=0.854210\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.854209562143\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:03 INFO 140024126494528] Epoch[23] Batch [5]#011Speed: 1057.95 samples/sec#011loss=0.854210\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] Epoch[23] Batch[10] avg_epoch_loss=0.842036\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.827427661419\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] Epoch[23] Batch [10]#011Speed: 326.62 samples/sec#011loss=0.827428\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2572.8108882904053, \"sum\": 2572.8108882904053, \"min\": 2572.8108882904053}}, \"EndTime\": 1591332424.847931, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332422.27505}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.128312574 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.842035970905\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:04 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_eec1edbd-e6fb-411e-a472-1e8146425279-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.78413963317871, \"sum\": 20.78413963317871, \"min\": 20.78413963317871}}, \"EndTime\": 1591332424.869317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332424.84801}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:06 INFO 140024126494528] Epoch[24] Batch[0] avg_epoch_loss=0.885971\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.885970532894\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:06 INFO 140024126494528] Epoch[24] Batch[5] avg_epoch_loss=0.862318\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.862317621708\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:06 INFO 140024126494528] Epoch[24] Batch [5]#011Speed: 1034.95 samples/sec#011loss=0.862318\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2489.670991897583, \"sum\": 2489.670991897583, \"min\": 2489.670991897583}}, \"EndTime\": 1591332427.35912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332424.869381}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=240.180787385 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.81139305532\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:07 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_6c2c35c0-6772-4566-b9f9-fc14886e629a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.26811981201172, \"sum\": 31.26811981201172, \"min\": 31.26811981201172}}, \"EndTime\": 1591332427.390973, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332427.359202}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:08 INFO 140024126494528] Epoch[25] Batch[0] avg_epoch_loss=0.893296\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.893295824528\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] Epoch[25] Batch[5] avg_epoch_loss=0.844693\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.844692528248\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] Epoch[25] Batch [5]#011Speed: 339.99 samples/sec#011loss=0.844693\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2456.610918045044, \"sum\": 2456.610918045044, \"min\": 2456.610918045044}}, \"EndTime\": 1591332429.847686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332427.391027}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.1230568 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.820230638981\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:09 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:11 INFO 140024126494528] Epoch[26] Batch[0] avg_epoch_loss=0.840234\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.840234279633\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:11 INFO 140024126494528] Epoch[26] Batch[5] avg_epoch_loss=0.816970\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.81696982185\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:11 INFO 140024126494528] Epoch[26] Batch [5]#011Speed: 1036.41 samples/sec#011loss=0.816970\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] Epoch[26] Batch[10] avg_epoch_loss=0.815825\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=0.814451920986\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] Epoch[26] Batch [10]#011Speed: 327.63 samples/sec#011loss=0.814452\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2566.350221633911, \"sum\": 2566.350221633911, \"min\": 2566.350221633911}}, \"EndTime\": 1591332432.414542, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332429.847759}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=258.721732951 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.815825321458\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:12 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:13 INFO 140024126494528] Epoch[27] Batch[0] avg_epoch_loss=0.853113\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.853113472462\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:13 INFO 140024126494528] Epoch[27] Batch[5] avg_epoch_loss=0.832924\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.83292372028\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:13 INFO 140024126494528] Epoch[27] Batch [5]#011Speed: 1064.79 samples/sec#011loss=0.832924\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:14 INFO 140024126494528] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2493.14284324646, \"sum\": 2493.14284324646, \"min\": 2493.14284324646}}, \"EndTime\": 1591332434.908166, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332432.414619}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:14 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.873668729 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:14 INFO 140024126494528] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:14 INFO 140024126494528] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.837474399805\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:14 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:16 INFO 140024126494528] Epoch[28] Batch[0] avg_epoch_loss=0.836503\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.836503267288\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:16 INFO 140024126494528] Epoch[28] Batch[5] avg_epoch_loss=0.824914\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.824914087852\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:16 INFO 140024126494528] Epoch[28] Batch [5]#011Speed: 1055.21 samples/sec#011loss=0.824914\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:17 INFO 140024126494528] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2509.261131286621, \"sum\": 2509.261131286621, \"min\": 2509.261131286621}}, \"EndTime\": 1591332437.417963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332434.908246}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:17 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.847770671 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:17 INFO 140024126494528] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.816952085495\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:17 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:18 INFO 140024126494528] Epoch[29] Batch[0] avg_epoch_loss=0.829773\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:18 INFO 140024126494528] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.829773306847\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] Epoch[29] Batch[5] avg_epoch_loss=0.844902\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.844901869694\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] Epoch[29] Batch [5]#011Speed: 1058.33 samples/sec#011loss=0.844902\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] Epoch[29] Batch[10] avg_epoch_loss=0.817999\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.785715305805\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] Epoch[29] Batch [10]#011Speed: 330.31 samples/sec#011loss=0.785715\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2557.5878620147705, \"sum\": 2557.5878620147705, \"min\": 2557.5878620147705}}, \"EndTime\": 1591332439.976134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332437.418043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=257.262800023 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.817998886108\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:19 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:21 INFO 140024126494528] Epoch[30] Batch[0] avg_epoch_loss=0.838892\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.838891625404\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:21 INFO 140024126494528] Epoch[30] Batch[5] avg_epoch_loss=0.825627\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.825626959403\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:21 INFO 140024126494528] Epoch[30] Batch [5]#011Speed: 1069.40 samples/sec#011loss=0.825627\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] Epoch[30] Batch[10] avg_epoch_loss=0.795823\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=0.760057532787\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] Epoch[30] Batch [10]#011Speed: 324.37 samples/sec#011loss=0.760058\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2590.79909324646, \"sum\": 2590.79909324646, \"min\": 2590.79909324646}}, \"EndTime\": 1591332442.567418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332439.976209}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=255.895546031 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.795822674578\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:22 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_c36b679d-867f-4525-8c08-a7e9a166794c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.473003387451172, \"sum\": 20.473003387451172, \"min\": 20.473003387451172}}, \"EndTime\": 1591332442.588452, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332442.567482}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:23 INFO 140024126494528] Epoch[31] Batch[0] avg_epoch_loss=0.728277\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:23 INFO 140024126494528] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.728277266026\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:24 INFO 140024126494528] Epoch[31] Batch[5] avg_epoch_loss=0.817301\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:24 INFO 140024126494528] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.817301462094\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:24 INFO 140024126494528] Epoch[31] Batch [5]#011Speed: 1056.47 samples/sec#011loss=0.817301\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] Epoch[31] Batch[10] avg_epoch_loss=0.814695\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=0.811567103863\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] Epoch[31] Batch [10]#011Speed: 334.70 samples/sec#011loss=0.811567\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2564.7780895233154, \"sum\": 2564.7780895233154, \"min\": 2564.7780895233154}}, \"EndTime\": 1591332445.15335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332442.588519}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=269.408000949 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.814694935625\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:25 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:26 INFO 140024126494528] Epoch[32] Batch[0] avg_epoch_loss=0.786329\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.786328673363\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:26 INFO 140024126494528] Epoch[32] Batch[5] avg_epoch_loss=0.789285\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.789285222689\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:26 INFO 140024126494528] Epoch[32] Batch [5]#011Speed: 1006.07 samples/sec#011loss=0.789285\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2524.2669582366943, \"sum\": 2524.2669582366943, \"min\": 2524.2669582366943}}, \"EndTime\": 1591332447.678157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332445.153421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.080444884 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.783960109949\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:27 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_e4cefa9b-3986-4f16-8a79-f2690cd392d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.987821578979492, \"sum\": 19.987821578979492, \"min\": 19.987821578979492}}, \"EndTime\": 1591332447.698708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332447.678217}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:29 INFO 140024126494528] Epoch[33] Batch[0] avg_epoch_loss=0.827748\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:29 INFO 140024126494528] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.827747523785\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:29 INFO 140024126494528] Epoch[33] Batch[5] avg_epoch_loss=0.827477\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:29 INFO 140024126494528] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.827477425337\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:29 INFO 140024126494528] Epoch[33] Batch [5]#011Speed: 1049.00 samples/sec#011loss=0.827477\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:30 INFO 140024126494528] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2487.226963043213, \"sum\": 2487.226963043213, \"min\": 2487.226963043213}}, \"EndTime\": 1591332450.186084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332447.698784}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:30 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.901564753 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:30 INFO 140024126494528] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:30 INFO 140024126494528] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.805176109076\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:30 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:31 INFO 140024126494528] Epoch[34] Batch[0] avg_epoch_loss=0.805779\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.805778622627\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:31 INFO 140024126494528] Epoch[34] Batch[5] avg_epoch_loss=0.827919\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.82791869839\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:31 INFO 140024126494528] Epoch[34] Batch [5]#011Speed: 1071.83 samples/sec#011loss=0.827919\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:32 INFO 140024126494528] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2476.1970043182373, \"sum\": 2476.1970043182373, \"min\": 2476.1970043182373}}, \"EndTime\": 1591332452.662811, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332450.18615}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:32 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=255.62453943 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:32 INFO 140024126494528] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.819243824482\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:32 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:33 INFO 140024126494528] Epoch[35] Batch[0] avg_epoch_loss=0.815702\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:33 INFO 140024126494528] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.815701544285\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:34 INFO 140024126494528] Epoch[35] Batch[5] avg_epoch_loss=0.807410\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:34 INFO 140024126494528] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.807410011689\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:34 INFO 140024126494528] Epoch[35] Batch [5]#011Speed: 962.06 samples/sec#011loss=0.807410\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:35 INFO 140024126494528] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2492.388963699341, \"sum\": 2492.388963699341, \"min\": 2492.388963699341}}, \"EndTime\": 1591332455.155742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332452.662872}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:35 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.758582362 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:35 INFO 140024126494528] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.818200415373\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:35 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:36 INFO 140024126494528] Epoch[36] Batch[0] avg_epoch_loss=0.833508\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.833508253098\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:36 INFO 140024126494528] Epoch[36] Batch[5] avg_epoch_loss=0.819769\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.819768935442\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:36 INFO 140024126494528] Epoch[36] Batch [5]#011Speed: 1069.60 samples/sec#011loss=0.819769\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] Epoch[36] Batch[10] avg_epoch_loss=0.824343\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=0.829832935333\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] Epoch[36] Batch [10]#011Speed: 328.73 samples/sec#011loss=0.829833\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2558.665990829468, \"sum\": 2558.665990829468, \"min\": 2558.665990829468}}, \"EndTime\": 1591332457.71498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332455.155813}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=264.970646674 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.824343480847\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:37 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:39 INFO 140024126494528] Epoch[37] Batch[0] avg_epoch_loss=0.804853\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.804852545261\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:39 INFO 140024126494528] Epoch[37] Batch[5] avg_epoch_loss=0.779638\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.779637982448\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:39 INFO 140024126494528] Epoch[37] Batch [5]#011Speed: 1062.54 samples/sec#011loss=0.779638\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:40 INFO 140024126494528] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2517.536163330078, \"sum\": 2517.536163330078, \"min\": 2517.536163330078}}, \"EndTime\": 1591332460.233009, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332457.715053}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:40 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.410426135 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:40 INFO 140024126494528] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.787573957443\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:40 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:41 INFO 140024126494528] Epoch[38] Batch[0] avg_epoch_loss=0.805921\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.805920660496\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:41 INFO 140024126494528] Epoch[38] Batch[5] avg_epoch_loss=0.801673\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.80167328318\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:41 INFO 140024126494528] Epoch[38] Batch [5]#011Speed: 1004.88 samples/sec#011loss=0.801673\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] Epoch[38] Batch[10] avg_epoch_loss=0.822453\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=0.847388458252\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] Epoch[38] Batch [10]#011Speed: 320.08 samples/sec#011loss=0.847388\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2632.014036178589, \"sum\": 2632.014036178589, \"min\": 2632.014036178589}}, \"EndTime\": 1591332462.865549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332460.23309}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=259.484685636 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.822452908212\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:42 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:44 INFO 140024126494528] Epoch[39] Batch[0] avg_epoch_loss=0.841021\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.841020584106\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:44 INFO 140024126494528] Epoch[39] Batch[5] avg_epoch_loss=0.778289\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.778289318085\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:44 INFO 140024126494528] Epoch[39] Batch [5]#011Speed: 989.32 samples/sec#011loss=0.778289\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] Epoch[39] Batch[10] avg_epoch_loss=0.798270\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=0.822247350216\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] Epoch[39] Batch [10]#011Speed: 323.50 samples/sec#011loss=0.822247\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2630.4848194122314, \"sum\": 2630.4848194122314, \"min\": 2630.4848194122314}}, \"EndTime\": 1591332465.496544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332462.865619}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.571555134 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.798270241781\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:45 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:46 INFO 140024126494528] Epoch[40] Batch[0] avg_epoch_loss=0.853942\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:46 INFO 140024126494528] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.853942394257\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:47 INFO 140024126494528] Epoch[40] Batch[5] avg_epoch_loss=0.793323\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:47 INFO 140024126494528] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.793322910865\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:47 INFO 140024126494528] Epoch[40] Batch [5]#011Speed: 1006.77 samples/sec#011loss=0.793323\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] Epoch[40] Batch[10] avg_epoch_loss=0.802386\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=0.813262367249\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] Epoch[40] Batch [10]#011Speed: 321.33 samples/sec#011loss=0.813262\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2603.5702228546143, \"sum\": 2603.5702228546143, \"min\": 2603.5702228546143}}, \"EndTime\": 1591332468.100603, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332465.49662}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=257.327675669 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.80238630013\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:48 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:49 INFO 140024126494528] Epoch[41] Batch[0] avg_epoch_loss=0.721020\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.721019506454\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:49 INFO 140024126494528] Epoch[41] Batch[5] avg_epoch_loss=0.776551\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.776550789674\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:49 INFO 140024126494528] Epoch[41] Batch [5]#011Speed: 1038.81 samples/sec#011loss=0.776551\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:50 INFO 140024126494528] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2545.0689792633057, \"sum\": 2545.0689792633057, \"min\": 2545.0689792633057}}, \"EndTime\": 1591332470.646172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332468.100681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:50 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.48933064 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:50 INFO 140024126494528] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.797988414764\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:50 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:51 INFO 140024126494528] Epoch[42] Batch[0] avg_epoch_loss=0.843704\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:51 INFO 140024126494528] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.843703985214\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:52 INFO 140024126494528] Epoch[42] Batch[5] avg_epoch_loss=0.794664\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:52 INFO 140024126494528] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.794664134582\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:52 INFO 140024126494528] Epoch[42] Batch [5]#011Speed: 1035.37 samples/sec#011loss=0.794664\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2527.4269580841064, \"sum\": 2527.4269580841064, \"min\": 2527.4269580841064}}, \"EndTime\": 1591332473.174107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332470.646252}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.234014552 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.780318683386\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:53 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_cc9369e0-fc32-4d01-9f77-df3ff9a8c6b6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.9739933013916, \"sum\": 19.9739933013916, \"min\": 19.9739933013916}}, \"EndTime\": 1591332473.194667, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332473.174172}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:54 INFO 140024126494528] Epoch[43] Batch[0] avg_epoch_loss=0.776892\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.776892185211\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:54 INFO 140024126494528] Epoch[43] Batch[5] avg_epoch_loss=0.791408\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.791408369939\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:54 INFO 140024126494528] Epoch[43] Batch [5]#011Speed: 1060.59 samples/sec#011loss=0.791408\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:55 INFO 140024126494528] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2480.1599979400635, \"sum\": 2480.1599979400635, \"min\": 2480.1599979400635}}, \"EndTime\": 1591332475.674955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332473.194736}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:55 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=245.133998627 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:55 INFO 140024126494528] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:55 INFO 140024126494528] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.786481261253\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:55 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:56 INFO 140024126494528] Epoch[44] Batch[0] avg_epoch_loss=0.775350\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:56 INFO 140024126494528] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.775349795818\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:57 INFO 140024126494528] Epoch[44] Batch[5] avg_epoch_loss=0.792915\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:57 INFO 140024126494528] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.79291459918\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:57 INFO 140024126494528] Epoch[44] Batch [5]#011Speed: 1020.75 samples/sec#011loss=0.792915\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] Epoch[44] Batch[10] avg_epoch_loss=0.786078\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.777873110771\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] Epoch[44] Batch [10]#011Speed: 334.07 samples/sec#011loss=0.777873\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2559.5579147338867, \"sum\": 2559.5579147338867, \"min\": 2559.5579147338867}}, \"EndTime\": 1591332478.235036, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332475.675035}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=268.004972817 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.786077558994\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:58 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:59 INFO 140024126494528] Epoch[45] Batch[0] avg_epoch_loss=0.802898\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.802898049355\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:59 INFO 140024126494528] Epoch[45] Batch[5] avg_epoch_loss=0.764621\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.764621317387\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:47:59 INFO 140024126494528] Epoch[45] Batch [5]#011Speed: 1021.19 samples/sec#011loss=0.764621\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] Epoch[45] Batch[10] avg_epoch_loss=0.738200\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=0.706495308876\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] Epoch[45] Batch [10]#011Speed: 337.04 samples/sec#011loss=0.706495\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2543.4961318969727, \"sum\": 2543.4961318969727, \"min\": 2543.4961318969727}}, \"EndTime\": 1591332480.779019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332478.2351}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=258.687955452 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.738200404427\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:00 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_e0a9e2c8-629b-4640-a0f7-e449fd447df5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.143985748291016, \"sum\": 20.143985748291016, \"min\": 20.143985748291016}}, \"EndTime\": 1591332480.799729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332480.77909}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:02 INFO 140024126494528] Epoch[46] Batch[0] avg_epoch_loss=0.804730\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.804729998112\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:02 INFO 140024126494528] Epoch[46] Batch[5] avg_epoch_loss=0.769237\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.76923713088\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:02 INFO 140024126494528] Epoch[46] Batch [5]#011Speed: 1052.52 samples/sec#011loss=0.769237\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] Epoch[46] Batch[10] avg_epoch_loss=0.790687\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.816427183151\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] Epoch[46] Batch [10]#011Speed: 334.09 samples/sec#011loss=0.816427\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2559.6821308135986, \"sum\": 2559.6821308135986, \"min\": 2559.6821308135986}}, \"EndTime\": 1591332483.359526, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332480.799796}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=267.209828584 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.79068715464\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:03 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:04 INFO 140024126494528] Epoch[47] Batch[0] avg_epoch_loss=0.814702\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.814702033997\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:04 INFO 140024126494528] Epoch[47] Batch[5] avg_epoch_loss=0.802837\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.802837093671\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:04 INFO 140024126494528] Epoch[47] Batch [5]#011Speed: 1062.92 samples/sec#011loss=0.802837\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] Epoch[47] Batch[10] avg_epoch_loss=0.812529\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.824160301685\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] Epoch[47] Batch [10]#011Speed: 328.39 samples/sec#011loss=0.824160\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2569.956064224243, \"sum\": 2569.956064224243, \"min\": 2569.956064224243}}, \"EndTime\": 1591332485.930011, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332483.359601}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=260.693696764 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.81252946095\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:05 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:07 INFO 140024126494528] Epoch[48] Batch[0] avg_epoch_loss=0.779210\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.779209971428\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:07 INFO 140024126494528] Epoch[48] Batch[5] avg_epoch_loss=0.765142\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.765142440796\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:07 INFO 140024126494528] Epoch[48] Batch [5]#011Speed: 1066.96 samples/sec#011loss=0.765142\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:08 INFO 140024126494528] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2519.331932067871, \"sum\": 2519.331932067871, \"min\": 2519.331932067871}}, \"EndTime\": 1591332488.449879, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332485.930083}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:08 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.435335986 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:08 INFO 140024126494528] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.767407721281\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:08 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:09 INFO 140024126494528] Epoch[49] Batch[0] avg_epoch_loss=0.765783\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:09 INFO 140024126494528] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.765783011913\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:10 INFO 140024126494528] Epoch[49] Batch[5] avg_epoch_loss=0.781734\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:10 INFO 140024126494528] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.781733681758\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:10 INFO 140024126494528] Epoch[49] Batch [5]#011Speed: 1049.80 samples/sec#011loss=0.781734\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] Epoch[49] Batch[10] avg_epoch_loss=0.768189\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.751936209202\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] Epoch[49] Batch [10]#011Speed: 326.22 samples/sec#011loss=0.751936\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2593.5800075531006, \"sum\": 2593.5800075531006, \"min\": 2593.5800075531006}}, \"EndTime\": 1591332491.043991, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332488.449964}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=261.403333126 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.768189376051\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:11 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:12 INFO 140024126494528] Epoch[50] Batch[0] avg_epoch_loss=0.768338\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.768337905407\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:12 INFO 140024126494528] Epoch[50] Batch[5] avg_epoch_loss=0.777483\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.777482648691\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:12 INFO 140024126494528] Epoch[50] Batch [5]#011Speed: 1061.62 samples/sec#011loss=0.777483\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] Epoch[50] Batch[10] avg_epoch_loss=0.751553\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=0.720437645912\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] Epoch[50] Batch [10]#011Speed: 324.27 samples/sec#011loss=0.720438\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2578.9778232574463, \"sum\": 2578.9778232574463, \"min\": 2578.9778232574463}}, \"EndTime\": 1591332493.623448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332491.044067}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=250.088590947 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.751553101973\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:13 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:14 INFO 140024126494528] Epoch[51] Batch[0] avg_epoch_loss=0.730556\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:14 INFO 140024126494528] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.730556368828\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:15 INFO 140024126494528] Epoch[51] Batch[5] avg_epoch_loss=0.731892\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:15 INFO 140024126494528] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.731891860565\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:15 INFO 140024126494528] Epoch[51] Batch [5]#011Speed: 1067.31 samples/sec#011loss=0.731892\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:16 INFO 140024126494528] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2480.5099964141846, \"sum\": 2480.5099964141846, \"min\": 2480.5099964141846}}, \"EndTime\": 1591332496.104466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332493.623524}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:16 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=254.776445694 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:16 INFO 140024126494528] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.743338316679\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:16 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:17 INFO 140024126494528] Epoch[52] Batch[0] avg_epoch_loss=0.737427\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.737426578999\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:17 INFO 140024126494528] Epoch[52] Batch[5] avg_epoch_loss=0.764333\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.764333158731\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:17 INFO 140024126494528] Epoch[52] Batch [5]#011Speed: 1034.49 samples/sec#011loss=0.764333\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:18 INFO 140024126494528] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2502.703905105591, \"sum\": 2502.703905105591, \"min\": 2502.703905105591}}, \"EndTime\": 1591332498.607747, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332496.104528}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:18 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=255.711945581 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:18 INFO 140024126494528] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:18 INFO 140024126494528] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.757550448179\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:18 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:19 INFO 140024126494528] Epoch[53] Batch[0] avg_epoch_loss=0.776266\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.776265978813\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:20 INFO 140024126494528] Epoch[53] Batch[5] avg_epoch_loss=0.770486\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:20 INFO 140024126494528] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.770486354828\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:20 INFO 140024126494528] Epoch[53] Batch [5]#011Speed: 1039.18 samples/sec#011loss=0.770486\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:21 INFO 140024126494528] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2521.9979286193848, \"sum\": 2521.9979286193848, \"min\": 2521.9979286193848}}, \"EndTime\": 1591332501.130244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332498.607824}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:21 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=248.204197088 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:21 INFO 140024126494528] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.769616442919\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:21 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:22 INFO 140024126494528] Epoch[54] Batch[0] avg_epoch_loss=0.731151\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.731151342392\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:22 INFO 140024126494528] Epoch[54] Batch[5] avg_epoch_loss=0.750158\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.750157694022\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:22 INFO 140024126494528] Epoch[54] Batch [5]#011Speed: 1075.27 samples/sec#011loss=0.750158\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2474.85089302063, \"sum\": 2474.85089302063, \"min\": 2474.85089302063}}, \"EndTime\": 1591332503.605618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332501.130326}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=248.48756294 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.73731982708\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:23 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_075ca462-96b2-417d-9f75-d25128257042-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.062923431396484, \"sum\": 20.062923431396484, \"min\": 20.062923431396484}}, \"EndTime\": 1591332503.62629, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332503.605711}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:24 INFO 140024126494528] Epoch[55] Batch[0] avg_epoch_loss=0.771127\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:24 INFO 140024126494528] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.771126568317\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:25 INFO 140024126494528] Epoch[55] Batch[5] avg_epoch_loss=0.742077\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.742077449958\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:25 INFO 140024126494528] Epoch[55] Batch [5]#011Speed: 957.74 samples/sec#011loss=0.742077\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] Epoch[55] Batch[10] avg_epoch_loss=0.720204\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.693955242634\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] Epoch[55] Batch [10]#011Speed: 327.55 samples/sec#011loss=0.693955\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2591.536045074463, \"sum\": 2591.536045074463, \"min\": 2591.536045074463}}, \"EndTime\": 1591332506.218141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332503.626555}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.892493947 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.720203719356\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:26 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_be388f90-bde7-4e02-bc16-627a44b5940a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.058000564575195, \"sum\": 32.058000564575195, \"min\": 32.058000564575195}}, \"EndTime\": 1591332506.250746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332506.218219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:27 INFO 140024126494528] Epoch[56] Batch[0] avg_epoch_loss=0.736844\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:27 INFO 140024126494528] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.736844480038\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:27 INFO 140024126494528] Epoch[56] Batch[5] avg_epoch_loss=0.738703\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:27 INFO 140024126494528] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.738702774048\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:27 INFO 140024126494528] Epoch[56] Batch [5]#011Speed: 1071.33 samples/sec#011loss=0.738703\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:28 INFO 140024126494528] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2543.553113937378, \"sum\": 2543.553113937378, \"min\": 2543.553113937378}}, \"EndTime\": 1591332508.794428, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332506.25082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:28 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=250.035087693 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:28 INFO 140024126494528] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:28 INFO 140024126494528] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.731435739994\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:28 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:30 INFO 140024126494528] Epoch[57] Batch[0] avg_epoch_loss=0.744523\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:30 INFO 140024126494528] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.744523346424\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:30 INFO 140024126494528] Epoch[57] Batch[5] avg_epoch_loss=0.743112\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:30 INFO 140024126494528] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.743111570676\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:30 INFO 140024126494528] Epoch[57] Batch [5]#011Speed: 1070.62 samples/sec#011loss=0.743112\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] Epoch[57] Batch[10] avg_epoch_loss=0.713148\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.67719180584\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] Epoch[57] Batch [10]#011Speed: 331.51 samples/sec#011loss=0.677192\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2556.1680793762207, \"sum\": 2556.1680793762207, \"min\": 2556.1680793762207}}, \"EndTime\": 1591332511.351074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332508.794487}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.711075023 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.713148041205\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:31 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_a9b655db-9066-45cb-b273-1c33b79f5326-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.75791358947754, \"sum\": 20.75791358947754, \"min\": 20.75791358947754}}, \"EndTime\": 1591332511.372395, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332511.351146}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:32 INFO 140024126494528] Epoch[58] Batch[0] avg_epoch_loss=0.778048\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.778047859669\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:32 INFO 140024126494528] Epoch[58] Batch[5] avg_epoch_loss=0.742530\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.742530037959\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:32 INFO 140024126494528] Epoch[58] Batch [5]#011Speed: 1041.43 samples/sec#011loss=0.742530\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:33 INFO 140024126494528] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2494.5759773254395, \"sum\": 2494.5759773254395, \"min\": 2494.5759773254395}}, \"EndTime\": 1591332513.867091, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332511.372461}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:33 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.732127851 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:33 INFO 140024126494528] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:33 INFO 140024126494528] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.734963411093\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:33 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:35 INFO 140024126494528] Epoch[59] Batch[0] avg_epoch_loss=0.663788\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.663788080215\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:35 INFO 140024126494528] Epoch[59] Batch[5] avg_epoch_loss=0.743131\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.743130654097\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:35 INFO 140024126494528] Epoch[59] Batch [5]#011Speed: 974.61 samples/sec#011loss=0.743131\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] Epoch[59] Batch[10] avg_epoch_loss=0.698770\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=0.645537680387\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] Epoch[59] Batch [10]#011Speed: 322.69 samples/sec#011loss=0.645538\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2614.9981021881104, \"sum\": 2614.9981021881104, \"min\": 2614.9981021881104}}, \"EndTime\": 1591332516.482618, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332513.867157}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.614364267 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.698770211502\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:36 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_2f7088ac-fa59-48d0-ad93-cecb38d14add-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.867027282714844, \"sum\": 31.867027282714844, \"min\": 31.867027282714844}}, \"EndTime\": 1591332516.515026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332516.482697}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:37 INFO 140024126494528] Epoch[60] Batch[0] avg_epoch_loss=0.821681\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.821680545807\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:38 INFO 140024126494528] Epoch[60] Batch[5] avg_epoch_loss=0.734269\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:38 INFO 140024126494528] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.734269201756\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:38 INFO 140024126494528] Epoch[60] Batch [5]#011Speed: 984.92 samples/sec#011loss=0.734269\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:39 INFO 140024126494528] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2539.8409366607666, \"sum\": 2539.8409366607666, \"min\": 2539.8409366607666}}, \"EndTime\": 1591332519.054992, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332516.515093}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:39 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.610824925 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:39 INFO 140024126494528] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.724490362406\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:39 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:40 INFO 140024126494528] Epoch[61] Batch[0] avg_epoch_loss=0.675575\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.675575077534\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:40 INFO 140024126494528] Epoch[61] Batch[5] avg_epoch_loss=0.708604\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.708603739738\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:40 INFO 140024126494528] Epoch[61] Batch [5]#011Speed: 1053.62 samples/sec#011loss=0.708604\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] Epoch[61] Batch[10] avg_epoch_loss=0.681086\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.648064815998\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] Epoch[61] Batch [10]#011Speed: 330.80 samples/sec#011loss=0.648065\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2571.094036102295, \"sum\": 2571.094036102295, \"min\": 2571.094036102295}}, \"EndTime\": 1591332521.626697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332519.055068}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=250.854758044 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.681086047129\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:41 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_a5f917dc-7f50-492f-bfe6-4e2521d73a36-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.915985107421875, \"sum\": 20.915985107421875, \"min\": 20.915985107421875}}, \"EndTime\": 1591332521.648226, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332521.626772}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:42 INFO 140024126494528] Epoch[62] Batch[0] avg_epoch_loss=0.786253\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:42 INFO 140024126494528] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.786252677441\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:43 INFO 140024126494528] Epoch[62] Batch[5] avg_epoch_loss=0.718631\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:43 INFO 140024126494528] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.718630880117\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:43 INFO 140024126494528] Epoch[62] Batch [5]#011Speed: 1025.69 samples/sec#011loss=0.718631\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] Epoch[62] Batch[10] avg_epoch_loss=0.729016\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.741477811337\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] Epoch[62] Batch [10]#011Speed: 311.37 samples/sec#011loss=0.741478\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2652.4319648742676, \"sum\": 2652.4319648742676, \"min\": 2652.4319648742676}}, \"EndTime\": 1591332524.300784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332521.648299}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=247.687860152 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.729015848853\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:44 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:45 INFO 140024126494528] Epoch[63] Batch[0] avg_epoch_loss=0.620728\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.620727777481\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:45 INFO 140024126494528] Epoch[63] Batch[5] avg_epoch_loss=0.690048\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.690047562122\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:45 INFO 140024126494528] Epoch[63] Batch [5]#011Speed: 1070.56 samples/sec#011loss=0.690048\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:46 INFO 140024126494528] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2488.715887069702, \"sum\": 2488.715887069702, \"min\": 2488.715887069702}}, \"EndTime\": 1591332526.790037, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332524.300853}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:46 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.91611574 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:46 INFO 140024126494528] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:46 INFO 140024126494528] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.690458530188\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:46 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:48 INFO 140024126494528] Epoch[64] Batch[0] avg_epoch_loss=0.680776\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.68077647686\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:48 INFO 140024126494528] Epoch[64] Batch[5] avg_epoch_loss=0.662941\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.662940611442\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:48 INFO 140024126494528] Epoch[64] Batch [5]#011Speed: 1041.25 samples/sec#011loss=0.662941\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2502.861976623535, \"sum\": 2502.861976623535, \"min\": 2502.861976623535}}, \"EndTime\": 1591332529.293436, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332526.790118}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.915213656 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.672560727596\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:49 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_52e27290-44bd-4725-a8a8-29ab25e6fe94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.01500129699707, \"sum\": 20.01500129699707, \"min\": 20.01500129699707}}, \"EndTime\": 1591332529.314108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332529.293516}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:50 INFO 140024126494528] Epoch[65] Batch[0] avg_epoch_loss=0.692016\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.692015886307\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:50 INFO 140024126494528] Epoch[65] Batch[5] avg_epoch_loss=0.652999\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.652998675903\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:50 INFO 140024126494528] Epoch[65] Batch [5]#011Speed: 863.02 samples/sec#011loss=0.652999\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] Epoch[65] Batch[10] avg_epoch_loss=0.640269\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.624993228912\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] Epoch[65] Batch [10]#011Speed: 320.37 samples/sec#011loss=0.624993\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2663.408041000366, \"sum\": 2663.408041000366, \"min\": 2663.408041000366}}, \"EndTime\": 1591332531.977656, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332529.314169}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=247.039642411 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.640268927271\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:51 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:52 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_26b20be7-547d-415b-ac0c-43baf59f69c3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.99103546142578, \"sum\": 24.99103546142578, \"min\": 24.99103546142578}}, \"EndTime\": 1591332532.003194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332531.977733}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:53 INFO 140024126494528] Epoch[66] Batch[0] avg_epoch_loss=0.641123\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.641123175621\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:53 INFO 140024126494528] Epoch[66] Batch[5] avg_epoch_loss=0.675090\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.675089816252\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:53 INFO 140024126494528] Epoch[66] Batch [5]#011Speed: 1063.61 samples/sec#011loss=0.675090\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] Epoch[66] Batch[10] avg_epoch_loss=0.644801\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.608454978466\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] Epoch[66] Batch [10]#011Speed: 329.01 samples/sec#011loss=0.608455\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2577.0089626312256, \"sum\": 2577.0089626312256, \"min\": 2577.0089626312256}}, \"EndTime\": 1591332534.580324, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332532.003255}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.443465373 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.644801253622\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:54 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:55 INFO 140024126494528] Epoch[67] Batch[0] avg_epoch_loss=0.671432\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:55 INFO 140024126494528] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.671432197094\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:56 INFO 140024126494528] Epoch[67] Batch[5] avg_epoch_loss=0.645185\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:56 INFO 140024126494528] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.645184795062\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:56 INFO 140024126494528] Epoch[67] Batch [5]#011Speed: 335.55 samples/sec#011loss=0.645185\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2490.9088611602783, \"sum\": 2490.9088611602783, \"min\": 2490.9088611602783}}, \"EndTime\": 1591332537.071713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332534.580399}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=238.45854774 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.610652437806\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:57 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_04610d28-69b6-42eb-af58-66fa36ae275f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.373987197875977, \"sum\": 21.373987197875977, \"min\": 21.373987197875977}}, \"EndTime\": 1591332537.093666, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332537.071774}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:58 INFO 140024126494528] Epoch[68] Batch[0] avg_epoch_loss=0.577349\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.577348768711\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:58 INFO 140024126494528] Epoch[68] Batch[5] avg_epoch_loss=0.592914\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.592913667361\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:58 INFO 140024126494528] Epoch[68] Batch [5]#011Speed: 946.67 samples/sec#011loss=0.592914\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] Epoch[68] Batch[10] avg_epoch_loss=0.561703\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=0.524249571562\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] Epoch[68] Batch [10]#011Speed: 332.00 samples/sec#011loss=0.524250\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2607.063055038452, \"sum\": 2607.063055038452, \"min\": 2607.063055038452}}, \"EndTime\": 1591332539.700862, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332537.093733}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.696020508 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.561702714725\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:48:59 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_232adbbf-2fe4-465d-a717-4b3b75b75918-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.55191993713379, \"sum\": 20.55191993713379, \"min\": 20.55191993713379}}, \"EndTime\": 1591332539.722, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332539.700931}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:01 INFO 140024126494528] Epoch[69] Batch[0] avg_epoch_loss=0.576349\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.576349496841\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:01 INFO 140024126494528] Epoch[69] Batch[5] avg_epoch_loss=0.566814\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.566813737154\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:01 INFO 140024126494528] Epoch[69] Batch [5]#011Speed: 1046.17 samples/sec#011loss=0.566814\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:02 INFO 140024126494528] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2508.1160068511963, \"sum\": 2508.1160068511963, \"min\": 2508.1160068511963}}, \"EndTime\": 1591332542.230237, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332539.722068}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:02 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=255.162247591 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:02 INFO 140024126494528] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.564632433653\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:02 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:03 INFO 140024126494528] Epoch[70] Batch[0] avg_epoch_loss=0.659251\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.65925103426\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:03 INFO 140024126494528] Epoch[70] Batch[5] avg_epoch_loss=0.585520\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.585519969463\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:03 INFO 140024126494528] Epoch[70] Batch [5]#011Speed: 1074.80 samples/sec#011loss=0.585520\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] Epoch[70] Batch[10] avg_epoch_loss=0.581952\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.577669376135\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] Epoch[70] Batch [10]#011Speed: 328.64 samples/sec#011loss=0.577669\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2587.6832008361816, \"sum\": 2587.6832008361816, \"min\": 2587.6832008361816}}, \"EndTime\": 1591332544.818454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332542.230298}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.884002796 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.58195151795\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:04 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:06 INFO 140024126494528] Epoch[71] Batch[0] avg_epoch_loss=0.527245\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.527244508266\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:06 INFO 140024126494528] Epoch[71] Batch[5] avg_epoch_loss=0.520751\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.520750706395\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:06 INFO 140024126494528] Epoch[71] Batch [5]#011Speed: 1059.11 samples/sec#011loss=0.520751\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2486.8760108947754, \"sum\": 2486.8760108947754, \"min\": 2486.8760108947754}}, \"EndTime\": 1591332547.305852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332544.81853}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=243.669429198 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.53176908195\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:07 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_12b4d4e0-89bc-48fa-b5ec-d59fff012a77-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.129919052124023, \"sum\": 20.129919052124023, \"min\": 20.129919052124023}}, \"EndTime\": 1591332547.326551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332547.305925}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:08 INFO 140024126494528] Epoch[72] Batch[0] avg_epoch_loss=0.576476\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.57647562027\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:08 INFO 140024126494528] Epoch[72] Batch[5] avg_epoch_loss=0.554013\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.554013441006\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:08 INFO 140024126494528] Epoch[72] Batch [5]#011Speed: 1075.29 samples/sec#011loss=0.554013\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] Epoch[72] Batch[10] avg_epoch_loss=0.535305\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=0.512855285406\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] Epoch[72] Batch [10]#011Speed: 334.08 samples/sec#011loss=0.512855\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2590.2180671691895, \"sum\": 2590.2180671691895, \"min\": 2590.2180671691895}}, \"EndTime\": 1591332549.916885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332547.326614}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=261.358067915 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.535305188461\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:09 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:11 INFO 140024126494528] Epoch[73] Batch[0] avg_epoch_loss=0.370926\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.370926201344\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:11 INFO 140024126494528] Epoch[73] Batch[5] avg_epoch_loss=0.484620\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.484619994958\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:11 INFO 140024126494528] Epoch[73] Batch [5]#011Speed: 1048.20 samples/sec#011loss=0.484620\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2478.2259464263916, \"sum\": 2478.2259464263916, \"min\": 2478.2259464263916}}, \"EndTime\": 1591332552.395655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332549.916952}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=257.026370928 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.478589579463\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:12 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_bd5ffcd6-325b-40c2-90cd-8c4c99df7a61-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.565032958984375, \"sum\": 20.565032958984375, \"min\": 20.565032958984375}}, \"EndTime\": 1591332552.416768, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332552.395736}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:13 INFO 140024126494528] Epoch[74] Batch[0] avg_epoch_loss=0.431368\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.431368261576\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] Epoch[74] Batch[5] avg_epoch_loss=0.484174\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.484174162149\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] Epoch[74] Batch [5]#011Speed: 1036.15 samples/sec#011loss=0.484174\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] Epoch[74] Batch[10] avg_epoch_loss=0.443855\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.395472347736\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] Epoch[74] Batch [10]#011Speed: 337.30 samples/sec#011loss=0.395472\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2560.374975204468, \"sum\": 2560.374975204468, \"min\": 2560.374975204468}}, \"EndTime\": 1591332554.977274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332552.416843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=260.107274122 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.443855155598\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:14 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_fa2cf8a5-a05b-405b-95f8-b9a42809e12c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.26987075805664, \"sum\": 20.26987075805664, \"min\": 20.26987075805664}}, \"EndTime\": 1591332554.998141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332554.977349}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:16 INFO 140024126494528] Epoch[75] Batch[0] avg_epoch_loss=0.490616\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.490616410971\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:16 INFO 140024126494528] Epoch[75] Batch[5] avg_epoch_loss=0.459370\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.459369535247\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:16 INFO 140024126494528] Epoch[75] Batch [5]#011Speed: 1050.38 samples/sec#011loss=0.459370\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] Epoch[75] Batch[10] avg_epoch_loss=0.442801\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.42291790247\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] Epoch[75] Batch [10]#011Speed: 334.45 samples/sec#011loss=0.422918\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2538.5499000549316, \"sum\": 2538.5499000549316, \"min\": 2538.5499000549316}}, \"EndTime\": 1591332557.536815, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332554.998205}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=266.281581493 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.442800611258\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:17 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_b063c1fb-4e89-4f29-8858-58f16eb37ad8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.47896385192871, \"sum\": 20.47896385192871, \"min\": 20.47896385192871}}, \"EndTime\": 1591332557.557916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332557.536893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:18 INFO 140024126494528] Epoch[76] Batch[0] avg_epoch_loss=0.623698\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:18 INFO 140024126494528] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.623697936535\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:19 INFO 140024126494528] Epoch[76] Batch[5] avg_epoch_loss=0.495935\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.495935087403\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:19 INFO 140024126494528] Epoch[76] Batch [5]#011Speed: 1079.76 samples/sec#011loss=0.495935\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] Epoch[76] Batch[10] avg_epoch_loss=0.487357\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=0.477063000202\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] Epoch[76] Batch [10]#011Speed: 337.57 samples/sec#011loss=0.477063\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2538.444995880127, \"sum\": 2538.444995880127, \"min\": 2538.444995880127}}, \"EndTime\": 1591332560.096476, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332557.557976}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=267.081712553 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.487356865948\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:20 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:21 INFO 140024126494528] Epoch[77] Batch[0] avg_epoch_loss=0.390243\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.390242904425\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:21 INFO 140024126494528] Epoch[77] Batch[5] avg_epoch_loss=0.413254\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:21 INFO 140024126494528] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.413254434864\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:21 INFO 140024126494528] Epoch[77] Batch [5]#011Speed: 1059.16 samples/sec#011loss=0.413254\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] Epoch[77] Batch[10] avg_epoch_loss=0.408406\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.402587234974\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] Epoch[77] Batch [10]#011Speed: 334.10 samples/sec#011loss=0.402587\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2574.3160247802734, \"sum\": 2574.3160247802734, \"min\": 2574.3160247802734}}, \"EndTime\": 1591332562.671311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332560.096543}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=259.476932555 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.408405707641\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:22 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_6051a4bb-52c4-48a3-8970-8ade9bb6be4c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.325971603393555, \"sum\": 19.325971603393555, \"min\": 19.325971603393555}}, \"EndTime\": 1591332562.691171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332562.671369}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:23 INFO 140024126494528] Epoch[78] Batch[0] avg_epoch_loss=0.293543\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:23 INFO 140024126494528] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.293542593718\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:24 INFO 140024126494528] Epoch[78] Batch[5] avg_epoch_loss=0.381872\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:24 INFO 140024126494528] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.381871794661\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:24 INFO 140024126494528] Epoch[78] Batch [5]#011Speed: 1062.48 samples/sec#011loss=0.381872\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] Epoch[78] Batch[10] avg_epoch_loss=0.439297\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.508206325769\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] Epoch[78] Batch [10]#011Speed: 331.88 samples/sec#011loss=0.508206\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2565.7129287719727, \"sum\": 2565.7129287719727, \"min\": 2565.7129287719727}}, \"EndTime\": 1591332565.257001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332562.691233}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.331686764 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.439296581528\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:25 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:26 INFO 140024126494528] Epoch[79] Batch[0] avg_epoch_loss=0.381723\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.3817230165\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:26 INFO 140024126494528] Epoch[79] Batch[5] avg_epoch_loss=0.406330\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:26 INFO 140024126494528] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.406330058972\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:26 INFO 140024126494528] Epoch[79] Batch [5]#011Speed: 1050.79 samples/sec#011loss=0.406330\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:27 INFO 140024126494528] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2476.155996322632, \"sum\": 2476.155996322632, \"min\": 2476.155996322632}}, \"EndTime\": 1591332567.733657, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332565.257059}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:27 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=254.815848537 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:27 INFO 140024126494528] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:27 INFO 140024126494528] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.427642580867\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:27 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:29 INFO 140024126494528] Epoch[80] Batch[0] avg_epoch_loss=0.302279\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:29 INFO 140024126494528] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.302279114723\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:29 INFO 140024126494528] Epoch[80] Batch[5] avg_epoch_loss=0.349752\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:29 INFO 140024126494528] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.349752366543\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:29 INFO 140024126494528] Epoch[80] Batch [5]#011Speed: 1027.22 samples/sec#011loss=0.349752\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2542.6690578460693, \"sum\": 2542.6690578460693, \"min\": 2542.6690578460693}}, \"EndTime\": 1591332570.276963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332567.733739}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=244.613986423 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.372728896141\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:30 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_4bae2ef7-4124-42f4-95ef-ceac125f248c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.16506004333496, \"sum\": 22.16506004333496, \"min\": 22.16506004333496}}, \"EndTime\": 1591332570.29967, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332570.277041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:31 INFO 140024126494528] Epoch[81] Batch[0] avg_epoch_loss=0.439993\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.439992517233\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:31 INFO 140024126494528] Epoch[81] Batch[5] avg_epoch_loss=0.383616\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:31 INFO 140024126494528] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.383616030216\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:31 INFO 140024126494528] Epoch[81] Batch [5]#011Speed: 1044.39 samples/sec#011loss=0.383616\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] Epoch[81] Batch[10] avg_epoch_loss=0.418867\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.461168962717\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] Epoch[81] Batch [10]#011Speed: 326.06 samples/sec#011loss=0.461169\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2580.936908721924, \"sum\": 2580.936908721924, \"min\": 2580.936908721924}}, \"EndTime\": 1591332572.880739, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332570.299745}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.387411488 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.418867363171\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:32 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:34 INFO 140024126494528] Epoch[82] Batch[0] avg_epoch_loss=0.377145\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:34 INFO 140024126494528] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.377145141363\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:34 INFO 140024126494528] Epoch[82] Batch[5] avg_epoch_loss=0.334014\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:34 INFO 140024126494528] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.334013864398\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:34 INFO 140024126494528] Epoch[82] Batch [5]#011Speed: 1030.13 samples/sec#011loss=0.334014\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] Epoch[82] Batch[10] avg_epoch_loss=0.256682\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.163882911205\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] Epoch[82] Batch [10]#011Speed: 336.62 samples/sec#011loss=0.163883\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2535.439968109131, \"sum\": 2535.439968109131, \"min\": 2535.439968109131}}, \"EndTime\": 1591332575.416679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332572.880798}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.354625182 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.256681612947\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:35 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_a67895bf-aa27-4f4c-b1ff-e86d5c979740-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.739151000976562, \"sum\": 19.739151000976562, \"min\": 19.739151000976562}}, \"EndTime\": 1591332575.437015, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332575.416753}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:36 INFO 140024126494528] Epoch[83] Batch[0] avg_epoch_loss=0.331109\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:36 INFO 140024126494528] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.331109285355\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] Epoch[83] Batch[5] avg_epoch_loss=0.392268\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.39226787289\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] Epoch[83] Batch [5]#011Speed: 1052.04 samples/sec#011loss=0.392268\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2549.5498180389404, \"sum\": 2549.5498180389404, \"min\": 2549.5498180389404}}, \"EndTime\": 1591332577.986702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332575.437091}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.014333326 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.392540815473\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:37 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:39 INFO 140024126494528] Epoch[84] Batch[0] avg_epoch_loss=0.355381\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.355380833149\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:39 INFO 140024126494528] Epoch[84] Batch[5] avg_epoch_loss=0.325744\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:39 INFO 140024126494528] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.325743697584\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:39 INFO 140024126494528] Epoch[84] Batch [5]#011Speed: 1046.59 samples/sec#011loss=0.325744\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] Epoch[84] Batch[10] avg_epoch_loss=0.326558\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=0.327534499764\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] Epoch[84] Batch [10]#011Speed: 331.00 samples/sec#011loss=0.327534\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2568.9640045166016, \"sum\": 2568.9640045166016, \"min\": 2568.9640045166016}}, \"EndTime\": 1591332580.556169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332577.986772}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=258.847105291 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.326557698575\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:40 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:41 INFO 140024126494528] Epoch[85] Batch[0] avg_epoch_loss=0.406985\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:41 INFO 140024126494528] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.406985074282\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:42 INFO 140024126494528] Epoch[85] Batch[5] avg_epoch_loss=0.298407\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:42 INFO 140024126494528] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.298407256603\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:42 INFO 140024126494528] Epoch[85] Batch [5]#011Speed: 1062.26 samples/sec#011loss=0.298407\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] Epoch[85] Batch[10] avg_epoch_loss=0.341772\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.393809375167\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] Epoch[85] Batch [10]#011Speed: 323.59 samples/sec#011loss=0.393809\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2594.8259830474854, \"sum\": 2594.8259830474854, \"min\": 2594.8259830474854}}, \"EndTime\": 1591332583.151541, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332580.556251}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.185422095 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.34177185595\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:43 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:44 INFO 140024126494528] Epoch[86] Batch[0] avg_epoch_loss=0.365272\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.365271508694\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:44 INFO 140024126494528] Epoch[86] Batch[5] avg_epoch_loss=0.316611\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:44 INFO 140024126494528] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.316611491144\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:44 INFO 140024126494528] Epoch[86] Batch [5]#011Speed: 1066.56 samples/sec#011loss=0.316611\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] Epoch[86] Batch[10] avg_epoch_loss=0.269893\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.213831575215\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] Epoch[86] Batch [10]#011Speed: 326.45 samples/sec#011loss=0.213832\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2579.988956451416, \"sum\": 2579.988956451416, \"min\": 2579.988956451416}}, \"EndTime\": 1591332585.732012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332583.151616}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=253.479431578 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.26989334754\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:45 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:47 INFO 140024126494528] Epoch[87] Batch[0] avg_epoch_loss=0.483371\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:47 INFO 140024126494528] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.483371198177\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:47 INFO 140024126494528] Epoch[87] Batch[5] avg_epoch_loss=0.356986\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:47 INFO 140024126494528] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.3569859912\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:47 INFO 140024126494528] Epoch[87] Batch [5]#011Speed: 982.93 samples/sec#011loss=0.356986\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:48 INFO 140024126494528] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2523.3700275421143, \"sum\": 2523.3700275421143, \"min\": 2523.3700275421143}}, \"EndTime\": 1591332588.255909, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332585.732084}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:48 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=242.125089661 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:48 INFO 140024126494528] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:48 INFO 140024126494528] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.303881135583\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:48 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:49 INFO 140024126494528] Epoch[88] Batch[0] avg_epoch_loss=0.452540\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.452540427446\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:49 INFO 140024126494528] Epoch[88] Batch[5] avg_epoch_loss=0.322594\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:49 INFO 140024126494528] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.322594230374\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:49 INFO 140024126494528] Epoch[88] Batch [5]#011Speed: 1067.15 samples/sec#011loss=0.322594\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] Epoch[88] Batch[10] avg_epoch_loss=0.328846\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.336347854137\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] Epoch[88] Batch [10]#011Speed: 325.36 samples/sec#011loss=0.336348\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2585.775852203369, \"sum\": 2585.775852203369, \"min\": 2585.775852203369}}, \"EndTime\": 1591332590.842232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332588.255991}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=262.192435992 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.328845877539\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:50 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:52 INFO 140024126494528] Epoch[89] Batch[0] avg_epoch_loss=0.373752\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:52 INFO 140024126494528] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.373751699924\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:52 INFO 140024126494528] Epoch[89] Batch[5] avg_epoch_loss=0.254293\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:52 INFO 140024126494528] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.254292768737\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:52 INFO 140024126494528] Epoch[89] Batch [5]#011Speed: 1039.44 samples/sec#011loss=0.254293\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] Epoch[89] Batch[10] avg_epoch_loss=0.245307\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.234523303807\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] Epoch[89] Batch [10]#011Speed: 332.62 samples/sec#011loss=0.234523\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2611.104965209961, \"sum\": 2611.104965209961, \"min\": 2611.104965209961}}, \"EndTime\": 1591332593.453826, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332590.842307}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.755220635 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.245306648314\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:53 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_c271a2f3-037f-4fd2-9fbd-311e2698921c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.313024520874023, \"sum\": 20.313024520874023, \"min\": 20.313024520874023}}, \"EndTime\": 1591332593.474717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332593.453905}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:54 INFO 140024126494528] Epoch[90] Batch[0] avg_epoch_loss=0.254626\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:54 INFO 140024126494528] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.254625946283\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:55 INFO 140024126494528] Epoch[90] Batch[5] avg_epoch_loss=0.258099\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:55 INFO 140024126494528] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.258099041879\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:55 INFO 140024126494528] Epoch[90] Batch [5]#011Speed: 1052.69 samples/sec#011loss=0.258099\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] Epoch[90] Batch[10] avg_epoch_loss=0.214772\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=0.162780611962\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] Epoch[90] Batch [10]#011Speed: 331.60 samples/sec#011loss=0.162781\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2553.6129474639893, \"sum\": 2553.6129474639893, \"min\": 2553.6129474639893}}, \"EndTime\": 1591332596.02863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332593.474956}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=251.396482224 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.214772482826\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:56 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_d9f32498-1a2a-460f-9081-d573ca94aaa4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.730972290039062, \"sum\": 20.730972290039062, \"min\": 20.730972290039062}}, \"EndTime\": 1591332596.049958, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332596.028711}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:57 INFO 140024126494528] Epoch[91] Batch[0] avg_epoch_loss=0.290258\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:57 INFO 140024126494528] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.290257960558\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:57 INFO 140024126494528] Epoch[91] Batch[5] avg_epoch_loss=0.241971\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:57 INFO 140024126494528] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.241970735292\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:57 INFO 140024126494528] Epoch[91] Batch [5]#011Speed: 1040.20 samples/sec#011loss=0.241971\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] Epoch[91] Batch[10] avg_epoch_loss=0.273906\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.312228012085\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] Epoch[91] Batch [10]#011Speed: 330.09 samples/sec#011loss=0.312228\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2566.176176071167, \"sum\": 2566.176176071167, \"min\": 2566.176176071167}}, \"EndTime\": 1591332598.616255, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332596.050024}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=256.012795541 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.273905861107\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:58 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:59 INFO 140024126494528] Epoch[92] Batch[0] avg_epoch_loss=0.243796\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:49:59 INFO 140024126494528] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.243796497583\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:00 INFO 140024126494528] Epoch[92] Batch[5] avg_epoch_loss=0.244974\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:00 INFO 140024126494528] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.244974342485\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:00 INFO 140024126494528] Epoch[92] Batch [5]#011Speed: 1046.09 samples/sec#011loss=0.244974\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] Epoch[92] Batch[10] avg_epoch_loss=0.386651\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.556662681699\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] Epoch[92] Batch [10]#011Speed: 329.67 samples/sec#011loss=0.556663\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2594.4180488586426, \"sum\": 2594.4180488586426, \"min\": 2594.4180488586426}}, \"EndTime\": 1591332601.211185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332598.616321}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=247.057565618 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.38665086031\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:01 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:02 INFO 140024126494528] Epoch[93] Batch[0] avg_epoch_loss=0.275504\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.275503546\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:02 INFO 140024126494528] Epoch[93] Batch[5] avg_epoch_loss=0.202481\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:02 INFO 140024126494528] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.202481036385\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:02 INFO 140024126494528] Epoch[93] Batch [5]#011Speed: 990.47 samples/sec#011loss=0.202481\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2521.662950515747, \"sum\": 2521.662950515747, \"min\": 2521.662950515747}}, \"EndTime\": 1591332603.733401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332601.211265}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=250.618522096 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.210311114788\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:03 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_2ac999fb-7cfa-4392-90f8-68118aee4423-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.005868911743164, \"sum\": 21.005868911743164, \"min\": 21.005868911743164}}, \"EndTime\": 1591332603.75503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332603.733463}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:05 INFO 140024126494528] Epoch[94] Batch[0] avg_epoch_loss=0.147763\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:05 INFO 140024126494528] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.147763341665\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:05 INFO 140024126494528] Epoch[94] Batch[5] avg_epoch_loss=0.188663\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:05 INFO 140024126494528] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.188663456589\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:05 INFO 140024126494528] Epoch[94] Batch [5]#011Speed: 1049.82 samples/sec#011loss=0.188663\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2487.4510765075684, \"sum\": 2487.4510765075684, \"min\": 2487.4510765075684}}, \"EndTime\": 1591332606.242624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332603.755111}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=252.055353813 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.204532223195\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:06 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_31f36f12-2d13-4292-aa48-39cb78d6da9c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.904945373535156, \"sum\": 21.904945373535156, \"min\": 21.904945373535156}}, \"EndTime\": 1591332606.26511, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332606.242685}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:07 INFO 140024126494528] Epoch[95] Batch[0] avg_epoch_loss=0.231373\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.231373041868\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:07 INFO 140024126494528] Epoch[95] Batch[5] avg_epoch_loss=0.220498\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:07 INFO 140024126494528] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.220498201748\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:07 INFO 140024126494528] Epoch[95] Batch [5]#011Speed: 1052.91 samples/sec#011loss=0.220498\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:08 INFO 140024126494528] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2489.4418716430664, \"sum\": 2489.4418716430664, \"min\": 2489.4418716430664}}, \"EndTime\": 1591332608.754685, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332606.265183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:08 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=249.041003822 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:08 INFO 140024126494528] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:08 INFO 140024126494528] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.208932794631\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:08 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:10 INFO 140024126494528] Epoch[96] Batch[0] avg_epoch_loss=0.219156\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:10 INFO 140024126494528] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.219156354666\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:10 INFO 140024126494528] Epoch[96] Batch[5] avg_epoch_loss=0.204974\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:10 INFO 140024126494528] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.204973543684\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:10 INFO 140024126494528] Epoch[96] Batch [5]#011Speed: 1054.09 samples/sec#011loss=0.204974\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] Epoch[96] Batch[10] avg_epoch_loss=0.212318\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.221131664515\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] Epoch[96] Batch [10]#011Speed: 331.82 samples/sec#011loss=0.221132\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2554.727077484131, \"sum\": 2554.727077484131, \"min\": 2554.727077484131}}, \"EndTime\": 1591332611.30997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332608.754762}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=261.07360312 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.212318144061\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:11 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:12 INFO 140024126494528] Epoch[97] Batch[0] avg_epoch_loss=0.217224\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.217224225402\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:12 INFO 140024126494528] Epoch[97] Batch[5] avg_epoch_loss=0.198721\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:12 INFO 140024126494528] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.198720629017\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:12 INFO 140024126494528] Epoch[97] Batch [5]#011Speed: 1039.56 samples/sec#011loss=0.198721\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] Epoch[97] Batch[10] avg_epoch_loss=0.177495\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=0.152024900168\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] Epoch[97] Batch [10]#011Speed: 327.37 samples/sec#011loss=0.152025\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2576.4548778533936, \"sum\": 2576.4548778533936, \"min\": 2576.4548778533936}}, \"EndTime\": 1591332613.886887, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332611.310042}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=259.648017002 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.177495297722\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:13 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_acfd02d7-84d0-48e8-b7bc-0c66bad72fe0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.07516670227051, \"sum\": 32.07516670227051, \"min\": 32.07516670227051}}, \"EndTime\": 1591332613.919491, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332613.886962}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:15 INFO 140024126494528] Epoch[98] Batch[0] avg_epoch_loss=0.167025\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:15 INFO 140024126494528] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.167025148869\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:15 INFO 140024126494528] Epoch[98] Batch[5] avg_epoch_loss=0.162031\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:15 INFO 140024126494528] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.162030895551\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:15 INFO 140024126494528] Epoch[98] Batch [5]#011Speed: 989.65 samples/sec#011loss=0.162031\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] Epoch[98] Batch[10] avg_epoch_loss=0.153155\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=0.142504461017\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] Epoch[98] Batch [10]#011Speed: 319.23 samples/sec#011loss=0.142504\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2619.819164276123, \"sum\": 2619.819164276123, \"min\": 2619.819164276123}}, \"EndTime\": 1591332616.539434, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332613.91956}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=261.075769747 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.15315524349\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:16 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/state_07df4022-a21b-47f7-83a5-023e37806ee4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.713899612426758, \"sum\": 22.713899612426758, \"min\": 22.713899612426758}}, \"EndTime\": 1591332616.562677, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332616.539509}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:17 INFO 140024126494528] Epoch[99] Batch[0] avg_epoch_loss=0.165419\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:17 INFO 140024126494528] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.165419191122\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:18 INFO 140024126494528] Epoch[99] Batch[5] avg_epoch_loss=0.208808\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:18 INFO 140024126494528] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.208807726701\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:18 INFO 140024126494528] Epoch[99] Batch [5]#011Speed: 1006.62 samples/sec#011loss=0.208808\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Epoch[99] Batch[10] avg_epoch_loss=0.215806\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=0.224204630405\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Epoch[99] Batch [10]#011Speed: 317.98 samples/sec#011loss=0.224205\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2638.5951042175293, \"sum\": 2638.5951042175293, \"min\": 2638.5951042175293}}, \"EndTime\": 1591332619.201402, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332616.562748}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] #throughput_metric: host=algo-1, train throughput=246.332864545 records/second\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.215806319294\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Final loss: 0.15315524349 (occurred at epoch 98)\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] #quality_metric: host=algo-1, train final_loss <loss>=0.15315524349\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 WARNING 140024126494528] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 263.65208625793457, \"sum\": 263.65208625793457, \"min\": 263.65208625793457}}, \"EndTime\": 1591332619.466031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332619.201478}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 322.16715812683105, \"sum\": 322.16715812683105, \"min\": 322.16715812683105}}, \"EndTime\": 1591332619.524513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332619.466102}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 9.283065795898438, \"sum\": 9.283065795898438, \"min\": 9.283065795898438}}, \"EndTime\": 1591332619.533918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332619.524587}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:19 INFO 140024126494528] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03409385681152344, \"sum\": 0.03409385681152344, \"min\": 0.03409385681152344}}, \"EndTime\": 1591332619.534647, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332619.533969}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:23 INFO 140024126494528] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:26 INFO 140024126494528] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:30 INFO 140024126494528] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:34 INFO 140024126494528] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:37 INFO 140024126494528] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:41 INFO 140024126494528] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:45 INFO 140024126494528] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 28126.983880996704, \"sum\": 28126.983880996704, \"min\": 28126.983880996704}}, \"EndTime\": 1591332647.661601, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332619.534694}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, RMSE): 0.708135977678\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, mean_absolute_QuantileLoss): 1714.9514801402047\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, mean_wQuantileLoss): 0.4332873876049026\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.1]): 0.47252684169406084\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.2]): 0.5134540666418999\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.3]): 0.5124819353436736\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.4]): 0.490987843704954\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.5]): 0.45662380041783224\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.6]): 0.41933553075994834\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.7]): 0.3915031188549055\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.8]): 0.3556615291554866\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #test_score (algo-1, wQuantileLoss[0.9]): 0.28701182187136265\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.433287387605\u001b[0m\n",
      "\u001b[34m[06/05/2020 04:50:47 INFO 140024126494528] #quality_metric: host=algo-1, test RMSE <loss>=0.708135977678\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 298946.9790458679, \"sum\": 298946.9790458679, \"min\": 298946.9790458679}, \"setuptime\": {\"count\": 1, \"max\": 8.192062377929688, \"sum\": 8.192062377929688, \"min\": 8.192062377929688}}, \"EndTime\": 1591332647.682056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591332647.661715}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-05 04:50:54 Uploading - Uploading generated training model\n",
      "2020-06-05 04:50:54 Completed - Training job completed\n",
      "Training seconds: 365\n",
      "Billable seconds: 365\n",
      "CPU times: user 1.23 s, sys: 69 ms, total: 1.29 s\n",
      "Wall time: 8min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator_ind_cat.fit(inputs=data_channels, wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor_ind_cat = estimator_ind_cat.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor_indicator = sagemaker.predictor.RealTimePredictor(endpoint='MLEND-Capstone-Project-2020-06-03-23-30-23-410')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:89: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  date_start = date_pred-50\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:94: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  result_df['prediction'] = pred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>-1.389812</td>\n",
       "      <td>7.484644</td>\n",
       "      <td>3.785415</td>\n",
       "      <td>-4.228811</td>\n",
       "      <td>0.443095</td>\n",
       "      <td>1.235278</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1       PC2       PC3  \\\n",
       "Date       Ticker                                                      \n",
       "2019-01-02 AAPL      0.03987  0.043087 -1.389812  7.484644  3.785415   \n",
       "\n",
       "                        PC4       PC5       PC6  target  prediction  \n",
       "Date       Ticker                                                    \n",
       "2019-01-02 AAPL   -4.228811  0.443095  1.235278      -1          -1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('AAPL', '2019-01-02',stock_ind_cat_data,predictor_ind_cat, dynamic_feat, cat['AAPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:89: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  date_start = date_pred-50\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:94: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  result_df['prediction'] = pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.746031746031746,             target  prediction\n",
       " 2019-01-02      -1          -1\n",
       " 2019-01-03       1           0\n",
       " 2019-01-04       1           1\n",
       " 2019-01-07       1           1\n",
       " 2019-01-08       1           1\n",
       " 2019-01-09       0           1\n",
       " 2019-01-10      -1           0\n",
       " 2019-01-11       1          -1\n",
       " 2019-01-14       1           1\n",
       " 2019-01-15       1           1\n",
       " 2019-01-16       0           1\n",
       " 2019-01-17       0           0\n",
       " 2019-01-18      -1           0\n",
       " 2019-01-22       1          -1\n",
       " 2019-01-23       1           1\n",
       " 2019-01-24       1           1\n",
       " 2019-01-25       1           1\n",
       " 2019-01-28       1           1\n",
       " 2019-01-29       1           1\n",
       " 2019-01-30       1           1\n",
       " 2019-01-31       1           1\n",
       " 2019-02-01       1           1\n",
       " 2019-02-04       0           1\n",
       " 2019-02-05       0           0\n",
       " 2019-02-06       0           0\n",
       " 2019-02-07       0           0\n",
       " 2019-02-08       0           0\n",
       " 2019-02-11       0           0\n",
       " 2019-02-12       0           0\n",
       " 2019-02-13       1           0\n",
       " ...            ...         ...\n",
       " 2019-11-18       0           0\n",
       " 2019-11-19       0           0\n",
       " 2019-11-20       0           0\n",
       " 2019-11-21       0           0\n",
       " 2019-11-22       0           0\n",
       " 2019-11-25      -1           0\n",
       " 2019-11-26       1          -1\n",
       " 2019-11-27      -1           1\n",
       " 2019-11-29      -1          -1\n",
       " 2019-12-02       1          -1\n",
       " 2019-12-03       1           1\n",
       " 2019-12-04       1           1\n",
       " 2019-12-05       1           1\n",
       " 2019-12-06       1           1\n",
       " 2019-12-09       1           1\n",
       " 2019-12-10       1           1\n",
       " 2019-12-11       1           1\n",
       " 2019-12-12       1           1\n",
       " 2019-12-13       1           1\n",
       " 2019-12-16       1           1\n",
       " 2019-12-17       1           1\n",
       " 2019-12-18       1           1\n",
       " 2019-12-19       1           1\n",
       " 2019-12-20       1           1\n",
       " 2019-12-23       1           1\n",
       " 2019-12-24       1           1\n",
       " 2019-12-26       0           1\n",
       " 2019-12-27       0           0\n",
       " 2019-12-30       0           0\n",
       " 2019-12-31       0           0\n",
       " \n",
       " [252 rows x 2 columns])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "get_prediction_accuracy('AAPL', date_index, stock_ind_cat_data,predictor_ind_cat, dynamic_feat, cat['AAPL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
