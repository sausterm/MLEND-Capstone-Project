{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_indicator\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_indicator\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_indicator_data = pd.read_csv('stock_indicator_data.csv',parse_dates=True, index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_tickers():\n",
    "    resp = requests.get('https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "    soup = bs.BeautifulSoup(resp.text, \"lxml\")\n",
    "    table = soup.find('table', {'class':'wikitable sortable'})\n",
    "    tickers = []\n",
    "    for row in table.findAll('tr')[1:]:\n",
    "        ticker = row.findAll('td')[0].text\n",
    "        ticker = ticker[:-1]\n",
    "        tickers.append(ticker)\n",
    "\n",
    "    \n",
    "    tickers.sort()\n",
    "    tickers.remove('BF.B')\n",
    "    tickers.remove('BRK.B')\n",
    "    tickers.remove('CARR')\n",
    "    tickers.remove('DPZ')\n",
    "    tickers.remove('DXCM')\n",
    "    tickers.remove('OTIS')\n",
    "    tickers.remove('WST')\n",
    "    \n",
    "    return tickers\n",
    "tickers = get_sp500_tickers()\n",
    "\n",
    "# we use 2 hour frequency for the time series\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_indicator_data.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_dataset(filename, data): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for d in data:\n",
    "            json_line = json.dumps(d) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line) \n",
    "            \n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_data[0]\n",
    "#test_data[0]['dynamic_feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + (2*k * prediction_length)].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']][ts.index[0][0]:end_training + (2*k * prediction_length)].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 1.35 s, total: 1min 11s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train_indicator.json\", training_data)\n",
    "write_json_dataset(\"test_indicator.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_indicator/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_indicator/test/test.json\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train_indicator.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test_indicator.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_indicator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_indicator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-03 17:01:46 Starting - Starting the training job...\n",
      "2020-06-03 17:01:49 Starting - Launching requested ML instances......\n",
      "2020-06-03 17:02:50 Starting - Preparing the instances for training...\n",
      "2020-06-03 17:03:46 Downloading - Downloading input data......\n",
      "2020-06-03 17:04:42 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:45 INFO 140291508827968] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] Real time series\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:46 INFO 140291508827968] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] Real time series\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] number of observations: 10562640\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] mean target length: 2151\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] min/mean/max target: -1.0/0.0763878159248/1.0\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] mean abs(target): 0.697724527202\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] nvidia-smi took: 0.025190114975 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 229.92897033691406, \"sum\": 229.92897033691406, \"min\": 229.92897033691406}}, \"EndTime\": 1591203899.428358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203899.197591}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:04:59 INFO 140291508827968] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 428.3430576324463, \"sum\": 428.3430576324463, \"min\": 428.3430576324463}}, \"EndTime\": 1591203899.626052, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203899.42843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:01 INFO 140291508827968] Epoch[0] Batch[0] avg_epoch_loss=1.332305\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.33230507374\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:01 INFO 140291508827968] Epoch[0] Batch[5] avg_epoch_loss=1.361877\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.36187670628\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:01 INFO 140291508827968] Epoch[0] Batch [5]#011Speed: 957.08 samples/sec#011loss=1.361877\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] Epoch[0] Batch[10] avg_epoch_loss=1.342990\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=1.32032601833\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] Epoch[0] Batch [10]#011Speed: 299.68 samples/sec#011loss=1.320326\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 2835.4721069335938, \"sum\": 2835.4721069335938, \"min\": 2835.4721069335938}}, \"EndTime\": 1591203902.461686, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203899.626117}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=233.811227956 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.34299002994\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:02 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_dd23db2f-fd99-4b61-9fdd-f35d674d7f7e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.315927505493164, \"sum\": 26.315927505493164, \"min\": 26.315927505493164}}, \"EndTime\": 1591203902.488736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203902.46179}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:03 INFO 140291508827968] Epoch[1] Batch[0] avg_epoch_loss=1.303546\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.30354619026\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:04 INFO 140291508827968] Epoch[1] Batch[5] avg_epoch_loss=1.288144\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.28814359506\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:04 INFO 140291508827968] Epoch[1] Batch [5]#011Speed: 974.10 samples/sec#011loss=1.288144\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] Epoch[1] Batch[10] avg_epoch_loss=1.289866\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=1.29193232059\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] Epoch[1] Batch [10]#011Speed: 310.30 samples/sec#011loss=1.291932\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2752.979040145874, \"sum\": 2752.979040145874, \"min\": 2752.979040145874}}, \"EndTime\": 1591203905.241859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203902.488813}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=239.366186975 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.28986574303\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:05 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_b6bace9a-2dc1-4246-98af-b8a98b22ca9c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.392915725708008, \"sum\": 23.392915725708008, \"min\": 23.392915725708008}}, \"EndTime\": 1591203905.265849, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203905.241939}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:06 INFO 140291508827968] Epoch[2] Batch[0] avg_epoch_loss=1.273651\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.273650527\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] Epoch[2] Batch[5] avg_epoch_loss=1.267308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.26730831464\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] Epoch[2] Batch [5]#011Speed: 327.53 samples/sec#011loss=1.267308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2545.44997215271, \"sum\": 2545.44997215271, \"min\": 2545.44997215271}}, \"EndTime\": 1591203907.81146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203905.265943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=238.453189486 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.21504362822\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:07 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_5c2bfb99-7a0e-460d-9793-6540cb8ac98a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.999027252197266, \"sum\": 19.999027252197266, \"min\": 19.999027252197266}}, \"EndTime\": 1591203907.83218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203907.811545}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:09 INFO 140291508827968] Epoch[3] Batch[0] avg_epoch_loss=1.276423\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.27642261982\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:09 INFO 140291508827968] Epoch[3] Batch[5] avg_epoch_loss=1.287827\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.28782691558\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:09 INFO 140291508827968] Epoch[3] Batch [5]#011Speed: 1022.93 samples/sec#011loss=1.287827\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] Epoch[3] Batch[10] avg_epoch_loss=1.285804\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.28337755203\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] Epoch[3] Batch [10]#011Speed: 323.37 samples/sec#011loss=1.283378\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2609.7891330718994, \"sum\": 2609.7891330718994, \"min\": 2609.7891330718994}}, \"EndTime\": 1591203910.442101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203907.832248}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.797963964 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.2858044776\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:10 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:11 INFO 140291508827968] Epoch[4] Batch[0] avg_epoch_loss=1.251054\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.25105416775\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:12 INFO 140291508827968] Epoch[4] Batch[5] avg_epoch_loss=1.225007\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.22500743469\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:12 INFO 140291508827968] Epoch[4] Batch [5]#011Speed: 1029.16 samples/sec#011loss=1.225007\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] Epoch[4] Batch[10] avg_epoch_loss=1.257645\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.2968091011\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] Epoch[4] Batch [10]#011Speed: 321.38 samples/sec#011loss=1.296809\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2615.9331798553467, \"sum\": 2615.9331798553467, \"min\": 2615.9331798553467}}, \"EndTime\": 1591203913.058538, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203910.442183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.523671013 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.25764455579\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:13 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:14 INFO 140291508827968] Epoch[5] Batch[0] avg_epoch_loss=1.187041\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.18704080582\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:14 INFO 140291508827968] Epoch[5] Batch[5] avg_epoch_loss=1.181077\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.18107744058\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:14 INFO 140291508827968] Epoch[5] Batch [5]#011Speed: 1010.39 samples/sec#011loss=1.181077\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2515.8779621124268, \"sum\": 2515.8779621124268, \"min\": 2515.8779621124268}}, \"EndTime\": 1591203915.575029, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203913.058622}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=253.178831266 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.18688458204\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:15 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_94cf3163-edb9-4a3b-ba4a-e83a1ec0e0e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.032085418701172, \"sum\": 31.032085418701172, \"min\": 31.032085418701172}}, \"EndTime\": 1591203915.606669, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203915.575117}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:16 INFO 140291508827968] Epoch[6] Batch[0] avg_epoch_loss=1.175555\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.17555546761\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:17 INFO 140291508827968] Epoch[6] Batch[5] avg_epoch_loss=1.181390\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:17 INFO 140291508827968] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.18139036496\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:17 INFO 140291508827968] Epoch[6] Batch [5]#011Speed: 1031.34 samples/sec#011loss=1.181390\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2571.5901851654053, \"sum\": 2571.5901851654053, \"min\": 2571.5901851654053}}, \"EndTime\": 1591203918.178385, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203915.606731}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=238.750707917 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.14342640042\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:18 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_4454a1d5-901a-4b91-96ba-3f6cd4297748-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.677825927734375, \"sum\": 23.677825927734375, \"min\": 23.677825927734375}}, \"EndTime\": 1591203918.20271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203918.178476}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:19 INFO 140291508827968] Epoch[7] Batch[0] avg_epoch_loss=1.179084\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.17908394337\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:19 INFO 140291508827968] Epoch[7] Batch[5] avg_epoch_loss=1.156152\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.15615200996\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:19 INFO 140291508827968] Epoch[7] Batch [5]#011Speed: 970.89 samples/sec#011loss=1.156152\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2583.7481021881104, \"sum\": 2583.7481021881104, \"min\": 2583.7481021881104}}, \"EndTime\": 1591203920.786597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203918.202781}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=239.94931707 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.12375304103\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:20 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_549361f2-f736-472b-95e1-1fa1fb0c0cce-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.068117141723633, \"sum\": 24.068117141723633, \"min\": 24.068117141723633}}, \"EndTime\": 1591203920.811325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203920.786688}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:22 INFO 140291508827968] Epoch[8] Batch[0] avg_epoch_loss=1.130908\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.13090777397\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:22 INFO 140291508827968] Epoch[8] Batch[5] avg_epoch_loss=1.137324\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.13732397556\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:22 INFO 140291508827968] Epoch[8] Batch [5]#011Speed: 1020.56 samples/sec#011loss=1.137324\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:23 INFO 140291508827968] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2575.531005859375, \"sum\": 2575.531005859375, \"min\": 2575.531005859375}}, \"EndTime\": 1591203923.38702, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203920.811386}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:23 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.651737404 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:23 INFO 140291508827968] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:23 INFO 140291508827968] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.13041222095\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:23 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:24 INFO 140291508827968] Epoch[9] Batch[0] avg_epoch_loss=1.076224\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.07622408867\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] Epoch[9] Batch[5] avg_epoch_loss=1.098471\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.09847148259\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] Epoch[9] Batch [5]#011Speed: 998.24 samples/sec#011loss=1.098471\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2559.1020584106445, \"sum\": 2559.1020584106445, \"min\": 2559.1020584106445}}, \"EndTime\": 1591203925.946715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203923.387116}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.386116167 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.09183876514\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:25 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_6a59daca-4797-4044-9266-ebcbbc1d2f3c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.673099517822266, \"sum\": 29.673099517822266, \"min\": 29.673099517822266}}, \"EndTime\": 1591203925.977024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203925.946803}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:27 INFO 140291508827968] Epoch[10] Batch[0] avg_epoch_loss=1.073368\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.07336843014\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:27 INFO 140291508827968] Epoch[10] Batch[5] avg_epoch_loss=1.101997\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.10199652116\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:27 INFO 140291508827968] Epoch[10] Batch [5]#011Speed: 1020.34 samples/sec#011loss=1.101997\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2542.3591136932373, \"sum\": 2542.3591136932373, \"min\": 2542.3591136932373}}, \"EndTime\": 1591203928.51949, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203925.977082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=250.150845167 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.09067224264\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:28 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_7da54a57-0501-4133-b380-44590764e0b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.189212799072266, \"sum\": 21.189212799072266, \"min\": 21.189212799072266}}, \"EndTime\": 1591203928.541314, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203928.519567}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:29 INFO 140291508827968] Epoch[11] Batch[0] avg_epoch_loss=1.101334\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:29 INFO 140291508827968] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.10133385658\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:30 INFO 140291508827968] Epoch[11] Batch[5] avg_epoch_loss=1.050176\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.05017604431\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:30 INFO 140291508827968] Epoch[11] Batch [5]#011Speed: 1032.21 samples/sec#011loss=1.050176\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] Epoch[11] Batch[10] avg_epoch_loss=1.047949\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.0452757597\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] Epoch[11] Batch [10]#011Speed: 329.47 samples/sec#011loss=1.045276\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2568.814992904663, \"sum\": 2568.814992904663, \"min\": 2568.814992904663}}, \"EndTime\": 1591203931.110249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203928.541375}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=257.305173788 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.04794864221\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:31 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_5a4a4ac0-8c68-4682-854e-fb039d94c734-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.21202850341797, \"sum\": 22.21202850341797, \"min\": 22.21202850341797}}, \"EndTime\": 1591203931.133079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203931.110328}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:32 INFO 140291508827968] Epoch[12] Batch[0] avg_epoch_loss=1.002793\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.00279331207\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:32 INFO 140291508827968] Epoch[12] Batch[5] avg_epoch_loss=1.020302\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.02030188839\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:32 INFO 140291508827968] Epoch[12] Batch [5]#011Speed: 1005.88 samples/sec#011loss=1.020302\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2531.9130420684814, \"sum\": 2531.9130420684814, \"min\": 2531.9130420684814}}, \"EndTime\": 1591203933.665131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203931.133154}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.651137632 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.02517072558\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:33 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_10af0510-6dd5-42e8-9698-e45509b6c7c9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.283077239990234, \"sum\": 22.283077239990234, \"min\": 22.283077239990234}}, \"EndTime\": 1591203933.688079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203933.665222}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:34 INFO 140291508827968] Epoch[13] Batch[0] avg_epoch_loss=1.004244\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:34 INFO 140291508827968] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.00424361229\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:35 INFO 140291508827968] Epoch[13] Batch[5] avg_epoch_loss=0.964474\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=0.964473704497\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:35 INFO 140291508827968] Epoch[13] Batch [5]#011Speed: 1021.73 samples/sec#011loss=0.964474\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] Epoch[13] Batch[10] avg_epoch_loss=0.997037\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=1.0361123085\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] Epoch[13] Batch [10]#011Speed: 321.87 samples/sec#011loss=1.036112\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2616.8370246887207, \"sum\": 2616.8370246887207, \"min\": 2616.8370246887207}}, \"EndTime\": 1591203936.305048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203933.68815}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=252.583628461 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=13, train loss <loss>=0.997036706318\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:36 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_b7cf618f-82bc-4ffb-9fa2-d9062d3fba81-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.272016525268555, \"sum\": 20.272016525268555, \"min\": 20.272016525268555}}, \"EndTime\": 1591203936.325937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203936.305129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:37 INFO 140291508827968] Epoch[14] Batch[0] avg_epoch_loss=0.935255\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.935255110264\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:37 INFO 140291508827968] Epoch[14] Batch[5] avg_epoch_loss=0.971344\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=0.971344480912\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:37 INFO 140291508827968] Epoch[14] Batch [5]#011Speed: 1008.02 samples/sec#011loss=0.971344\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] Epoch[14] Batch[10] avg_epoch_loss=0.941890\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=0.906544995308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] Epoch[14] Batch [10]#011Speed: 313.86 samples/sec#011loss=0.906545\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2653.6059379577637, \"sum\": 2653.6059379577637, \"min\": 2653.6059379577637}}, \"EndTime\": 1591203938.979672, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203936.326004}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=247.953268006 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=14, train loss <loss>=0.941890169274\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:38 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:39 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_be224077-98a2-4692-87b5-feadf3e4ba07-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.93297576904297, \"sum\": 29.93297576904297, \"min\": 29.93297576904297}}, \"EndTime\": 1591203939.010193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203938.979755}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:40 INFO 140291508827968] Epoch[15] Batch[0] avg_epoch_loss=0.985102\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=0.985101521015\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:40 INFO 140291508827968] Epoch[15] Batch[5] avg_epoch_loss=0.952339\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.952339470387\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:40 INFO 140291508827968] Epoch[15] Batch [5]#011Speed: 1029.48 samples/sec#011loss=0.952339\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] Epoch[15] Batch[10] avg_epoch_loss=0.931652\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=0.90682746172\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] Epoch[15] Batch [10]#011Speed: 322.65 samples/sec#011loss=0.906827\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2605.0899028778076, \"sum\": 2605.0899028778076, \"min\": 2605.0899028778076}}, \"EndTime\": 1591203941.615412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203939.01026}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=249.11621059 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.93165219372\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:41 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_1eaba1ec-a610-43bb-a294-b329905623ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.793075561523438, \"sum\": 25.793075561523438, \"min\": 25.793075561523438}}, \"EndTime\": 1591203941.641784, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203941.615495}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:42 INFO 140291508827968] Epoch[16] Batch[0] avg_epoch_loss=0.925356\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:42 INFO 140291508827968] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.925355911255\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:43 INFO 140291508827968] Epoch[16] Batch[5] avg_epoch_loss=0.879858\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:43 INFO 140291508827968] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.879858116309\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:43 INFO 140291508827968] Epoch[16] Batch [5]#011Speed: 908.15 samples/sec#011loss=0.879858\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2565.377950668335, \"sum\": 2565.377950668335, \"min\": 2565.377950668335}}, \"EndTime\": 1591203944.207287, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203941.641844}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=240.49657937 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.894037485123\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:44 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_96bdda4c-98dd-4db9-bbda-0978fa325229-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.96685218811035, \"sum\": 32.96685218811035, \"min\": 32.96685218811035}}, \"EndTime\": 1591203944.240877, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203944.207394}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:45 INFO 140291508827968] Epoch[17] Batch[0] avg_epoch_loss=0.936777\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.936776638031\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:45 INFO 140291508827968] Epoch[17] Batch[5] avg_epoch_loss=0.899042\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.899041642745\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:45 INFO 140291508827968] Epoch[17] Batch [5]#011Speed: 921.78 samples/sec#011loss=0.899042\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] Epoch[17] Batch[10] avg_epoch_loss=0.890277\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.87975935936\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] Epoch[17] Batch [10]#011Speed: 321.15 samples/sec#011loss=0.879759\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2680.2759170532227, \"sum\": 2680.2759170532227, \"min\": 2680.2759170532227}}, \"EndTime\": 1591203946.921297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203944.240956}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=248.844107873 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.890276968479\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:46 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_fc739b71-78d4-4a5b-b8d8-1892f2b54dd1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.382156372070312, \"sum\": 30.382156372070312, \"min\": 30.382156372070312}}, \"EndTime\": 1591203946.952205, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203946.921377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:48 INFO 140291508827968] Epoch[18] Batch[0] avg_epoch_loss=0.891160\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.891159772873\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:48 INFO 140291508827968] Epoch[18] Batch[5] avg_epoch_loss=0.891233\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.891233245532\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:48 INFO 140291508827968] Epoch[18] Batch [5]#011Speed: 1016.48 samples/sec#011loss=0.891233\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] Epoch[18] Batch[10] avg_epoch_loss=0.863287\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.829752075672\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] Epoch[18] Batch [10]#011Speed: 317.65 samples/sec#011loss=0.829752\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2645.350933074951, \"sum\": 2645.350933074951, \"min\": 2645.350933074951}}, \"EndTime\": 1591203949.597696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203946.952275}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.679069083 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.863287259232\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:49 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_b2de75dd-83f3-40df-a020-9e7b7e3b1a86-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.528959274291992, \"sum\": 21.528959274291992, \"min\": 21.528959274291992}}, \"EndTime\": 1591203949.619853, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203949.597773}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:50 INFO 140291508827968] Epoch[19] Batch[0] avg_epoch_loss=0.884140\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.884139835835\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:51 INFO 140291508827968] Epoch[19] Batch[5] avg_epoch_loss=0.877935\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:51 INFO 140291508827968] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.877935270468\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:51 INFO 140291508827968] Epoch[19] Batch [5]#011Speed: 1006.01 samples/sec#011loss=0.877935\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] Epoch[19] Batch[10] avg_epoch_loss=0.847092\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=0.810079443455\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] Epoch[19] Batch [10]#011Speed: 320.92 samples/sec#011loss=0.810079\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2592.6060676574707, \"sum\": 2592.6060676574707, \"min\": 2592.6060676574707}}, \"EndTime\": 1591203952.212592, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203949.619918}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.172238714 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.847091712735\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:52 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_309ba66f-9336-481d-b5bb-ea80a91682e7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.296977996826172, \"sum\": 21.296977996826172, \"min\": 21.296977996826172}}, \"EndTime\": 1591203952.234468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203952.212674}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:53 INFO 140291508827968] Epoch[20] Batch[0] avg_epoch_loss=0.847684\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.847684204578\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:53 INFO 140291508827968] Epoch[20] Batch[5] avg_epoch_loss=0.869308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.869308064381\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:53 INFO 140291508827968] Epoch[20] Batch [5]#011Speed: 987.06 samples/sec#011loss=0.869308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:54 INFO 140291508827968] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2544.7781085968018, \"sum\": 2544.7781085968018, \"min\": 2544.7781085968018}}, \"EndTime\": 1591203954.779386, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203952.234539}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:54 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=246.374416462 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:54 INFO 140291508827968] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:54 INFO 140291508827968] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.860703259706\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:54 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:56 INFO 140291508827968] Epoch[21] Batch[0] avg_epoch_loss=0.836088\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.836088418961\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:56 INFO 140291508827968] Epoch[21] Batch[5] avg_epoch_loss=0.829534\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.829534421364\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:56 INFO 140291508827968] Epoch[21] Batch [5]#011Speed: 1017.00 samples/sec#011loss=0.829534\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] Epoch[21] Batch[10] avg_epoch_loss=0.813190\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=0.793576788902\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] Epoch[21] Batch [10]#011Speed: 317.53 samples/sec#011loss=0.793577\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2610.273838043213, \"sum\": 2610.273838043213, \"min\": 2610.273838043213}}, \"EndTime\": 1591203957.390252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203954.779473}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=247.853312405 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.813190042973\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:57 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_c44d0e79-df80-4dee-8ce6-b3663c051e62-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.213054656982422, \"sum\": 21.213054656982422, \"min\": 21.213054656982422}}, \"EndTime\": 1591203957.412058, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203957.390351}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:58 INFO 140291508827968] Epoch[22] Batch[0] avg_epoch_loss=0.925141\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.925140917301\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:59 INFO 140291508827968] Epoch[22] Batch[5] avg_epoch_loss=0.847031\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:59 INFO 140291508827968] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.847031255563\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:05:59 INFO 140291508827968] Epoch[22] Batch [5]#011Speed: 998.29 samples/sec#011loss=0.847031\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] Epoch[22] Batch[10] avg_epoch_loss=0.857491\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=0.870041835308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] Epoch[22] Batch [10]#011Speed: 320.01 samples/sec#011loss=0.870042\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2612.9698753356934, \"sum\": 2612.9698753356934, \"min\": 2612.9698753356934}}, \"EndTime\": 1591203960.025147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203957.412121}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=247.217276302 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.857490609993\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:00 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:01 INFO 140291508827968] Epoch[23] Batch[0] avg_epoch_loss=0.802403\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.802402853966\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] Epoch[23] Batch[5] avg_epoch_loss=0.818506\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.818505525589\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] Epoch[23] Batch [5]#011Speed: 324.05 samples/sec#011loss=0.818506\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2549.9489307403564, \"sum\": 2549.9489307403564, \"min\": 2549.9489307403564}}, \"EndTime\": 1591203962.575598, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203960.025228}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=241.953097356 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.808340013027\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:02 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_04ecedcb-3ebf-4565-9e8d-7e5548853121-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.537065505981445, \"sum\": 21.537065505981445, \"min\": 21.537065505981445}}, \"EndTime\": 1591203962.597721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203962.575688}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:03 INFO 140291508827968] Epoch[24] Batch[0] avg_epoch_loss=0.809147\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.80914658308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:04 INFO 140291508827968] Epoch[24] Batch[5] avg_epoch_loss=0.820750\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.82074974974\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:04 INFO 140291508827968] Epoch[24] Batch [5]#011Speed: 980.11 samples/sec#011loss=0.820750\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] Epoch[24] Batch[10] avg_epoch_loss=0.813962\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.805817675591\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] Epoch[24] Batch [10]#011Speed: 313.32 samples/sec#011loss=0.805818\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2656.224012374878, \"sum\": 2656.224012374878, \"min\": 2656.224012374878}}, \"EndTime\": 1591203965.254065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203962.597783}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=246.955684261 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.813962443308\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:05 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:06 INFO 140291508827968] Epoch[25] Batch[0] avg_epoch_loss=0.838405\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.838405430317\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:06 INFO 140291508827968] Epoch[25] Batch[5] avg_epoch_loss=0.820011\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.820011178652\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:06 INFO 140291508827968] Epoch[25] Batch [5]#011Speed: 1016.58 samples/sec#011loss=0.820011\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] Epoch[25] Batch[10] avg_epoch_loss=0.815868\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=0.810897088051\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] Epoch[25] Batch [10]#011Speed: 323.71 samples/sec#011loss=0.810897\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2601.438045501709, \"sum\": 2601.438045501709, \"min\": 2601.438045501709}}, \"EndTime\": 1591203967.856021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203965.254151}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=255.231465137 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.815868410197\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:07 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:09 INFO 140291508827968] Epoch[26] Batch[0] avg_epoch_loss=0.821819\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.821819186211\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:09 INFO 140291508827968] Epoch[26] Batch[5] avg_epoch_loss=0.810167\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.810167312622\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:09 INFO 140291508827968] Epoch[26] Batch [5]#011Speed: 1007.94 samples/sec#011loss=0.810167\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2590.0847911834717, \"sum\": 2590.0847911834717, \"min\": 2590.0847911834717}}, \"EndTime\": 1591203970.446619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203967.856105}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=233.957308043 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.807771259546\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:10 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_bbe5e7c1-5569-44a8-8fba-5060981fdfd1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.480798721313477, \"sum\": 21.480798721313477, \"min\": 21.480798721313477}}, \"EndTime\": 1591203970.468772, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203970.446709}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:11 INFO 140291508827968] Epoch[27] Batch[0] avg_epoch_loss=0.869582\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.869582474232\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] Epoch[27] Batch[5] avg_epoch_loss=0.817928\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.817928036054\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] Epoch[27] Batch [5]#011Speed: 1005.16 samples/sec#011loss=0.817928\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2529.633045196533, \"sum\": 2529.633045196533, \"min\": 2529.633045196533}}, \"EndTime\": 1591203972.99852, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203970.468829}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=243.896366603 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.814022034407\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:12 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:14 INFO 140291508827968] Epoch[28] Batch[0] avg_epoch_loss=0.812682\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.812681615353\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:14 INFO 140291508827968] Epoch[28] Batch[5] avg_epoch_loss=0.818106\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.818105767171\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:14 INFO 140291508827968] Epoch[28] Batch [5]#011Speed: 880.64 samples/sec#011loss=0.818106\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] Epoch[28] Batch[10] avg_epoch_loss=0.841385\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=0.869319736958\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] Epoch[28] Batch [10]#011Speed: 322.31 samples/sec#011loss=0.869320\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2690.467119216919, \"sum\": 2690.467119216919, \"min\": 2690.467119216919}}, \"EndTime\": 1591203975.689597, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203972.998609}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=240.096217115 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.841384844346\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:15 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:16 INFO 140291508827968] Epoch[29] Batch[0] avg_epoch_loss=0.837060\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.837059617043\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:17 INFO 140291508827968] Epoch[29] Batch[5] avg_epoch_loss=0.802498\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:17 INFO 140291508827968] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.802497784297\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:17 INFO 140291508827968] Epoch[29] Batch [5]#011Speed: 873.10 samples/sec#011loss=0.802498\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] Epoch[29] Batch[10] avg_epoch_loss=0.799739\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.796427714825\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] Epoch[29] Batch [10]#011Speed: 308.64 samples/sec#011loss=0.796428\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2704.97989654541, \"sum\": 2704.97989654541, \"min\": 2704.97989654541}}, \"EndTime\": 1591203978.395103, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203975.689681}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.504080627 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.799738661809\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:18 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_4b6266d9-bb79-498a-b80d-1577ac965449-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.196054458618164, \"sum\": 22.196054458618164, \"min\": 22.196054458618164}}, \"EndTime\": 1591203978.417888, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203978.395191}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:19 INFO 140291508827968] Epoch[30] Batch[0] avg_epoch_loss=0.850572\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.850571930408\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] Epoch[30] Batch[5] avg_epoch_loss=0.795807\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.79580706358\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] Epoch[30] Batch [5]#011Speed: 1006.43 samples/sec#011loss=0.795807\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2533.747911453247, \"sum\": 2533.747911453247, \"min\": 2533.747911453247}}, \"EndTime\": 1591203980.951756, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203978.417952}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.319483885 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.792062956095\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:20 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_5862a6a1-d282-456f-b55a-37ec7e795a91-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.841049194335938, \"sum\": 21.841049194335938, \"min\": 21.841049194335938}}, \"EndTime\": 1591203980.974191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203980.951821}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:22 INFO 140291508827968] Epoch[31] Batch[0] avg_epoch_loss=0.810859\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.810858845711\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:22 INFO 140291508827968] Epoch[31] Batch[5] avg_epoch_loss=0.808305\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.808305233717\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:22 INFO 140291508827968] Epoch[31] Batch [5]#011Speed: 971.42 samples/sec#011loss=0.808305\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:23 INFO 140291508827968] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2546.466827392578, \"sum\": 2546.466827392578, \"min\": 2546.466827392578}}, \"EndTime\": 1591203983.520799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203980.974268}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:23 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=237.964713696 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:23 INFO 140291508827968] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:23 INFO 140291508827968] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.824039375782\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:23 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:24 INFO 140291508827968] Epoch[32] Batch[0] avg_epoch_loss=0.800731\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.800730764866\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:25 INFO 140291508827968] Epoch[32] Batch[5] avg_epoch_loss=0.789963\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.78996265928\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:25 INFO 140291508827968] Epoch[32] Batch [5]#011Speed: 976.74 samples/sec#011loss=0.789963\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] Epoch[32] Batch[10] avg_epoch_loss=0.802842\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=0.818298101425\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] Epoch[32] Batch [10]#011Speed: 323.62 samples/sec#011loss=0.818298\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2658.1859588623047, \"sum\": 2658.1859588623047, \"min\": 2658.1859588623047}}, \"EndTime\": 1591203986.179544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203983.520888}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=248.654315305 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.802842405709\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:26 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:27 INFO 140291508827968] Epoch[33] Batch[0] avg_epoch_loss=0.834893\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.834892511368\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] Epoch[33] Batch[5] avg_epoch_loss=0.814571\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.81457071503\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] Epoch[33] Batch [5]#011Speed: 315.25 samples/sec#011loss=0.814571\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] processed a total of 584 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2576.1468410491943, \"sum\": 2576.1468410491943, \"min\": 2576.1468410491943}}, \"EndTime\": 1591203988.756251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203986.17963}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=226.686151922 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.803890967369\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:28 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:30 INFO 140291508827968] Epoch[34] Batch[0] avg_epoch_loss=0.754062\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.754061818123\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:30 INFO 140291508827968] Epoch[34] Batch[5] avg_epoch_loss=0.764417\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.764417300622\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:30 INFO 140291508827968] Epoch[34] Batch [5]#011Speed: 1018.51 samples/sec#011loss=0.764417\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:31 INFO 140291508827968] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2528.541088104248, \"sum\": 2528.541088104248, \"min\": 2528.541088104248}}, \"EndTime\": 1591203991.285343, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203988.756319}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:31 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.023703466 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:31 INFO 140291508827968] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:31 INFO 140291508827968] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.810652297735\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:31 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:32 INFO 140291508827968] Epoch[35] Batch[0] avg_epoch_loss=0.735077\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.735076963902\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:32 INFO 140291508827968] Epoch[35] Batch[5] avg_epoch_loss=0.800651\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.800651431084\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:32 INFO 140291508827968] Epoch[35] Batch [5]#011Speed: 1013.61 samples/sec#011loss=0.800651\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] Epoch[35] Batch[10] avg_epoch_loss=0.786825\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.770233380795\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] Epoch[35] Batch [10]#011Speed: 323.14 samples/sec#011loss=0.770233\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2618.722915649414, \"sum\": 2618.722915649414, \"min\": 2618.722915649414}}, \"EndTime\": 1591203993.904699, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203991.285437}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=249.728025648 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.786825044589\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:33 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_13f93ac1-473c-4fb4-b4f8-2687e6c88675-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.665096282958984, \"sum\": 21.665096282958984, \"min\": 21.665096282958984}}, \"EndTime\": 1591203993.926922, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203993.904782}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:35 INFO 140291508827968] Epoch[36] Batch[0] avg_epoch_loss=0.794530\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.794529676437\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:35 INFO 140291508827968] Epoch[36] Batch[5] avg_epoch_loss=0.787025\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.787025094032\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:35 INFO 140291508827968] Epoch[36] Batch [5]#011Speed: 960.48 samples/sec#011loss=0.787025\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:36 INFO 140291508827968] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2567.284107208252, \"sum\": 2567.284107208252, \"min\": 2567.284107208252}}, \"EndTime\": 1591203996.494355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203993.927008}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:36 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=231.750871314 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:36 INFO 140291508827968] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.812809830904\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:36 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:37 INFO 140291508827968] Epoch[37] Batch[0] avg_epoch_loss=0.744450\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.744450390339\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:38 INFO 140291508827968] Epoch[37] Batch[5] avg_epoch_loss=0.761974\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.761974016825\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:38 INFO 140291508827968] Epoch[37] Batch [5]#011Speed: 1020.92 samples/sec#011loss=0.761974\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] Epoch[37] Batch[10] avg_epoch_loss=0.737976\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=0.7091776371\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] Epoch[37] Batch [10]#011Speed: 321.41 samples/sec#011loss=0.709178\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2647.784948348999, \"sum\": 2647.784948348999, \"min\": 2647.784948348999}}, \"EndTime\": 1591203999.142742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203996.494445}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.099792528 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.737975662405\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:39 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_690895fb-5f70-4054-9dcc-d9f3f8ed2fb8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.7130184173584, \"sum\": 21.7130184173584, \"min\": 21.7130184173584}}, \"EndTime\": 1591203999.165079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203999.142826}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:40 INFO 140291508827968] Epoch[38] Batch[0] avg_epoch_loss=0.744144\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.744143784046\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:40 INFO 140291508827968] Epoch[38] Batch[5] avg_epoch_loss=0.766602\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.766601751248\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:40 INFO 140291508827968] Epoch[38] Batch [5]#011Speed: 953.59 samples/sec#011loss=0.766602\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:41 INFO 140291508827968] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2540.9390926361084, \"sum\": 2540.9390926361084, \"min\": 2540.9390926361084}}, \"EndTime\": 1591204001.706141, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591203999.165141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:41 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=249.107660898 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:41 INFO 140291508827968] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.772114527225\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:41 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:43 INFO 140291508827968] Epoch[39] Batch[0] avg_epoch_loss=0.754316\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:43 INFO 140291508827968] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.754316031933\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:43 INFO 140291508827968] Epoch[39] Batch[5] avg_epoch_loss=0.796467\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:43 INFO 140291508827968] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.796467026075\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:43 INFO 140291508827968] Epoch[39] Batch [5]#011Speed: 986.44 samples/sec#011loss=0.796467\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] Epoch[39] Batch[10] avg_epoch_loss=0.811528\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=0.829600477219\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] Epoch[39] Batch [10]#011Speed: 321.94 samples/sec#011loss=0.829600\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2623.7568855285645, \"sum\": 2623.7568855285645, \"min\": 2623.7568855285645}}, \"EndTime\": 1591204004.330461, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204001.706234}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=248.106160129 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.811527685686\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:44 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:45 INFO 140291508827968] Epoch[40] Batch[0] avg_epoch_loss=0.819541\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.819540798664\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:45 INFO 140291508827968] Epoch[40] Batch[5] avg_epoch_loss=0.770383\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.770383268595\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:45 INFO 140291508827968] Epoch[40] Batch [5]#011Speed: 990.66 samples/sec#011loss=0.770383\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] Epoch[40] Batch[10] avg_epoch_loss=0.742512\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=0.709066164494\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] Epoch[40] Batch [10]#011Speed: 312.46 samples/sec#011loss=0.709066\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2688.632011413574, \"sum\": 2688.632011413574, \"min\": 2688.632011413574}}, \"EndTime\": 1591204007.019616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204004.330545}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.351461835 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.74251185764\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:47 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:48 INFO 140291508827968] Epoch[41] Batch[0] avg_epoch_loss=0.856901\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.856900632381\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:48 INFO 140291508827968] Epoch[41] Batch[5] avg_epoch_loss=0.788343\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.788343449434\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:48 INFO 140291508827968] Epoch[41] Batch [5]#011Speed: 912.09 samples/sec#011loss=0.788343\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:49 INFO 140291508827968] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2605.6129932403564, \"sum\": 2605.6129932403564, \"min\": 2605.6129932403564}}, \"EndTime\": 1591204009.625752, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204007.019697}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:49 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.074477568 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:49 INFO 140291508827968] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.774462854862\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:49 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:50 INFO 140291508827968] Epoch[42] Batch[0] avg_epoch_loss=0.783998\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.78399848938\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:51 INFO 140291508827968] Epoch[42] Batch[5] avg_epoch_loss=0.756946\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:51 INFO 140291508827968] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.756945868333\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:51 INFO 140291508827968] Epoch[42] Batch [5]#011Speed: 1012.48 samples/sec#011loss=0.756946\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:52 INFO 140291508827968] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2595.710039138794, \"sum\": 2595.710039138794, \"min\": 2595.710039138794}}, \"EndTime\": 1591204012.22203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204009.625846}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:52 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=241.928166513 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:52 INFO 140291508827968] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:52 INFO 140291508827968] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.770219355822\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:52 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:53 INFO 140291508827968] Epoch[43] Batch[0] avg_epoch_loss=0.752441\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.752441465855\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:53 INFO 140291508827968] Epoch[43] Batch[5] avg_epoch_loss=0.770616\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.77061611414\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:53 INFO 140291508827968] Epoch[43] Batch [5]#011Speed: 935.94 samples/sec#011loss=0.770616\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] Epoch[43] Batch[10] avg_epoch_loss=0.768400\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.765741276741\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] Epoch[43] Batch [10]#011Speed: 309.23 samples/sec#011loss=0.765741\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2691.7810440063477, \"sum\": 2691.7810440063477, \"min\": 2691.7810440063477}}, \"EndTime\": 1591204014.914351, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204012.222096}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=240.721632158 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.768400278958\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:54 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:56 INFO 140291508827968] Epoch[44] Batch[0] avg_epoch_loss=0.715665\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.715665340424\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:56 INFO 140291508827968] Epoch[44] Batch[5] avg_epoch_loss=0.738929\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.738928894202\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:56 INFO 140291508827968] Epoch[44] Batch [5]#011Speed: 1019.18 samples/sec#011loss=0.738929\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:57 INFO 140291508827968] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2552.81400680542, \"sum\": 2552.81400680542, \"min\": 2552.81400680542}}, \"EndTime\": 1591204017.467715, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204014.914435}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:57 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=229.539007337 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:57 INFO 140291508827968] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:57 INFO 140291508827968] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.741842591763\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:57 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:58 INFO 140291508827968] Epoch[45] Batch[0] avg_epoch_loss=0.810830\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.810830235481\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:59 INFO 140291508827968] Epoch[45] Batch[5] avg_epoch_loss=0.758278\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:59 INFO 140291508827968] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.758277555307\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:06:59 INFO 140291508827968] Epoch[45] Batch [5]#011Speed: 1004.73 samples/sec#011loss=0.758278\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:00 INFO 140291508827968] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2582.6170444488525, \"sum\": 2582.6170444488525, \"min\": 2582.6170444488525}}, \"EndTime\": 1591204020.050897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204017.467804}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:00 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=237.342638077 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:00 INFO 140291508827968] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:00 INFO 140291508827968] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.787020838261\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:00 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:01 INFO 140291508827968] Epoch[46] Batch[0] avg_epoch_loss=0.750783\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.750783085823\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:01 INFO 140291508827968] Epoch[46] Batch[5] avg_epoch_loss=0.755468\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.755467683077\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:01 INFO 140291508827968] Epoch[46] Batch [5]#011Speed: 1007.43 samples/sec#011loss=0.755468\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2537.3241901397705, \"sum\": 2537.3241901397705, \"min\": 2537.3241901397705}}, \"EndTime\": 1591204022.58884, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204020.051003}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=249.069431248 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.737240755558\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:02 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_45910632-51fb-4382-9218-181980b8971b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.917892456054688, \"sum\": 20.917892456054688, \"min\": 20.917892456054688}}, \"EndTime\": 1591204022.610391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204022.58892}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:03 INFO 140291508827968] Epoch[47] Batch[0] avg_epoch_loss=0.757677\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.757677018642\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:04 INFO 140291508827968] Epoch[47] Batch[5] avg_epoch_loss=0.773267\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.773266563813\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:04 INFO 140291508827968] Epoch[47] Batch [5]#011Speed: 975.84 samples/sec#011loss=0.773267\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] Epoch[47] Batch[10] avg_epoch_loss=0.761332\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.747010731697\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] Epoch[47] Batch [10]#011Speed: 317.47 samples/sec#011loss=0.747011\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2663.1200313568115, \"sum\": 2663.1200313568115, \"min\": 2663.1200313568115}}, \"EndTime\": 1591204025.273644, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204022.610462}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.197883269 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.761332094669\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:05 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:06 INFO 140291508827968] Epoch[48] Batch[0] avg_epoch_loss=0.763046\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.763046324253\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:06 INFO 140291508827968] Epoch[48] Batch[5] avg_epoch_loss=0.757130\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.757130235434\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:06 INFO 140291508827968] Epoch[48] Batch [5]#011Speed: 1005.94 samples/sec#011loss=0.757130\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:07 INFO 140291508827968] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2502.9571056365967, \"sum\": 2502.9571056365967, \"min\": 2502.9571056365967}}, \"EndTime\": 1591204027.777152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204025.273724}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:07 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.292024821 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:07 INFO 140291508827968] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.759159088135\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:07 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:09 INFO 140291508827968] Epoch[49] Batch[0] avg_epoch_loss=0.737302\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.737302482128\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:09 INFO 140291508827968] Epoch[49] Batch[5] avg_epoch_loss=0.760547\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.760546833277\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:09 INFO 140291508827968] Epoch[49] Batch [5]#011Speed: 1029.26 samples/sec#011loss=0.760547\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] Epoch[49] Batch[10] avg_epoch_loss=0.768249\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.777491152287\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] Epoch[49] Batch [10]#011Speed: 328.54 samples/sec#011loss=0.777491\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2580.4309844970703, \"sum\": 2580.4309844970703, \"min\": 2580.4309844970703}}, \"EndTime\": 1591204030.358153, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204027.77722}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.597480724 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.768248796463\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:10 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:11 INFO 140291508827968] Epoch[50] Batch[0] avg_epoch_loss=0.739864\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.739864230156\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:11 INFO 140291508827968] Epoch[50] Batch[5] avg_epoch_loss=0.777937\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.777937302987\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:11 INFO 140291508827968] Epoch[50] Batch [5]#011Speed: 999.41 samples/sec#011loss=0.777937\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] Epoch[50] Batch[10] avg_epoch_loss=0.764372\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=0.748093390465\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] Epoch[50] Batch [10]#011Speed: 325.15 samples/sec#011loss=0.748093\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2609.341859817505, \"sum\": 2609.341859817505, \"min\": 2609.341859817505}}, \"EndTime\": 1591204032.968041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204030.358224}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=264.803701213 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.764371888204\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:12 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:14 INFO 140291508827968] Epoch[51] Batch[0] avg_epoch_loss=0.731935\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.731934785843\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:14 INFO 140291508827968] Epoch[51] Batch[5] avg_epoch_loss=0.756436\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.756435563167\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:14 INFO 140291508827968] Epoch[51] Batch [5]#011Speed: 1003.04 samples/sec#011loss=0.756436\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] Epoch[51] Batch[10] avg_epoch_loss=0.767256\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.780239880085\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] Epoch[51] Batch [10]#011Speed: 321.51 samples/sec#011loss=0.780240\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2615.553140640259, \"sum\": 2615.553140640259, \"min\": 2615.553140640259}}, \"EndTime\": 1591204035.584167, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204032.968136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.443809199 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.767255707221\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:15 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:16 INFO 140291508827968] Epoch[52] Batch[0] avg_epoch_loss=0.742019\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.742019116879\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:17 INFO 140291508827968] Epoch[52] Batch[5] avg_epoch_loss=0.710991\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:17 INFO 140291508827968] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.710991124312\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:17 INFO 140291508827968] Epoch[52] Batch [5]#011Speed: 954.69 samples/sec#011loss=0.710991\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2531.280040740967, \"sum\": 2531.280040740967, \"min\": 2531.280040740967}}, \"EndTime\": 1591204038.116006, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204035.584245}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.31849892 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.725974041224\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:18 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_8accbe9a-97a2-444b-a1ac-004982f8cd06-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.478933334350586, \"sum\": 27.478933334350586, \"min\": 27.478933334350586}}, \"EndTime\": 1591204038.144066, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204038.116091}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:19 INFO 140291508827968] Epoch[53] Batch[0] avg_epoch_loss=0.743477\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.743477106094\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:19 INFO 140291508827968] Epoch[53] Batch[5] avg_epoch_loss=0.745687\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.745686729749\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:19 INFO 140291508827968] Epoch[53] Batch [5]#011Speed: 959.89 samples/sec#011loss=0.745687\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:20 INFO 140291508827968] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2573.9359855651855, \"sum\": 2573.9359855651855, \"min\": 2573.9359855651855}}, \"EndTime\": 1591204040.718135, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204038.144132}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:20 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=235.036606579 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:20 INFO 140291508827968] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.730976068974\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:20 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:22 INFO 140291508827968] Epoch[54] Batch[0] avg_epoch_loss=0.725733\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.725733458996\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:22 INFO 140291508827968] Epoch[54] Batch[5] avg_epoch_loss=0.735450\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.735449800889\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:22 INFO 140291508827968] Epoch[54] Batch [5]#011Speed: 1010.60 samples/sec#011loss=0.735450\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2513.1478309631348, \"sum\": 2513.1478309631348, \"min\": 2513.1478309631348}}, \"EndTime\": 1591204043.231858, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204040.718226}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=237.538385414 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.720981669426\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:23 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_809e7c58-9c94-4ea3-8931-90a8579d2dfb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.404184341430664, \"sum\": 32.404184341430664, \"min\": 32.404184341430664}}, \"EndTime\": 1591204043.264885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204043.231948}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:24 INFO 140291508827968] Epoch[55] Batch[0] avg_epoch_loss=0.669177\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.669177055359\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:24 INFO 140291508827968] Epoch[55] Batch[5] avg_epoch_loss=0.714620\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.714620073636\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:24 INFO 140291508827968] Epoch[55] Batch [5]#011Speed: 1020.71 samples/sec#011loss=0.714620\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] Epoch[55] Batch[10] avg_epoch_loss=0.713185\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.711462295055\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] Epoch[55] Batch [10]#011Speed: 319.35 samples/sec#011loss=0.711462\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2634.2008113861084, \"sum\": 2634.2008113861084, \"min\": 2634.2008113861084}}, \"EndTime\": 1591204045.899206, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204043.264944}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.465362957 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.713184719736\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:25 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_8172090a-480c-4038-aa6a-7e2ef2286fd5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.615028381347656, \"sum\": 21.615028381347656, \"min\": 21.615028381347656}}, \"EndTime\": 1591204045.921381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204045.899285}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:27 INFO 140291508827968] Epoch[56] Batch[0] avg_epoch_loss=0.774956\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.774956405163\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:27 INFO 140291508827968] Epoch[56] Batch[5] avg_epoch_loss=0.754332\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.754331956307\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:27 INFO 140291508827968] Epoch[56] Batch [5]#011Speed: 1016.82 samples/sec#011loss=0.754332\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:28 INFO 140291508827968] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2527.7559757232666, \"sum\": 2527.7559757232666, \"min\": 2527.7559757232666}}, \"EndTime\": 1591204048.449257, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204045.921443}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:28 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=238.936343397 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:28 INFO 140291508827968] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:28 INFO 140291508827968] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.752133858204\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:28 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:29 INFO 140291508827968] Epoch[57] Batch[0] avg_epoch_loss=0.727950\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:29 INFO 140291508827968] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.727949857712\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:30 INFO 140291508827968] Epoch[57] Batch[5] avg_epoch_loss=0.722166\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.722166021665\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:30 INFO 140291508827968] Epoch[57] Batch [5]#011Speed: 1008.27 samples/sec#011loss=0.722166\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] Epoch[57] Batch[10] avg_epoch_loss=0.734710\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.749762415886\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] Epoch[57] Batch [10]#011Speed: 320.70 samples/sec#011loss=0.749762\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2619.1999912261963, \"sum\": 2619.1999912261963, \"min\": 2619.1999912261963}}, \"EndTime\": 1591204051.06905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204048.449332}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=256.937110284 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.73470983722\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:31 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:32 INFO 140291508827968] Epoch[58] Batch[0] avg_epoch_loss=0.736614\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.736613750458\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:32 INFO 140291508827968] Epoch[58] Batch[5] avg_epoch_loss=0.703656\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.703655531009\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:32 INFO 140291508827968] Epoch[58] Batch [5]#011Speed: 1001.48 samples/sec#011loss=0.703656\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] Epoch[58] Batch[10] avg_epoch_loss=0.722313\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=0.744702804089\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] Epoch[58] Batch [10]#011Speed: 312.18 samples/sec#011loss=0.744703\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2649.7700214385986, \"sum\": 2649.7700214385986, \"min\": 2649.7700214385986}}, \"EndTime\": 1591204053.719359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204051.069126}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.349888258 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.722313382409\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:33 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:35 INFO 140291508827968] Epoch[59] Batch[0] avg_epoch_loss=0.704219\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.704219102859\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:35 INFO 140291508827968] Epoch[59] Batch[5] avg_epoch_loss=0.732818\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.732817788919\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:35 INFO 140291508827968] Epoch[59] Batch [5]#011Speed: 1004.09 samples/sec#011loss=0.732818\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] Epoch[59] Batch[10] avg_epoch_loss=0.731673\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=0.730300319195\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] Epoch[59] Batch [10]#011Speed: 323.69 samples/sec#011loss=0.730300\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2645.444869995117, \"sum\": 2645.444869995117, \"min\": 2645.444869995117}}, \"EndTime\": 1591204056.365307, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204053.719443}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=253.254141764 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.731673484499\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:36 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:37 INFO 140291508827968] Epoch[60] Batch[0] avg_epoch_loss=0.768653\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.768653333187\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:37 INFO 140291508827968] Epoch[60] Batch[5] avg_epoch_loss=0.710385\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.710384507974\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:37 INFO 140291508827968] Epoch[60] Batch [5]#011Speed: 1001.02 samples/sec#011loss=0.710385\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2531.704902648926, \"sum\": 2531.704902648926, \"min\": 2531.704902648926}}, \"EndTime\": 1591204058.897588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204056.365391}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.671517399 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.710559260845\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:38 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_a7862f34-6882-436e-8493-2587232bfa41-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.375179290771484, \"sum\": 21.375179290771484, \"min\": 21.375179290771484}}, \"EndTime\": 1591204058.919575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204058.897677}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:40 INFO 140291508827968] Epoch[61] Batch[0] avg_epoch_loss=0.655628\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.655628204346\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:40 INFO 140291508827968] Epoch[61] Batch[5] avg_epoch_loss=0.709010\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.709009975195\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:40 INFO 140291508827968] Epoch[61] Batch [5]#011Speed: 977.63 samples/sec#011loss=0.709010\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] Epoch[61] Batch[10] avg_epoch_loss=0.723035\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.739864838123\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] Epoch[61] Batch [10]#011Speed: 316.31 samples/sec#011loss=0.739865\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2623.12388420105, \"sum\": 2623.12388420105, \"min\": 2623.12388420105}}, \"EndTime\": 1591204061.542812, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204058.919632}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=255.79034044 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.72303491289\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:41 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:42 INFO 140291508827968] Epoch[62] Batch[0] avg_epoch_loss=0.700907\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:42 INFO 140291508827968] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.700907111168\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:43 INFO 140291508827968] Epoch[62] Batch[5] avg_epoch_loss=0.666360\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:43 INFO 140291508827968] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.666359653076\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:43 INFO 140291508827968] Epoch[62] Batch [5]#011Speed: 332.00 samples/sec#011loss=0.666360\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:44 INFO 140291508827968] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2551.0571002960205, \"sum\": 2551.0571002960205, \"min\": 2551.0571002960205}}, \"EndTime\": 1591204064.094419, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204061.542894}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:44 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=235.968586753 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:44 INFO 140291508827968] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:44 INFO 140291508827968] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.721677833796\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:44 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:45 INFO 140291508827968] Epoch[63] Batch[0] avg_epoch_loss=0.684233\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.684233129025\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:45 INFO 140291508827968] Epoch[63] Batch[5] avg_epoch_loss=0.685089\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.685089061658\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:45 INFO 140291508827968] Epoch[63] Batch [5]#011Speed: 1014.61 samples/sec#011loss=0.685089\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2518.5649394989014, \"sum\": 2518.5649394989014, \"min\": 2518.5649394989014}}, \"EndTime\": 1591204066.613628, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204064.094507}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.762388392 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.652202495933\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:46 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_b820af4d-7f76-4788-aaf2-6d3dab023f42-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.312952041625977, \"sum\": 21.312952041625977, \"min\": 21.312952041625977}}, \"EndTime\": 1591204066.635599, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204066.613717}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:47 INFO 140291508827968] Epoch[64] Batch[0] avg_epoch_loss=0.689912\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:47 INFO 140291508827968] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.689912378788\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:48 INFO 140291508827968] Epoch[64] Batch[5] avg_epoch_loss=0.668651\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.668651292721\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:48 INFO 140291508827968] Epoch[64] Batch [5]#011Speed: 1023.03 samples/sec#011loss=0.668651\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] Epoch[64] Batch[10] avg_epoch_loss=0.651340\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.630565905571\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] Epoch[64] Batch [10]#011Speed: 313.34 samples/sec#011loss=0.630566\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2624.2611408233643, \"sum\": 2624.2611408233643, \"min\": 2624.2611408233643}}, \"EndTime\": 1591204069.259999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204066.635674}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=260.251744999 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.651339753108\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:49 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_28a8667d-3f20-4c0c-bb8a-61980e804f23-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.364845275878906, \"sum\": 32.364845275878906, \"min\": 32.364845275878906}}, \"EndTime\": 1591204069.292914, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204069.260082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:50 INFO 140291508827968] Epoch[65] Batch[0] avg_epoch_loss=0.669320\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.66932040453\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:50 INFO 140291508827968] Epoch[65] Batch[5] avg_epoch_loss=0.628785\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.62878498435\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:50 INFO 140291508827968] Epoch[65] Batch [5]#011Speed: 1008.57 samples/sec#011loss=0.628785\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:51 INFO 140291508827968] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2537.816047668457, \"sum\": 2537.816047668457, \"min\": 2537.816047668457}}, \"EndTime\": 1591204071.830863, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204069.292982}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:51 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=236.805581178 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:51 INFO 140291508827968] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:51 INFO 140291508827968] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.666053026915\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:51 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:53 INFO 140291508827968] Epoch[66] Batch[0] avg_epoch_loss=0.655713\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.655712962151\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:53 INFO 140291508827968] Epoch[66] Batch[5] avg_epoch_loss=0.630712\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.630711893241\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:53 INFO 140291508827968] Epoch[66] Batch [5]#011Speed: 1001.72 samples/sec#011loss=0.630712\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2583.3961963653564, \"sum\": 2583.3961963653564, \"min\": 2583.3961963653564}}, \"EndTime\": 1591204074.414897, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204071.830951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=246.558625754 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.621965265274\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:54 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_dcf0b95a-f73e-4fa8-be15-1c0fe490b94f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.92393684387207, \"sum\": 32.92393684387207, \"min\": 32.92393684387207}}, \"EndTime\": 1591204074.448479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204074.41502}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:55 INFO 140291508827968] Epoch[67] Batch[0] avg_epoch_loss=0.549087\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:55 INFO 140291508827968] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.549087405205\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] Epoch[67] Batch[5] avg_epoch_loss=0.599354\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.599353919427\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] Epoch[67] Batch [5]#011Speed: 1023.80 samples/sec#011loss=0.599354\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2532.3312282562256, \"sum\": 2532.3312282562256, \"min\": 2532.3312282562256}}, \"EndTime\": 1591204076.980941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204074.448548}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=241.662714732 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.605067497492\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:56 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:57 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_4317c95e-586e-4598-935d-2553515300e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.41604232788086, \"sum\": 23.41604232788086, \"min\": 23.41604232788086}}, \"EndTime\": 1591204077.004933, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204076.981027}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:58 INFO 140291508827968] Epoch[68] Batch[0] avg_epoch_loss=0.575789\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.575788676739\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:58 INFO 140291508827968] Epoch[68] Batch[5] avg_epoch_loss=0.597673\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.597673286994\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:58 INFO 140291508827968] Epoch[68] Batch [5]#011Speed: 940.82 samples/sec#011loss=0.597673\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:59 INFO 140291508827968] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2556.562900543213, \"sum\": 2556.562900543213, \"min\": 2556.562900543213}}, \"EndTime\": 1591204079.561616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204077.004991}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:59 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=237.807590597 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:59 INFO 140291508827968] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:59 INFO 140291508827968] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.625568437576\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:07:59 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:00 INFO 140291508827968] Epoch[69] Batch[0] avg_epoch_loss=0.680315\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:00 INFO 140291508827968] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.680314660072\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:01 INFO 140291508827968] Epoch[69] Batch[5] avg_epoch_loss=0.591906\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.591906398535\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:01 INFO 140291508827968] Epoch[69] Batch [5]#011Speed: 1019.92 samples/sec#011loss=0.591906\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2574.083089828491, \"sum\": 2574.083089828491, \"min\": 2574.083089828491}}, \"EndTime\": 1591204082.13625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204079.561704}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=237.35602815 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.597543954849\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:02 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_29134e7a-1d73-4eec-abb8-42fab0526af8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.239042282104492, \"sum\": 21.239042282104492, \"min\": 21.239042282104492}}, \"EndTime\": 1591204082.158106, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204082.136329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:03 INFO 140291508827968] Epoch[70] Batch[0] avg_epoch_loss=0.599841\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.599840581417\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:03 INFO 140291508827968] Epoch[70] Batch[5] avg_epoch_loss=0.575708\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.575707882643\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:03 INFO 140291508827968] Epoch[70] Batch [5]#011Speed: 975.76 samples/sec#011loss=0.575708\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] Epoch[70] Batch[10] avg_epoch_loss=0.564581\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.551229828596\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] Epoch[70] Batch [10]#011Speed: 317.72 samples/sec#011loss=0.551230\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2619.428873062134, \"sum\": 2619.428873062134, \"min\": 2619.428873062134}}, \"EndTime\": 1591204084.777641, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204082.158158}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=252.334106023 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.56458149444\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:04 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_f7c9b81b-8a81-4c74-97f6-6b1bbb48b0ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.269798278808594, \"sum\": 21.269798278808594, \"min\": 21.269798278808594}}, \"EndTime\": 1591204084.799477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204084.777719}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:06 INFO 140291508827968] Epoch[71] Batch[0] avg_epoch_loss=0.499222\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.49922183156\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:06 INFO 140291508827968] Epoch[71] Batch[5] avg_epoch_loss=0.509176\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.509175608555\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:06 INFO 140291508827968] Epoch[71] Batch [5]#011Speed: 1027.72 samples/sec#011loss=0.509176\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2495.604991912842, \"sum\": 2495.604991912842, \"min\": 2495.604991912842}}, \"EndTime\": 1591204087.295201, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204084.799538}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=242.014036732 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.505868026614\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:07 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_5203120c-0f44-4d9e-8e2e-3bb8f04f51f6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.351099014282227, \"sum\": 21.351099014282227, \"min\": 21.351099014282227}}, \"EndTime\": 1591204087.317172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204087.29528}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:08 INFO 140291508827968] Epoch[72] Batch[0] avg_epoch_loss=0.503407\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:08 INFO 140291508827968] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.5034070611\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:08 INFO 140291508827968] Epoch[72] Batch[5] avg_epoch_loss=0.519875\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:08 INFO 140291508827968] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.519874870777\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:08 INFO 140291508827968] Epoch[72] Batch [5]#011Speed: 988.77 samples/sec#011loss=0.519875\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] Epoch[72] Batch[10] avg_epoch_loss=0.551801\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=0.590112119913\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] Epoch[72] Batch [10]#011Speed: 316.75 samples/sec#011loss=0.590112\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2622.288942337036, \"sum\": 2622.288942337036, \"min\": 2622.288942337036}}, \"EndTime\": 1591204089.939582, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204087.317231}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.814841504 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.551800893112\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:09 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:11 INFO 140291508827968] Epoch[73] Batch[0] avg_epoch_loss=0.498434\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.498433589935\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:11 INFO 140291508827968] Epoch[73] Batch[5] avg_epoch_loss=0.504765\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.504764631391\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:11 INFO 140291508827968] Epoch[73] Batch [5]#011Speed: 997.12 samples/sec#011loss=0.504765\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] Epoch[73] Batch[10] avg_epoch_loss=0.479828\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.449904662371\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] Epoch[73] Batch [10]#011Speed: 317.39 samples/sec#011loss=0.449905\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2620.854139328003, \"sum\": 2620.854139328003, \"min\": 2620.854139328003}}, \"EndTime\": 1591204092.560944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204089.939646}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=257.156700471 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.479828281836\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:12 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_247f0ef5-2a64-43aa-b83a-75621c9ee5e3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.393123626708984, \"sum\": 30.393123626708984, \"min\": 30.393123626708984}}, \"EndTime\": 1591204092.59189, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204092.561021}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:13 INFO 140291508827968] Epoch[74] Batch[0] avg_epoch_loss=0.493574\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:13 INFO 140291508827968] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.493573874235\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:14 INFO 140291508827968] Epoch[74] Batch[5] avg_epoch_loss=0.457250\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.457249994079\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:14 INFO 140291508827968] Epoch[74] Batch [5]#011Speed: 907.53 samples/sec#011loss=0.457250\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] Epoch[74] Batch[10] avg_epoch_loss=0.407003\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.346705892682\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] Epoch[74] Batch [10]#011Speed: 321.65 samples/sec#011loss=0.346706\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2655.848979949951, \"sum\": 2655.848979949951, \"min\": 2655.848979949951}}, \"EndTime\": 1591204095.247841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204092.591931}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=241.343078683 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.407002675262\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:15 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_c64a12c7-f380-4d94-995a-05a75f3df29f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.460927963256836, \"sum\": 32.460927963256836, \"min\": 32.460927963256836}}, \"EndTime\": 1591204095.280891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204095.247923}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:16 INFO 140291508827968] Epoch[75] Batch[0] avg_epoch_loss=0.485677\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.485676944256\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:16 INFO 140291508827968] Epoch[75] Batch[5] avg_epoch_loss=0.475404\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.475403850277\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:16 INFO 140291508827968] Epoch[75] Batch [5]#011Speed: 1012.87 samples/sec#011loss=0.475404\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:17 INFO 140291508827968] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2531.139135360718, \"sum\": 2531.139135360718, \"min\": 2531.139135360718}}, \"EndTime\": 1591204097.812154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204095.280956}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:17 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=234.665362581 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:17 INFO 140291508827968] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:17 INFO 140291508827968] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.438565506786\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:17 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:19 INFO 140291508827968] Epoch[76] Batch[0] avg_epoch_loss=0.450592\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.450591534376\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:19 INFO 140291508827968] Epoch[76] Batch[5] avg_epoch_loss=0.465959\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.465959325433\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:19 INFO 140291508827968] Epoch[76] Batch [5]#011Speed: 925.89 samples/sec#011loss=0.465959\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] Epoch[76] Batch[10] avg_epoch_loss=0.395702\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=0.311392952502\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] Epoch[76] Batch [10]#011Speed: 323.83 samples/sec#011loss=0.311393\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2652.3258686065674, \"sum\": 2652.3258686065674, \"min\": 2652.3258686065674}}, \"EndTime\": 1591204100.465018, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204097.81224}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=248.072261212 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.395701883191\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:20 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_d8c19e1f-9a63-4561-8725-1d4894358440-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.721839904785156, \"sum\": 21.721839904785156, \"min\": 21.721839904785156}}, \"EndTime\": 1591204100.487359, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204100.465103}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:21 INFO 140291508827968] Epoch[77] Batch[0] avg_epoch_loss=0.504839\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:21 INFO 140291508827968] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.504838645458\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:22 INFO 140291508827968] Epoch[77] Batch[5] avg_epoch_loss=0.409517\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:22 INFO 140291508827968] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.409517064691\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:22 INFO 140291508827968] Epoch[77] Batch [5]#011Speed: 1008.04 samples/sec#011loss=0.409517\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] Epoch[77] Batch[10] avg_epoch_loss=0.396832\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.381610828638\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] Epoch[77] Batch [10]#011Speed: 328.99 samples/sec#011loss=0.381611\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2600.1710891723633, \"sum\": 2600.1710891723633, \"min\": 2600.1710891723633}}, \"EndTime\": 1591204103.087649, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204100.487421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=252.664235923 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.396832411939\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:23 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:24 INFO 140291508827968] Epoch[78] Batch[0] avg_epoch_loss=0.364725\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.364725023508\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:24 INFO 140291508827968] Epoch[78] Batch[5] avg_epoch_loss=0.387552\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:24 INFO 140291508827968] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.387551893791\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:24 INFO 140291508827968] Epoch[78] Batch [5]#011Speed: 1015.99 samples/sec#011loss=0.387552\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] Epoch[78] Batch[10] avg_epoch_loss=0.397107\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.408573555946\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] Epoch[78] Batch [10]#011Speed: 315.39 samples/sec#011loss=0.408574\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2639.4309997558594, \"sum\": 2639.4309997558594, \"min\": 2639.4309997558594}}, \"EndTime\": 1591204105.727566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204103.08773}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=252.694499606 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.39710719477\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:25 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:27 INFO 140291508827968] Epoch[79] Batch[0] avg_epoch_loss=0.385023\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.385023355484\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:27 INFO 140291508827968] Epoch[79] Batch[5] avg_epoch_loss=0.403323\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:27 INFO 140291508827968] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.403322895368\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:27 INFO 140291508827968] Epoch[79] Batch [5]#011Speed: 1013.50 samples/sec#011loss=0.403323\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:28 INFO 140291508827968] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2581.5751552581787, \"sum\": 2581.5751552581787, \"min\": 2581.5751552581787}}, \"EndTime\": 1591204108.309643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204105.727648}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:28 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=243.250293602 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:28 INFO 140291508827968] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:28 INFO 140291508827968] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.397928440571\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:28 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:29 INFO 140291508827968] Epoch[80] Batch[0] avg_epoch_loss=0.447718\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:29 INFO 140291508827968] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.447718054056\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:29 INFO 140291508827968] Epoch[80] Batch[5] avg_epoch_loss=0.375039\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:29 INFO 140291508827968] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.37503931423\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:29 INFO 140291508827968] Epoch[80] Batch [5]#011Speed: 947.77 samples/sec#011loss=0.375039\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] Epoch[80] Batch[10] avg_epoch_loss=0.389273\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.406353610754\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] Epoch[80] Batch [10]#011Speed: 313.25 samples/sec#011loss=0.406354\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2676.9039630889893, \"sum\": 2676.9039630889893, \"min\": 2676.9039630889893}}, \"EndTime\": 1591204110.9871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204108.309732}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.674236189 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.389273085377\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:30 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:31 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_bb30e698-04d4-421d-b6af-868b1920cb9d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.407978057861328, \"sum\": 29.407978057861328, \"min\": 29.407978057861328}}, \"EndTime\": 1591204111.017091, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204110.987184}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:32 INFO 140291508827968] Epoch[81] Batch[0] avg_epoch_loss=0.370591\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.370591342449\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:32 INFO 140291508827968] Epoch[81] Batch[5] avg_epoch_loss=0.323361\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:32 INFO 140291508827968] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.323361252745\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:32 INFO 140291508827968] Epoch[81] Batch [5]#011Speed: 1011.88 samples/sec#011loss=0.323361\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] Epoch[81] Batch[10] avg_epoch_loss=0.300840\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.273815096542\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] Epoch[81] Batch [10]#011Speed: 312.39 samples/sec#011loss=0.273815\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2639.1470432281494, \"sum\": 2639.1470432281494, \"min\": 2639.1470432281494}}, \"EndTime\": 1591204113.656366, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204111.017157}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.964833841 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.300840272653\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:33 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_36d9d18c-ee43-499e-971e-e24d50416085-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.981000900268555, \"sum\": 21.981000900268555, \"min\": 21.981000900268555}}, \"EndTime\": 1591204113.678898, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204113.656436}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:34 INFO 140291508827968] Epoch[82] Batch[0] avg_epoch_loss=0.334263\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:34 INFO 140291508827968] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.334262579679\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:35 INFO 140291508827968] Epoch[82] Batch[5] avg_epoch_loss=0.336475\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:35 INFO 140291508827968] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.336474942664\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:35 INFO 140291508827968] Epoch[82] Batch [5]#011Speed: 1012.68 samples/sec#011loss=0.336475\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] Epoch[82] Batch[10] avg_epoch_loss=0.346953\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.359526151419\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] Epoch[82] Batch [10]#011Speed: 322.11 samples/sec#011loss=0.359526\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2612.089157104492, \"sum\": 2612.089157104492, \"min\": 2612.089157104492}}, \"EndTime\": 1591204116.291148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204113.678996}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.770026577 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.346952764825\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:36 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:37 INFO 140291508827968] Epoch[83] Batch[0] avg_epoch_loss=0.279595\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.279595404863\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:37 INFO 140291508827968] Epoch[83] Batch[5] avg_epoch_loss=0.279349\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:37 INFO 140291508827968] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.2793492799\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:37 INFO 140291508827968] Epoch[83] Batch [5]#011Speed: 984.37 samples/sec#011loss=0.279349\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] Epoch[83] Batch[10] avg_epoch_loss=0.259373\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=0.235400694609\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] Epoch[83] Batch [10]#011Speed: 322.84 samples/sec#011loss=0.235401\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2618.3090209960938, \"sum\": 2618.3090209960938, \"min\": 2618.3090209960938}}, \"EndTime\": 1591204118.909978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204116.291221}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=245.568597085 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.259372650222\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:38 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_981fe13f-2564-487b-98d7-e1529f7ca532-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.15011215209961, \"sum\": 21.15011215209961, \"min\": 21.15011215209961}}, \"EndTime\": 1591204118.931697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204118.910045}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:40 INFO 140291508827968] Epoch[84] Batch[0] avg_epoch_loss=0.305071\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.305071055889\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:40 INFO 140291508827968] Epoch[84] Batch[5] avg_epoch_loss=0.309163\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:40 INFO 140291508827968] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.309162537257\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:40 INFO 140291508827968] Epoch[84] Batch [5]#011Speed: 979.74 samples/sec#011loss=0.309163\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] Epoch[84] Batch[10] avg_epoch_loss=0.291028\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=0.269265823811\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] Epoch[84] Batch [10]#011Speed: 324.80 samples/sec#011loss=0.269266\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2625.684976577759, \"sum\": 2625.684976577759, \"min\": 2625.684976577759}}, \"EndTime\": 1591204121.557515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204118.931762}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=260.492713269 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.291027667509\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:41 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:42 INFO 140291508827968] Epoch[85] Batch[0] avg_epoch_loss=0.281030\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:42 INFO 140291508827968] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.281030237675\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:43 INFO 140291508827968] Epoch[85] Batch[5] avg_epoch_loss=0.295792\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:43 INFO 140291508827968] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.295791548987\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:43 INFO 140291508827968] Epoch[85] Batch [5]#011Speed: 969.10 samples/sec#011loss=0.295792\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:44 INFO 140291508827968] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2542.526960372925, \"sum\": 2542.526960372925, \"min\": 2542.526960372925}}, \"EndTime\": 1591204124.100573, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204121.557581}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:44 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=243.055810533 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:44 INFO 140291508827968] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:44 INFO 140291508827968] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.339453236759\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:44 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:45 INFO 140291508827968] Epoch[86] Batch[0] avg_epoch_loss=0.243565\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.243564933538\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:45 INFO 140291508827968] Epoch[86] Batch[5] avg_epoch_loss=0.306777\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:45 INFO 140291508827968] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.306777179241\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:45 INFO 140291508827968] Epoch[86] Batch [5]#011Speed: 979.41 samples/sec#011loss=0.306777\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] Epoch[86] Batch[10] avg_epoch_loss=0.280180\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.248264434934\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] Epoch[86] Batch [10]#011Speed: 324.85 samples/sec#011loss=0.248264\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2622.2660541534424, \"sum\": 2622.2660541534424, \"min\": 2622.2660541534424}}, \"EndTime\": 1591204126.723346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204124.100637}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=250.916442527 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.280180477283\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:46 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:48 INFO 140291508827968] Epoch[87] Batch[0] avg_epoch_loss=0.226497\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.22649680078\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:48 INFO 140291508827968] Epoch[87] Batch[5] avg_epoch_loss=0.255906\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:48 INFO 140291508827968] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.255906442801\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:48 INFO 140291508827968] Epoch[87] Batch [5]#011Speed: 978.26 samples/sec#011loss=0.255906\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:49 INFO 140291508827968] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2594.0799713134766, \"sum\": 2594.0799713134766, \"min\": 2594.0799713134766}}, \"EndTime\": 1591204129.31792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204126.723428}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:49 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=243.234068943 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:49 INFO 140291508827968] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:49 INFO 140291508827968] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.266726458073\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:49 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:50 INFO 140291508827968] Epoch[88] Batch[0] avg_epoch_loss=0.254835\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.254834771156\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:50 INFO 140291508827968] Epoch[88] Batch[5] avg_epoch_loss=0.211409\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:50 INFO 140291508827968] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.211409129202\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:50 INFO 140291508827968] Epoch[88] Batch [5]#011Speed: 1023.10 samples/sec#011loss=0.211409\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] Epoch[88] Batch[10] avg_epoch_loss=0.199121\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.184374713339\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] Epoch[88] Batch [10]#011Speed: 319.06 samples/sec#011loss=0.184375\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2613.4588718414307, \"sum\": 2613.4588718414307, \"min\": 2613.4588718414307}}, \"EndTime\": 1591204131.931942, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204129.318009}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=256.737049816 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.199120758355\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:51 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_0c8036c3-665f-4de4-b4cb-a679da42ad01-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.79994773864746, \"sum\": 19.79994773864746, \"min\": 19.79994773864746}}, \"EndTime\": 1591204131.9523, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204131.932015}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:53 INFO 140291508827968] Epoch[89] Batch[0] avg_epoch_loss=0.348237\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.348237365484\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:53 INFO 140291508827968] Epoch[89] Batch[5] avg_epoch_loss=0.234964\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:53 INFO 140291508827968] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.234963620702\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:53 INFO 140291508827968] Epoch[89] Batch [5]#011Speed: 1028.82 samples/sec#011loss=0.234964\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:54 INFO 140291508827968] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2510.967969894409, \"sum\": 2510.967969894409, \"min\": 2510.967969894409}}, \"EndTime\": 1591204134.463391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204131.952365}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:54 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=249.295310725 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:54 INFO 140291508827968] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:54 INFO 140291508827968] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.205333093554\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:54 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:55 INFO 140291508827968] Epoch[90] Batch[0] avg_epoch_loss=0.322258\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:55 INFO 140291508827968] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.322258293629\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:56 INFO 140291508827968] Epoch[90] Batch[5] avg_epoch_loss=0.225124\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:56 INFO 140291508827968] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.225123636425\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:56 INFO 140291508827968] Epoch[90] Batch [5]#011Speed: 1027.43 samples/sec#011loss=0.225124\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] Epoch[90] Batch[10] avg_epoch_loss=0.190090\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=0.148050308228\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] Epoch[90] Batch [10]#011Speed: 315.58 samples/sec#011loss=0.148050\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2609.7028255462646, \"sum\": 2609.7028255462646, \"min\": 2609.7028255462646}}, \"EndTime\": 1591204137.073659, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204134.463468}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=248.677095909 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.190090305426\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:57 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_b60635db-c236-48c7-a621-896b60f78ad4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.239042282104492, \"sum\": 21.239042282104492, \"min\": 21.239042282104492}}, \"EndTime\": 1591204137.095463, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204137.073734}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:58 INFO 140291508827968] Epoch[91] Batch[0] avg_epoch_loss=0.534590\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.534590125084\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:58 INFO 140291508827968] Epoch[91] Batch[5] avg_epoch_loss=0.362148\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:58 INFO 140291508827968] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.362147897482\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:58 INFO 140291508827968] Epoch[91] Batch [5]#011Speed: 1016.34 samples/sec#011loss=0.362148\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:59 INFO 140291508827968] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2509.716033935547, \"sum\": 2509.716033935547, \"min\": 2509.716033935547}}, \"EndTime\": 1591204139.605301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204137.095526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:59 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.412572862 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:59 INFO 140291508827968] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:59 INFO 140291508827968] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.319291359186\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:08:59 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:00 INFO 140291508827968] Epoch[92] Batch[0] avg_epoch_loss=0.251654\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:00 INFO 140291508827968] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.251654326916\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:01 INFO 140291508827968] Epoch[92] Batch[5] avg_epoch_loss=0.267497\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:01 INFO 140291508827968] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.26749681433\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:01 INFO 140291508827968] Epoch[92] Batch [5]#011Speed: 1029.17 samples/sec#011loss=0.267497\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] Epoch[92] Batch[10] avg_epoch_loss=0.266111\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.264448276162\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] Epoch[92] Batch [10]#011Speed: 314.59 samples/sec#011loss=0.264448\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2614.220142364502, \"sum\": 2614.220142364502, \"min\": 2614.220142364502}}, \"EndTime\": 1591204142.220079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204139.605368}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.366992187 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.266111115163\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:02 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:03 INFO 140291508827968] Epoch[93] Batch[0] avg_epoch_loss=0.241633\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.241633325815\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:03 INFO 140291508827968] Epoch[93] Batch[5] avg_epoch_loss=0.212653\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:03 INFO 140291508827968] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.212652511895\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:03 INFO 140291508827968] Epoch[93] Batch [5]#011Speed: 1020.74 samples/sec#011loss=0.212653\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] Epoch[93] Batch[10] avg_epoch_loss=0.165282\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.108437293768\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] Epoch[93] Batch [10]#011Speed: 330.55 samples/sec#011loss=0.108437\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2581.246852874756, \"sum\": 2581.246852874756, \"min\": 2581.246852874756}}, \"EndTime\": 1591204144.801895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204142.220151}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=256.067552806 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.165281958201\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:04 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_f7155a15-0806-4044-8dc4-d5666cc7c3cf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.368026733398438, \"sum\": 21.368026733398438, \"min\": 21.368026733398438}}, \"EndTime\": 1591204144.823829, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204144.801959}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:06 INFO 140291508827968] Epoch[94] Batch[0] avg_epoch_loss=0.305467\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.305466800928\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:06 INFO 140291508827968] Epoch[94] Batch[5] avg_epoch_loss=0.223323\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:06 INFO 140291508827968] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.223323295514\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:06 INFO 140291508827968] Epoch[94] Batch [5]#011Speed: 1018.19 samples/sec#011loss=0.223323\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] Epoch[94] Batch[10] avg_epoch_loss=0.161951\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=0.0883040398359\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] Epoch[94] Batch [10]#011Speed: 328.50 samples/sec#011loss=0.088304\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2573.8160610198975, \"sum\": 2573.8160610198975, \"min\": 2573.8160610198975}}, \"EndTime\": 1591204147.397758, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204144.823885}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=253.697650531 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.161950906569\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:07 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_63427863-1a95-458d-be87-1b947db926e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.115137100219727, \"sum\": 20.115137100219727, \"min\": 20.115137100219727}}, \"EndTime\": 1591204147.418503, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204147.397834}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:08 INFO 140291508827968] Epoch[95] Batch[0] avg_epoch_loss=0.156154\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:08 INFO 140291508827968] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.156153708696\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:09 INFO 140291508827968] Epoch[95] Batch[5] avg_epoch_loss=0.179796\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:09 INFO 140291508827968] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.179795568188\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:09 INFO 140291508827968] Epoch[95] Batch [5]#011Speed: 811.78 samples/sec#011loss=0.179796\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] Epoch[95] Batch[10] avg_epoch_loss=0.113906\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=0.0348379984498\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] Epoch[95] Batch [10]#011Speed: 321.50 samples/sec#011loss=0.034838\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2698.0950832366943, \"sum\": 2698.0950832366943, \"min\": 2698.0950832366943}}, \"EndTime\": 1591204150.116696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204147.418543}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=238.676478109 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.113905763762\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:10 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/state_4be76ec2-df56-4026-845e-d8440ccc3c34-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.13310241699219, \"sum\": 32.13310241699219, \"min\": 32.13310241699219}}, \"EndTime\": 1591204150.149381, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204150.116778}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:11 INFO 140291508827968] Epoch[96] Batch[0] avg_epoch_loss=0.107887\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.107886694372\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:11 INFO 140291508827968] Epoch[96] Batch[5] avg_epoch_loss=0.152444\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:11 INFO 140291508827968] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.152443730583\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:11 INFO 140291508827968] Epoch[96] Batch [5]#011Speed: 1021.54 samples/sec#011loss=0.152444\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] Epoch[96] Batch[10] avg_epoch_loss=0.202835\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.263303869963\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] Epoch[96] Batch [10]#011Speed: 314.10 samples/sec#011loss=0.263304\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2654.3359756469727, \"sum\": 2654.3359756469727, \"min\": 2654.3359756469727}}, \"EndTime\": 1591204152.803833, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204150.149437}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=244.871076787 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.202834703028\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:12 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:14 INFO 140291508827968] Epoch[97] Batch[0] avg_epoch_loss=0.250203\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.250202596188\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:14 INFO 140291508827968] Epoch[97] Batch[5] avg_epoch_loss=0.228374\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:14 INFO 140291508827968] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.228373970836\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:14 INFO 140291508827968] Epoch[97] Batch [5]#011Speed: 1001.81 samples/sec#011loss=0.228374\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] Epoch[97] Batch[10] avg_epoch_loss=0.186282\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=0.13577156812\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] Epoch[97] Batch [10]#011Speed: 304.15 samples/sec#011loss=0.135772\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2667.52290725708, \"sum\": 2667.52290725708, \"min\": 2667.52290725708}}, \"EndTime\": 1591204155.471903, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204152.803916}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=254.157894135 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.186281969601\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:15 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:16 INFO 140291508827968] Epoch[98] Batch[0] avg_epoch_loss=0.118681\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:16 INFO 140291508827968] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.118681140244\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:17 INFO 140291508827968] Epoch[98] Batch[5] avg_epoch_loss=0.142949\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:17 INFO 140291508827968] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.14294883733\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:17 INFO 140291508827968] Epoch[98] Batch [5]#011Speed: 1001.14 samples/sec#011loss=0.142949\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:18 INFO 140291508827968] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2576.7998695373535, \"sum\": 2576.7998695373535, \"min\": 2576.7998695373535}}, \"EndTime\": 1591204158.049224, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204155.471972}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:18 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=247.581603232 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:18 INFO 140291508827968] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:18 INFO 140291508827968] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.13011424616\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:18 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:19 INFO 140291508827968] Epoch[99] Batch[0] avg_epoch_loss=0.162177\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.162177354097\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:19 INFO 140291508827968] Epoch[99] Batch[5] avg_epoch_loss=0.161012\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:19 INFO 140291508827968] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.16101210316\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:19 INFO 140291508827968] Epoch[99] Batch [5]#011Speed: 1010.25 samples/sec#011loss=0.161012\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] Epoch[99] Batch[10] avg_epoch_loss=0.148123\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=0.132655020058\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] Epoch[99] Batch [10]#011Speed: 310.10 samples/sec#011loss=0.132655\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 2654.503107070923, \"sum\": 2654.503107070923, \"min\": 2654.503107070923}}, \"EndTime\": 1591204160.704278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204158.049313}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] #throughput_metric: host=algo-1, train throughput=251.259870677 records/second\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.148122519932\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] Final loss: 0.113905763762 (occurred at epoch 95)\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] #quality_metric: host=algo-1, train final_loss <loss>=0.113905763762\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 WARNING 140291508827968] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:20 INFO 140291508827968] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 270.7810401916504, \"sum\": 270.7810401916504, \"min\": 270.7810401916504}}, \"EndTime\": 1591204160.975976, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204160.704359}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:21 INFO 140291508827968] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 333.6000442504883, \"sum\": 333.6000442504883, \"min\": 333.6000442504883}}, \"EndTime\": 1591204161.038757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204160.976032}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:21 INFO 140291508827968] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:21 INFO 140291508827968] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 14.478921890258789, \"sum\": 14.478921890258789, \"min\": 14.478921890258789}}, \"EndTime\": 1591204161.053353, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204161.038829}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:21 INFO 140291508827968] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:21 INFO 140291508827968] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.03790855407714844, \"sum\": 0.03790855407714844, \"min\": 0.03790855407714844}}, \"EndTime\": 1591204161.054194, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204161.053412}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:24 INFO 140291508827968] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:28 INFO 140291508827968] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:32 INFO 140291508827968] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:35 INFO 140291508827968] Number of test batches scored: 40\u001b[0m\n",
      "\n",
      "2020-06-03 17:09:49 Uploading - Uploading generated training model\u001b[34m[06/03/2020 17:09:39 INFO 140291508827968] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:43 INFO 140291508827968] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:46 INFO 140291508827968] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 28269.038200378418, \"sum\": 28269.038200378418, \"min\": 28269.038200378418}}, \"EndTime\": 1591204189.323193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204161.054248}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, RMSE): 0.711304451531\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, mean_absolute_QuantileLoss): 1710.0916326759898\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, mean_wQuantileLoss): 0.4320595332683148\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.1]): 0.45978030515284085\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.2]): 0.5076599693221349\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.3]): 0.510551156517306\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.4]): 0.489804257366803\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.5]): 0.4546876677877621\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.6]): 0.42023245116967295\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.7]): 0.3958960448752217\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.8]): 0.3592391431860591\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #test_score (algo-1, wQuantileLoss[0.9]): 0.2906848040370327\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.432059533268\u001b[0m\n",
      "\u001b[34m[06/03/2020 17:09:49 INFO 140291508827968] #quality_metric: host=algo-1, test RMSE <loss>=0.711304451531\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 304213.5360240936, \"sum\": 304213.5360240936, \"min\": 304213.5360240936}, \"setuptime\": {\"count\": 1, \"max\": 10.039806365966797, \"sum\": 10.039806365966797, \"min\": 10.039806365966797}}, \"EndTime\": 1591204189.343808, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591204189.323265}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-03 17:09:56 Completed - Training job completed\n",
      "Training seconds: 370\n",
      "Billable seconds: 370\n",
      "CPU times: user 1.3 s, sys: 78.8 ms, total: 1.38 s\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator_indicator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor_indicator = estimator_indicator.deploy(\n",
    "#    initial_instance_count=1,\n",
    "#    instance_type='ml.m4.xlarge',\n",
    "#    content_type=\"application/json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor_indicator = sagemaker.predictor.RealTimePredictor(endpoint='MLEND-Capstone-Project-2020-06-03-14-40-11-261')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_request(instance, num_samples, quantiles):\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-03-16</th>\n",
       "      <th>A</th>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>-0.752058</td>\n",
       "      <td>-1.622765</td>\n",
       "      <td>1.379154</td>\n",
       "      <td>0.829721</td>\n",
       "      <td>0.766182</td>\n",
       "      <td>-0.299314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>-1.155141</td>\n",
       "      <td>-1.345847</td>\n",
       "      <td>0.508184</td>\n",
       "      <td>0.746179</td>\n",
       "      <td>1.022953</td>\n",
       "      <td>-0.384040</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP</th>\n",
       "      <td>0.010572</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>-0.637966</td>\n",
       "      <td>-0.966827</td>\n",
       "      <td>0.851218</td>\n",
       "      <td>0.315813</td>\n",
       "      <td>0.047650</td>\n",
       "      <td>-0.163081</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.129969</td>\n",
       "      <td>-0.503010</td>\n",
       "      <td>-1.684244</td>\n",
       "      <td>1.363548</td>\n",
       "      <td>0.603027</td>\n",
       "      <td>0.750602</td>\n",
       "      <td>-0.149463</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABC</th>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>-1.058094</td>\n",
       "      <td>-0.997907</td>\n",
       "      <td>0.891929</td>\n",
       "      <td>0.373091</td>\n",
       "      <td>-0.156564</td>\n",
       "      <td>-0.012736</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1       PC2       PC3  \\\n",
       "Date       Ticker                                                      \n",
       "2010-03-16 A        0.005604  0.003822 -0.752058 -1.622765  1.379154   \n",
       "           AAL      0.001880  0.006881 -1.155141 -1.345847  0.508184   \n",
       "           AAP      0.010572  0.000981 -0.637966 -0.966827  0.851218   \n",
       "           AAPL     0.007148  0.129969 -0.503010 -1.684244  1.363548   \n",
       "           ABC      0.006258  0.004898 -1.058094 -0.997907  0.891929   \n",
       "\n",
       "                        PC4       PC5       PC6  target  \n",
       "Date       Ticker                                        \n",
       "2010-03-16 A       0.829721  0.766182 -0.299314       1  \n",
       "           AAL     0.746179  1.022953 -0.384040      -1  \n",
       "           AAP     0.315813  0.047650 -0.163081       1  \n",
       "           AAPL    0.603027  0.750602 -0.149463       0  \n",
       "           ABC     0.373091 -0.156564 -0.012736       1  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_indicator_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prediction(ticker,date):\n",
    "    date_pred = pd.Timestamp(date, freq='D')\n",
    "    date_start = date_pred-50\n",
    "    #\n",
    "    pred_df = stock_indicator_data.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "    result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "\n",
    "    pred = {\n",
    "            \"start\": str(date_pred),\n",
    "            \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
    "            \"dynamic_feat\": pred_df[['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']][date_start:date_pred].values.T.tolist()\n",
    "           }\n",
    "\n",
    "    req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "    res = predictor_indicator.predict(req)\n",
    "\n",
    "    prediction_data = json.loads(res.decode('utf-8'))\n",
    "    pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "    result_df['prediction'] = pred\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:3: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "  app.launch_new_instance()\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03886</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>-0.847888</td>\n",
       "      <td>4.83929</td>\n",
       "      <td>2.19196</td>\n",
       "      <td>-3.383544</td>\n",
       "      <td>1.655766</td>\n",
       "      <td>-0.505774</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1      PC2      PC3       PC4  \\\n",
       "Date       Ticker                                                              \n",
       "2019-01-23 AAPL      0.03886  0.026907 -0.847888  4.83929  2.19196 -3.383544   \n",
       "\n",
       "                        PC5       PC6  target  prediction  \n",
       "Date       Ticker                                          \n",
       "2019-01-23 AAPL    1.655766 -0.505774       1           1  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('AAPL', '2019-01-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_prediction_accuracy(ticker, date_range):\n",
    "    ticker = str(ticker)\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "\n",
    "    for date in date_range:\n",
    "        target.append(get_stock_prediction(ticker, date)['target'].values[0])\n",
    "        prediction.append(int(get_stock_prediction(ticker, date)['prediction'].values[0]))\n",
    "    target = list(np.array(target).reshape(252))\n",
    "    prediction = list(np.array(prediction).reshape(252))\n",
    "    data = {'target': list(target), 'prediction': list(prediction)}\n",
    "    prediction_df = pd.DataFrame(data=data,index=date_index, columns=['target','prediction'])\n",
    "    \n",
    "    return accuracy_score(target, prediction), prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction_accuracy('AAPL', date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
