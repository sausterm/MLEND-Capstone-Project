{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_preprocessed = pd.read_csv('stock_data_preprocessed.csv',parse_dates=True, index_col=[0,1])\n",
    "print(\"-1: \", stock_data_preprocessed[stock_data_preprocessed['target']==-1].size)\n",
    "print(\" 0: \", stock_data_preprocessed[stock_data_preprocessed['target']==0].size)\n",
    "print(\" 1: \", stock_data_preprocessed[stock_data_preprocessed['target']==1].size)\n",
    "#stock_data_preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 1\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp(\"2018-12-31 00:00:00\", freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + (2*k * prediction_length)].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training + (2*k * prediction_length)].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_dataset(filename, data): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for d in data:\n",
    "            json_line = json.dumps(d) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train.json\", training_data)\n",
    "write_json_dataset(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/test/test.json\n",
      "CPU times: user 2.82 s, sys: 311 ms, total: 3.13 s\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-01-04 00:00:00\", \"target\": [-1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-01 21:26:26 Starting - Starting the training job...\n",
      "2020-06-01 21:26:28 Starting - Launching requested ML instances......\n",
      "2020-06-01 21:27:31 Starting - Preparing the instances for training...\n",
      "2020-06-01 21:28:19 Downloading - Downloading input data...\n",
      "2020-06-01 21:28:45 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] number of observations: 1074589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] mean target length: 2188\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] min/mean/max target: -1.0/0.0764683055568/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] mean abs(target): 0.698935127756\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] number of observations: 10781660\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] mean target length: 2195\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] min/mean/max target: -1.0/0.078016464997/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] mean abs(target): 0.699434317165\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] nvidia-smi took: 0.0251898765564 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 237.9310131072998, \"sum\": 237.9310131072998, \"min\": 237.9310131072998}}, \"EndTime\": 1591046952.135294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046951.896421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:12 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 454.9570083618164, \"sum\": 454.9570083618164, \"min\": 454.9570083618164}}, \"EndTime\": 1591046952.35154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046952.135377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch[0] avg_epoch_loss=1.434263\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.43426251411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch[5] avg_epoch_loss=1.347553\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.34755325317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch [5]#011Speed: 1057.48 samples/sec#011loss=1.347553\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 1520.5039978027344, \"sum\": 1520.5039978027344, \"min\": 1520.5039978027344}}, \"EndTime\": 1591046953.872186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046952.351601}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=406.412889916 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.33785898685\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_5ca4bba6-b581-4608-99b1-c634cbc6fd74-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.88897132873535, \"sum\": 21.88897132873535, \"min\": 21.88897132873535}}, \"EndTime\": 1591046953.894778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046953.872269}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch[0] avg_epoch_loss=1.307173\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.30717325211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch[5] avg_epoch_loss=1.274586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.27458610137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch [5]#011Speed: 955.40 samples/sec#011loss=1.274586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.981035232544, \"sum\": 1435.981035232544, \"min\": 1435.981035232544}}, \"EndTime\": 1591046955.330912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046953.894858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.989672728 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.28925626278\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_4784194a-534a-4cc3-9991-313d0e9e070d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.56996726989746, \"sum\": 21.56996726989746, \"min\": 21.56996726989746}}, \"EndTime\": 1591046955.353148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046955.330997}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-01 21:29:05 Training - Training image download completed. Training in progress.\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch[0] avg_epoch_loss=1.323897\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.32389700413\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch[5] avg_epoch_loss=1.301920\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.30192011595\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch [5]#011Speed: 856.68 samples/sec#011loss=1.301920\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[2] Batch[10] avg_epoch_loss=1.275736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.24431614876\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[2] Batch [10]#011Speed: 509.51 samples/sec#011loss=1.244316\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1665.7500267028809, \"sum\": 1665.7500267028809, \"min\": 1665.7500267028809}}, \"EndTime\": 1591046957.019038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046955.353216}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=401.592202613 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.2757364945\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_cbb51694-2816-4cbc-a53b-e9332d5c27b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.693220138549805, \"sum\": 31.693220138549805, \"min\": 31.693220138549805}}, \"EndTime\": 1591046957.051301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046957.019118}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch[0] avg_epoch_loss=1.203594\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.20359432697\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch[5] avg_epoch_loss=1.217710\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.21770979961\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch [5]#011Speed: 1025.84 samples/sec#011loss=1.217710\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Epoch[3] Batch[10] avg_epoch_loss=1.210092\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.20095090866\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Epoch[3] Batch [10]#011Speed: 551.13 samples/sec#011loss=1.200951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.8831119537354, \"sum\": 1490.8831119537354, \"min\": 1490.8831119537354}}, \"EndTime\": 1591046958.542325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046957.051375}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.679527929 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.2100921219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b9b64dda-4a21-4d76-ad11-68c0fb8e4068-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.612001419067383, \"sum\": 20.612001419067383, \"min\": 20.612001419067383}}, \"EndTime\": 1591046958.563548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046958.542405}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch[0] avg_epoch_loss=1.175823\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.17582261562\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch[5] avg_epoch_loss=1.188703\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.18870335817\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch [5]#011Speed: 1051.15 samples/sec#011loss=1.188703\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1418.821096420288, \"sum\": 1418.821096420288, \"min\": 1418.821096420288}}, \"EndTime\": 1591046959.982502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046958.563615}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.220008062 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.18140784502\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_0db4e8a0-1d16-4b68-9c06-b5ddbc638c73-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.331829071044922, \"sum\": 27.331829071044922, \"min\": 27.331829071044922}}, \"EndTime\": 1591046960.010443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046959.982587}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch[0] avg_epoch_loss=1.216788\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.21678829193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch[5] avg_epoch_loss=1.202264\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.20226403077\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch [5]#011Speed: 1033.04 samples/sec#011loss=1.202264\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1455.3709030151367, \"sum\": 1455.3709030151367, \"min\": 1455.3709030151367}}, \"EndTime\": 1591046961.465959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046960.010515}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.902983998 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.17771522999\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_6c540192-de02-4353-ac66-05a0c230a269-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.270980834960938, \"sum\": 31.270980834960938, \"min\": 31.270980834960938}}, \"EndTime\": 1591046961.497824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046961.466043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch[0] avg_epoch_loss=1.206661\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.20666122437\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch[5] avg_epoch_loss=1.149882\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.14988209804\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch [5]#011Speed: 1040.98 samples/sec#011loss=1.149882\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[6] Batch[10] avg_epoch_loss=1.148364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.14654290676\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[6] Batch [10]#011Speed: 526.90 samples/sec#011loss=1.146543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.862102508545, \"sum\": 1520.862102508545, \"min\": 1520.862102508545}}, \"EndTime\": 1591046963.018824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046961.497897}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.732063104 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.14836428382\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_180e14ef-68ac-4cda-95d9-1babde6633e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.22196006774902, \"sum\": 33.22196006774902, \"min\": 33.22196006774902}}, \"EndTime\": 1591046963.052633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046963.018922}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch[0] avg_epoch_loss=1.096255\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.09625530243\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch[5] avg_epoch_loss=1.125320\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.125319918\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch [5]#011Speed: 989.42 samples/sec#011loss=1.125320\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1430.624008178711, \"sum\": 1430.624008178711, \"min\": 1430.624008178711}}, \"EndTime\": 1591046964.483405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046963.052712}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.338901291 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.12926626205\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_7a62479f-6d6c-4272-962e-fdd7d7aab96e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.73502540588379, \"sum\": 20.73502540588379, \"min\": 20.73502540588379}}, \"EndTime\": 1591046964.504816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046964.48349}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[0] avg_epoch_loss=1.076855\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.07685542107\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[5] avg_epoch_loss=1.104612\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.10461242994\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch [5]#011Speed: 1059.97 samples/sec#011loss=1.104612\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[10] avg_epoch_loss=1.091296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.0753166914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch [10]#011Speed: 547.63 samples/sec#011loss=1.075317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1473.8872051239014, \"sum\": 1473.8872051239014, \"min\": 1473.8872051239014}}, \"EndTime\": 1591046965.978834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046964.504888}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=465.402461675 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.09129618515\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_eeb65e78-7f8f-4418-9ee7-d47ece5cef28-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.09305763244629, \"sum\": 22.09305763244629, \"min\": 22.09305763244629}}, \"EndTime\": 1591046966.001504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046965.978903}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch[0] avg_epoch_loss=1.095278\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.09527790546\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch[5] avg_epoch_loss=1.059614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.05961447954\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch [5]#011Speed: 1032.40 samples/sec#011loss=1.059614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.45991897583, \"sum\": 1490.45991897583, \"min\": 1490.45991897583}}, \"EndTime\": 1591046967.49208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046966.001562}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=422.654313178 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.07141076326\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_f62ae336-7240-4652-a154-d834ca5f4fa6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.196998596191406, \"sum\": 32.196998596191406, \"min\": 32.196998596191406}}, \"EndTime\": 1591046967.52489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046967.492162}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch[0] avg_epoch_loss=1.059746\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.05974590778\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch[5] avg_epoch_loss=1.055233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.05523314079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch [5]#011Speed: 1047.59 samples/sec#011loss=1.055233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.3737716674805, \"sum\": 1422.3737716674805, \"min\": 1422.3737716674805}}, \"EndTime\": 1591046968.947409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046967.524967}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=432.347176113 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.05586122274\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e57c85bc-d58c-4101-b5a8-9c3bc9ababaa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.725011825561523, \"sum\": 20.725011825561523, \"min\": 20.725011825561523}}, \"EndTime\": 1591046968.968726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046968.947471}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch[0] avg_epoch_loss=1.057597\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.05759680271\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch[5] avg_epoch_loss=1.058986\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.05898612738\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch [5]#011Speed: 1028.71 samples/sec#011loss=1.058986\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Epoch[11] Batch[10] avg_epoch_loss=0.981045\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=0.887515249848\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Epoch[11] Batch [10]#011Speed: 519.53 samples/sec#011loss=0.887515\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.25394821167, \"sum\": 1516.25394821167, \"min\": 1516.25394821167}}, \"EndTime\": 1591046970.485108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046968.968794}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=427.997471735 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, train loss <loss>=0.981044819409\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c389346c-b50d-4d84-8a21-6d5ddbf90e1a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.47800064086914, \"sum\": 30.47800064086914, \"min\": 30.47800064086914}}, \"EndTime\": 1591046970.516098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046970.485183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch[0] avg_epoch_loss=1.046480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.04647958279\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch[5] avg_epoch_loss=1.043967\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.04396746556\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch [5]#011Speed: 879.94 samples/sec#011loss=1.043967\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1527.3561477661133, \"sum\": 1527.3561477661133, \"min\": 1527.3561477661133}}, \"EndTime\": 1591046972.043584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046970.516167}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=400.005766457 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.03745931983\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] Epoch[13] Batch[0] avg_epoch_loss=1.040768\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.0407679081\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] Epoch[13] Batch[5] avg_epoch_loss=1.000455\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.00045485298\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] Epoch[13] Batch [5]#011Speed: 520.63 samples/sec#011loss=1.000455\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1444.3769454956055, \"sum\": 1444.3769454956055, \"min\": 1444.3769454956055}}, \"EndTime\": 1591046973.488524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046972.043669}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.745486259 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.00560665131\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch[0] avg_epoch_loss=0.973218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.973218262196\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch[5] avg_epoch_loss=1.018670\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.01867032051\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch [5]#011Speed: 999.10 samples/sec#011loss=1.018670\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[14] Batch[10] avg_epoch_loss=1.036511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=1.05791945457\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[14] Batch [10]#011Speed: 520.46 samples/sec#011loss=1.057919\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.0660648345947, \"sum\": 1526.0660648345947, \"min\": 1526.0660648345947}}, \"EndTime\": 1591046975.01518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046973.488634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.933729411 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.03651083599\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch[0] avg_epoch_loss=1.023366\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.0233656168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch[5] avg_epoch_loss=0.998777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.99877657493\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch [5]#011Speed: 1029.44 samples/sec#011loss=0.998777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] Epoch[15] Batch[10] avg_epoch_loss=1.005851\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=1.01434094906\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] Epoch[15] Batch [10]#011Speed: 512.18 samples/sec#011loss=1.014341\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1542.576789855957, \"sum\": 1542.576789855957, \"min\": 1542.576789855957}}, \"EndTime\": 1591046976.558272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046975.01526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.936180413 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.00585129044\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch[0] avg_epoch_loss=0.974487\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.974487304688\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch[5] avg_epoch_loss=0.974869\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.97486932079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch [5]#011Speed: 993.04 samples/sec#011loss=0.974869\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.9577884674072, \"sum\": 1486.9577884674072, \"min\": 1486.9577884674072}}, \"EndTime\": 1591046978.045735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046976.558343}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.665832862 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.974238604307\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_5d588f61-d101-4ffa-97c6-ad60c7344677-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.818058013916016, \"sum\": 29.818058013916016, \"min\": 29.818058013916016}}, \"EndTime\": 1591046978.076143, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046978.045821}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch[0] avg_epoch_loss=1.008034\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.00803422928\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch[5] avg_epoch_loss=0.996452\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.996452351411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch [5]#011Speed: 1044.86 samples/sec#011loss=0.996452\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] Epoch[17] Batch[10] avg_epoch_loss=0.993349\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.989624071121\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] Epoch[17] Batch [10]#011Speed: 530.68 samples/sec#011loss=0.989624\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1505.0280094146729, \"sum\": 1505.0280094146729, \"min\": 1505.0280094146729}}, \"EndTime\": 1591046979.581294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046978.076207}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=449.792952386 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.993348587643\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch[0] avg_epoch_loss=0.944226\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.94422596693\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch[5] avg_epoch_loss=0.968781\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.968780736128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch [5]#011Speed: 1046.89 samples/sec#011loss=0.968781\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[18] Batch[10] avg_epoch_loss=0.969385\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.970109069347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[18] Batch [10]#011Speed: 537.83 samples/sec#011loss=0.970109\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1493.8538074493408, \"sum\": 1493.8538074493408, \"min\": 1493.8538074493408}}, \"EndTime\": 1591046981.07561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046979.581366}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=456.503286232 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.969384523955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_d227e5ad-530b-40a2-8438-efa9d09c6f71-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.90423011779785, \"sum\": 21.90423011779785, \"min\": 21.90423011779785}}, \"EndTime\": 1591046981.098114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046981.075683}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[19] Batch[0] avg_epoch_loss=0.960261\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.960261046886\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch[5] avg_epoch_loss=0.944997\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.944997131824\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch [5]#011Speed: 1023.09 samples/sec#011loss=0.944997\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch[10] avg_epoch_loss=0.984822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.03261078596\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch [10]#011Speed: 496.46 samples/sec#011loss=1.032611\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1567.7821636199951, \"sum\": 1567.7821636199951, \"min\": 1567.7821636199951}}, \"EndTime\": 1591046982.666057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046981.098201}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=414.567522956 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.984821520068\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch[0] avg_epoch_loss=0.900181\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.90018093586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch[5] avg_epoch_loss=0.950742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.95074219505\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch [5]#011Speed: 1005.57 samples/sec#011loss=0.950742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1469.6581363677979, \"sum\": 1469.6581363677979, \"min\": 1469.6581363677979}}, \"EndTime\": 1591046984.136249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046982.666135}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=414.347129542 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.951052796841\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_db3f2e13-5fef-4282-9274-7847fec9bc1e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.32009506225586, \"sum\": 31.32009506225586, \"min\": 31.32009506225586}}, \"EndTime\": 1591046984.168199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046984.136335}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] Epoch[21] Batch[0] avg_epoch_loss=0.859429\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.859429001808\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Epoch[21] Batch[5] avg_epoch_loss=0.910801\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.910800645749\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Epoch[21] Batch [5]#011Speed: 1013.13 samples/sec#011loss=0.910801\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1478.4798622131348, \"sum\": 1478.4798622131348, \"min\": 1478.4798622131348}}, \"EndTime\": 1591046985.646816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046984.168272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.137787481 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.937435770035\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_88e11870-345d-40a1-aa8c-4ce4df868a22-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.582977294921875, \"sum\": 29.582977294921875, \"min\": 29.582977294921875}}, \"EndTime\": 1591046985.676963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046985.646897}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch[0] avg_epoch_loss=0.900517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.90051728487\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch[5] avg_epoch_loss=0.904150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.90415028731\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch [5]#011Speed: 979.91 samples/sec#011loss=0.904150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1517.3249244689941, \"sum\": 1517.3249244689941, \"min\": 1517.3249244689941}}, \"EndTime\": 1591046987.194438, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046985.677041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=415.828115958 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.915119498968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_93c67a9b-49d9-44c7-8154-e864f7b3b793-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.981861114501953, \"sum\": 19.981861114501953, \"min\": 19.981861114501953}}, \"EndTime\": 1591046987.21509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046987.194525}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] Epoch[23] Batch[0] avg_epoch_loss=0.934589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.934589147568\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch[5] avg_epoch_loss=0.893955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.893955220779\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch [5]#011Speed: 1019.85 samples/sec#011loss=0.893955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch[10] avg_epoch_loss=0.895388\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.897106790543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch [10]#011Speed: 536.78 samples/sec#011loss=0.897107\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.322093963623, \"sum\": 1510.322093963623, \"min\": 1510.322093963623}}, \"EndTime\": 1591046988.725555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046987.21517}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=450.201400929 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.89538775249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e742f2d4-2c49-4bed-b448-b72961016676-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.193981170654297, \"sum\": 21.193981170654297, \"min\": 21.193981170654297}}, \"EndTime\": 1591046988.747294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046988.725628}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch[0] avg_epoch_loss=0.989229\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.989228904247\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch[5] avg_epoch_loss=0.916364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.916364053885\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch [5]#011Speed: 1013.31 samples/sec#011loss=0.916364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[24] Batch[10] avg_epoch_loss=0.932889\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.952719438076\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[24] Batch [10]#011Speed: 546.80 samples/sec#011loss=0.952719\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1500.5691051483154, \"sum\": 1500.5691051483154, \"min\": 1500.5691051483154}}, \"EndTime\": 1591046990.247988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046988.747359}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=436.466174564 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.932889228517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[25] Batch[0] avg_epoch_loss=0.871741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.871741056442\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch[5] avg_epoch_loss=0.896741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.896740913391\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch [5]#011Speed: 1015.28 samples/sec#011loss=0.896741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch[10] avg_epoch_loss=0.882382\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=0.865150702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch [10]#011Speed: 524.81 samples/sec#011loss=0.865151\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.7550525665283, \"sum\": 1520.7550525665283, \"min\": 1520.7550525665283}}, \"EndTime\": 1591046991.769274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046990.248069}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=424.098242235 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.882381726395\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_2eeff84a-8846-4632-9d4b-7dfb985b7aa8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.378082275390625, \"sum\": 27.378082275390625, \"min\": 27.378082275390625}}, \"EndTime\": 1591046991.797239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046991.769355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:52 INFO 140176366790464] Epoch[26] Batch[0] avg_epoch_loss=0.900884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.90088391304\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[26] Batch[5] avg_epoch_loss=0.930150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.930150111516\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[26] Batch [5]#011Speed: 513.46 samples/sec#011loss=0.930150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.180856704712, \"sum\": 1508.180856704712, \"min\": 1508.180856704712}}, \"EndTime\": 1591046993.305568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046991.797317}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=397.798325935 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.919858849049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[27] Batch[0] avg_epoch_loss=0.872662\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.872662246227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Epoch[27] Batch[5] avg_epoch_loss=0.880638\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.880638460318\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Epoch[27] Batch [5]#011Speed: 1033.57 samples/sec#011loss=0.880638\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1451.3449668884277, \"sum\": 1451.3449668884277, \"min\": 1451.3449668884277}}, \"EndTime\": 1591046994.757464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046993.305651}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.019173411 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.857738488913\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b407efdd-c237-4b12-ac97-97d70583c0ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.90289306640625, \"sum\": 23.90289306640625, \"min\": 23.90289306640625}}, \"EndTime\": 1591046994.781988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046994.75755}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch[0] avg_epoch_loss=0.805616\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.805615723133\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch[5] avg_epoch_loss=0.833884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.83388414979\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch [5]#011Speed: 1053.10 samples/sec#011loss=0.833884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1402.8701782226562, \"sum\": 1402.8701782226562, \"min\": 1402.8701782226562}}, \"EndTime\": 1591046996.184978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046994.782041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.086631121 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.824995923042\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_af48909c-b47f-41c5-aa56-8357005b75d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.855924606323242, \"sum\": 23.855924606323242, \"min\": 23.855924606323242}}, \"EndTime\": 1591046996.209454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046996.185046}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] Epoch[29] Batch[0] avg_epoch_loss=0.852328\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.852328002453\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch[5] avg_epoch_loss=0.813227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.813226789236\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch [5]#011Speed: 1041.36 samples/sec#011loss=0.813227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch[10] avg_epoch_loss=0.829030\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.847994363308\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch [10]#011Speed: 546.26 samples/sec#011loss=0.847994\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1489.7451400756836, \"sum\": 1489.7451400756836, \"min\": 1489.7451400756836}}, \"EndTime\": 1591046997.69934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046996.209526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=452.393943758 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.829030231996\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch[0] avg_epoch_loss=0.797326\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.797326207161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch[5] avg_epoch_loss=0.826752\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.826751907667\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch [5]#011Speed: 1062.37 samples/sec#011loss=0.826752\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[30] Batch[10] avg_epoch_loss=0.772874\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=0.70821980834\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[30] Batch [10]#011Speed: 548.04 samples/sec#011loss=0.708220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1472.1829891204834, \"sum\": 1472.1829891204834, \"min\": 1472.1829891204834}}, \"EndTime\": 1591046999.172072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046997.69941}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.962559449 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.7728736807\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_40fd83f6-240e-429b-bf24-f4cadd3079e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.91193199157715, \"sum\": 20.91193199157715, \"min\": 20.91193199157715}}, \"EndTime\": 1591046999.193551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046999.172136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[31] Batch[0] avg_epoch_loss=0.690408\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.690408170223\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch[5] avg_epoch_loss=0.771321\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.771320551634\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch [5]#011Speed: 1057.79 samples/sec#011loss=0.771321\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch[10] avg_epoch_loss=0.776506\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=0.782727503777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch [10]#011Speed: 506.31 samples/sec#011loss=0.782728\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1525.5892276763916, \"sum\": 1525.5892276763916, \"min\": 1525.5892276763916}}, \"EndTime\": 1591047000.719277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046999.193627}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.310304845 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.776505529881\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch[0] avg_epoch_loss=0.747332\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.747332274914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch[5] avg_epoch_loss=0.777958\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.777958105008\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch [5]#011Speed: 904.56 samples/sec#011loss=0.777958\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1547.8250980377197, \"sum\": 1547.8250980377197, \"min\": 1547.8250980377197}}, \"EndTime\": 1591047002.26761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047000.719355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=392.781348354 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.760727983713\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_0bd85a49-98a3-4a76-bde0-48584aa7952b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.591995239257812, \"sum\": 23.591995239257812, \"min\": 23.591995239257812}}, \"EndTime\": 1591047002.29186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047002.267688}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] Epoch[33] Batch[0] avg_epoch_loss=0.679262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.679261684418\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Epoch[33] Batch[5] avg_epoch_loss=0.704604\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.704604069392\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Epoch[33] Batch [5]#011Speed: 972.71 samples/sec#011loss=0.704604\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1465.4648303985596, \"sum\": 1465.4648303985596, \"min\": 1465.4648303985596}}, \"EndTime\": 1591047003.757483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047002.291946}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=422.361273366 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.758519339561\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_3f172a59-3fdd-4b04-8a33-587e9660c683-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.65699005126953, \"sum\": 21.65699005126953, \"min\": 21.65699005126953}}, \"EndTime\": 1591047003.779776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047003.757547}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch[0] avg_epoch_loss=0.685365\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.68536490202\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch[5] avg_epoch_loss=0.711852\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.711852282286\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch [5]#011Speed: 1047.69 samples/sec#011loss=0.711852\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[34] Batch[10] avg_epoch_loss=0.718220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=0.72586145401\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[34] Batch [10]#011Speed: 535.75 samples/sec#011loss=0.725861\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1519.247055053711, \"sum\": 1519.247055053711, \"min\": 1519.247055053711}}, \"EndTime\": 1591047005.29915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047003.779839}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=458.74699171 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.718220087615\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_703bd155-b559-472a-8eea-88f581a3d661-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.399084091186523, \"sum\": 30.399084091186523, \"min\": 30.399084091186523}}, \"EndTime\": 1591047005.33008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047005.299225}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[35] Batch[0] avg_epoch_loss=0.663839\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.66383934021\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch[5] avg_epoch_loss=0.678925\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.678924550613\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch [5]#011Speed: 1026.90 samples/sec#011loss=0.678925\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch[10] avg_epoch_loss=0.638208\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.589347708225\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch [10]#011Speed: 494.47 samples/sec#011loss=0.589348\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1580.7859897613525, \"sum\": 1580.7859897613525, \"min\": 1580.7859897613525}}, \"EndTime\": 1591047006.911003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047005.330153}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.86854421 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.638207804073\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_1f88cf85-aa6a-4a59-bb88-f0712d0bcf36-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.71007537841797, \"sum\": 32.71007537841797, \"min\": 32.71007537841797}}, \"EndTime\": 1591047006.944249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047006.911082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch[0] avg_epoch_loss=0.784701\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.7847006917\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch[5] avg_epoch_loss=0.709399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.70939947168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch [5]#011Speed: 922.08 samples/sec#011loss=0.709399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] Epoch[36] Batch[10] avg_epoch_loss=0.642862\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=0.563017517328\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] Epoch[36] Batch [10]#011Speed: 498.17 samples/sec#011loss=0.563018\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1616.5931224822998, \"sum\": 1616.5931224822998, \"min\": 1616.5931224822998}}, \"EndTime\": 1591047008.560985, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047006.944326}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.648318169 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.642862219702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch[0] avg_epoch_loss=0.639017\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.639016926289\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch[5] avg_epoch_loss=0.659143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.659143437942\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch [5]#011Speed: 1011.03 samples/sec#011loss=0.659143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[37] Batch[10] avg_epoch_loss=0.640500\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=0.618128192425\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[37] Batch [10]#011Speed: 529.71 samples/sec#011loss=0.618128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1532.477855682373, \"sum\": 1532.477855682373, \"min\": 1532.477855682373}}, \"EndTime\": 1591047010.093961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047008.561064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=418.245414824 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.640500144525\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch[0] avg_epoch_loss=0.569648\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.569647967815\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch[5] avg_epoch_loss=0.622329\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.622328837713\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch [5]#011Speed: 1050.82 samples/sec#011loss=0.622329\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1462.6328945159912, \"sum\": 1462.6328945159912, \"min\": 1462.6328945159912}}, \"EndTime\": 1591047011.557092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047010.094039}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.694496738 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.608988711238\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_1130916f-95dc-4ed1-b09c-44ef7fd39402-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.601055145263672, \"sum\": 23.601055145263672, \"min\": 23.601055145263672}}, \"EndTime\": 1591047011.58131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047011.557175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch[0] avg_epoch_loss=0.636542\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.63654178381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch[5] avg_epoch_loss=0.626477\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.626477410396\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch [5]#011Speed: 1027.83 samples/sec#011loss=0.626477\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1454.9999237060547, \"sum\": 1454.9999237060547, \"min\": 1454.9999237060547}}, \"EndTime\": 1591047013.036443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047011.581377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=424.700465469 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.644965153933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch[0] avg_epoch_loss=0.650846\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.650845944881\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch[5] avg_epoch_loss=0.589878\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.589877973\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch [5]#011Speed: 1053.75 samples/sec#011loss=0.589878\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] Epoch[40] Batch[10] avg_epoch_loss=0.628673\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=0.675227642059\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] Epoch[40] Batch [10]#011Speed: 525.85 samples/sec#011loss=0.675228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.4601287841797, \"sum\": 1520.4601287841797, \"min\": 1520.4601287841797}}, \"EndTime\": 1591047014.55748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047013.036527}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.785957162 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.628673277118\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch[0] avg_epoch_loss=0.540145\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.540144562721\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch[5] avg_epoch_loss=0.614805\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.61480542024\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch [5]#011Speed: 1033.56 samples/sec#011loss=0.614805\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1445.2619552612305, \"sum\": 1445.2619552612305, \"min\": 1445.2619552612305}}, \"EndTime\": 1591047016.003262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047014.557554}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.495966072 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.632378613949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch[0] avg_epoch_loss=0.599517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.599517285824\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch[5] avg_epoch_loss=0.578702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.578702489535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch [5]#011Speed: 1044.48 samples/sec#011loss=0.578702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Epoch[42] Batch[10] avg_epoch_loss=0.608220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=0.643640971184\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Epoch[42] Batch [10]#011Speed: 515.94 samples/sec#011loss=0.643641\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.4539813995361, \"sum\": 1516.4539813995361, \"min\": 1516.4539813995361}}, \"EndTime\": 1591047017.520317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047016.003334}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.144838229 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.608219981194\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_6b355eb6-5353-42d8-960f-ab8c3271abf7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.00419044494629, \"sum\": 31.00419044494629, \"min\": 31.00419044494629}}, \"EndTime\": 1591047017.551926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047017.52041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch[0] avg_epoch_loss=0.652531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.652530789375\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch[5] avg_epoch_loss=0.583306\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.583305845658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch [5]#011Speed: 1051.99 samples/sec#011loss=0.583306\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[43] Batch[10] avg_epoch_loss=0.660428\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.752974402905\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[43] Batch [10]#011Speed: 526.94 samples/sec#011loss=0.752974\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1512.1538639068604, \"sum\": 1512.1538639068604, \"min\": 1512.1538639068604}}, \"EndTime\": 1591047019.064214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047017.551996}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=435.768134248 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.660427917134\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch[0] avg_epoch_loss=0.417933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.417933017015\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch[5] avg_epoch_loss=0.573763\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.573763405283\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch [5]#011Speed: 1026.28 samples/sec#011loss=0.573763\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Epoch[44] Batch[10] avg_epoch_loss=0.598684\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.628589510918\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Epoch[44] Batch [10]#011Speed: 540.49 samples/sec#011loss=0.628590\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1512.887954711914, \"sum\": 1512.887954711914, \"min\": 1512.887954711914}}, \"EndTime\": 1591047020.577655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047019.064294}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=450.764550887 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.59868436239\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e33839ee-ad1c-4816-8189-c82dd7dfa918-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.82316780090332, \"sum\": 21.82316780090332, \"min\": 21.82316780090332}}, \"EndTime\": 1591047020.600062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047020.577721}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch[0] avg_epoch_loss=0.526484\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.526483535767\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch[5] avg_epoch_loss=0.554193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.554193456968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch [5]#011Speed: 1038.26 samples/sec#011loss=0.554193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.3970851898193, \"sum\": 1426.3970851898193, \"min\": 1426.3970851898193}}, \"EndTime\": 1591047022.026577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047020.600122}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.244747105 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.534400874376\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_31c5899b-2c77-40cb-9d36-88d22e292c8e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.10004425048828, \"sum\": 21.10004425048828, \"min\": 21.10004425048828}}, \"EndTime\": 1591047022.04832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047022.026652}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch[0] avg_epoch_loss=0.542186\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.542185842991\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch[5] avg_epoch_loss=0.497872\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.497871950269\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch [5]#011Speed: 1056.82 samples/sec#011loss=0.497872\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] Epoch[46] Batch[10] avg_epoch_loss=0.563041\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.641244953871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] Epoch[46] Batch [10]#011Speed: 545.60 samples/sec#011loss=0.641245\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1475.7030010223389, \"sum\": 1475.7030010223389, \"min\": 1475.7030010223389}}, \"EndTime\": 1591047023.524171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047022.048396}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=462.115955993 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.563041497361\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch[0] avg_epoch_loss=0.617388\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.617388248444\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch[5] avg_epoch_loss=0.500635\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.500635142128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch [5]#011Speed: 1039.63 samples/sec#011loss=0.500635\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[47] Batch[10] avg_epoch_loss=0.549917\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.609054267406\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[47] Batch [10]#011Speed: 528.87 samples/sec#011loss=0.609054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1524.4121551513672, \"sum\": 1524.4121551513672, \"min\": 1524.4121551513672}}, \"EndTime\": 1591047025.049136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047023.52425}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.953317705 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.549916562709\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[48] Batch[0] avg_epoch_loss=0.512581\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.512580811977\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch[5] avg_epoch_loss=0.496037\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.496037264665\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch [5]#011Speed: 1010.11 samples/sec#011loss=0.496037\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch[10] avg_epoch_loss=0.458899\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.414332565665\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch [10]#011Speed: 522.70 samples/sec#011loss=0.414333\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1564.054012298584, \"sum\": 1564.054012298584, \"min\": 1564.054012298584}}, \"EndTime\": 1591047026.61372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047025.049214}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=417.476189482 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.45889876512\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_f72693f2-b942-4b5e-97fa-d4867b0d40f6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.675918579101562, \"sum\": 23.675918579101562, \"min\": 23.675918579101562}}, \"EndTime\": 1591047026.638051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047026.613789}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch[0] avg_epoch_loss=0.516531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.516531229019\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch[5] avg_epoch_loss=0.484970\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.484969869256\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch [5]#011Speed: 1029.26 samples/sec#011loss=0.484970\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[49] Batch[10] avg_epoch_loss=0.448160\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.403988751769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[49] Batch [10]#011Speed: 539.28 samples/sec#011loss=0.403989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1504.9200057983398, \"sum\": 1504.9200057983398, \"min\": 1504.9200057983398}}, \"EndTime\": 1591047028.14313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047026.638139}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.493368254 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.448160270398\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_7b3377e1-061d-4122-8fd8-2112da2b3f53-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.9808349609375, \"sum\": 20.9808349609375, \"min\": 20.9808349609375}}, \"EndTime\": 1591047028.164716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047028.14321}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[50] Batch[0] avg_epoch_loss=0.587347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.587346613407\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Epoch[50] Batch[5] avg_epoch_loss=0.542049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.54204852879\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Epoch[50] Batch [5]#011Speed: 1036.25 samples/sec#011loss=0.542049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1423.9718914031982, \"sum\": 1423.9718914031982, \"min\": 1423.9718914031982}}, \"EndTime\": 1591047029.588825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047028.164786}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=412.194249202 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.439182299376\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_a91c38a4-3cee-482f-b660-a18741e75db7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.697044372558594, \"sum\": 21.697044372558594, \"min\": 21.697044372558594}}, \"EndTime\": 1591047029.611176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047029.588898}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch[0] avg_epoch_loss=0.562171\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.562170863152\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch[5] avg_epoch_loss=0.504951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.504951417446\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch [5]#011Speed: 1015.42 samples/sec#011loss=0.504951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[51] Batch[10] avg_epoch_loss=0.506237\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.507779818773\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[51] Batch [10]#011Speed: 522.18 samples/sec#011loss=0.507780\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1524.31321144104, \"sum\": 1524.31321144104, \"min\": 1524.31321144104}}, \"EndTime\": 1591047031.135638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047029.611256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.035673823 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.506237054413\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[52] Batch[0] avg_epoch_loss=0.312347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.312346637249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch[5] avg_epoch_loss=0.424499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.424499183893\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch [5]#011Speed: 1025.74 samples/sec#011loss=0.424499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch[10] avg_epoch_loss=0.420736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.416219693422\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch [10]#011Speed: 524.62 samples/sec#011loss=0.416220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1554.2430877685547, \"sum\": 1554.2430877685547, \"min\": 1554.2430877685547}}, \"EndTime\": 1591047032.690423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047031.135716}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.049433997 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.420735779134\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c4eb0e6f-c735-47a6-94ff-d6624037f5fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.22895622253418, \"sum\": 22.22895622253418, \"min\": 22.22895622253418}}, \"EndTime\": 1591047032.713197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047032.690487}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch[0] avg_epoch_loss=0.636280\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.63628000021\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch[5] avg_epoch_loss=0.493469\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.493469024698\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch [5]#011Speed: 1002.17 samples/sec#011loss=0.493469\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[53] Batch[10] avg_epoch_loss=0.446379\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=0.389871093631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[53] Batch [10]#011Speed: 530.24 samples/sec#011loss=0.389871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1515.8979892730713, \"sum\": 1515.8979892730713, \"min\": 1515.8979892730713}}, \"EndTime\": 1591047034.229281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047032.713275}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.956515148 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.446379056031\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[54] Batch[0] avg_epoch_loss=0.585544\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.585544168949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] Epoch[54] Batch[5] avg_epoch_loss=0.506531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.506531119347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] Epoch[54] Batch [5]#011Speed: 1047.04 samples/sec#011loss=0.506531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1434.4329833984375, \"sum\": 1434.4329833984375, \"min\": 1434.4329833984375}}, \"EndTime\": 1591047035.664246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047034.229362}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=420.339113214 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.531550344825\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch[0] avg_epoch_loss=0.521139\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.52113866806\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch[5] avg_epoch_loss=0.443499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.443498805165\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch [5]#011Speed: 1017.70 samples/sec#011loss=0.443499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1437.1540546417236, \"sum\": 1437.1540546417236, \"min\": 1437.1540546417236}}, \"EndTime\": 1591047037.102044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047035.664329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=420.938053999 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.485042539239\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch[0] avg_epoch_loss=0.519255\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.519254803658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch[5] avg_epoch_loss=0.441065\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.441064625978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch [5]#011Speed: 1039.01 samples/sec#011loss=0.441065\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] Epoch[56] Batch[10] avg_epoch_loss=0.500867\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=0.572630298138\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] Epoch[56] Batch [10]#011Speed: 550.95 samples/sec#011loss=0.572630\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1470.2210426330566, \"sum\": 1470.2210426330566, \"min\": 1470.2210426330566}}, \"EndTime\": 1591047038.572881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047037.102116}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=440.034273745 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.500867204233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch[0] avg_epoch_loss=0.522297\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.522297024727\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch[5] avg_epoch_loss=0.448813\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.448813100656\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch [5]#011Speed: 1052.69 samples/sec#011loss=0.448813\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[57] Batch[10] avg_epoch_loss=0.612590\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.809122800827\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[57] Batch [10]#011Speed: 542.01 samples/sec#011loss=0.809123\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1477.4610996246338, \"sum\": 1477.4610996246338, \"min\": 1477.4610996246338}}, \"EndTime\": 1591047040.050907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047038.57296}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.81596759 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.612590237097\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch[0] avg_epoch_loss=0.539978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.539977669716\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch[5] avg_epoch_loss=0.517978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.517978390058\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch [5]#011Speed: 1011.95 samples/sec#011loss=0.517978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.361083984375, \"sum\": 1426.361083984375, \"min\": 1426.361083984375}}, \"EndTime\": 1591047041.477856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047040.050991}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=421.319185828 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.464243760705\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch[0] avg_epoch_loss=0.443170\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.443169862032\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch[5] avg_epoch_loss=0.476053\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.476052607099\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch [5]#011Speed: 1056.49 samples/sec#011loss=0.476053\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1407.660961151123, \"sum\": 1407.660961151123, \"min\": 1407.660961151123}}, \"EndTime\": 1591047042.886101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047041.477925}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.037864796 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.529165032506\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch[0] avg_epoch_loss=0.403933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.403933376074\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch[5] avg_epoch_loss=0.428443\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.428443282843\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch [5]#011Speed: 1052.21 samples/sec#011loss=0.428443\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1420.5610752105713, \"sum\": 1420.5610752105713, \"min\": 1420.5610752105713}}, \"EndTime\": 1591047044.307232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047042.886201}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=419.523596181 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.461417669058\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] Epoch[61] Batch[0] avg_epoch_loss=0.348168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.348167806864\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch[5] avg_epoch_loss=0.394218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.394217674931\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch [5]#011Speed: 1047.05 samples/sec#011loss=0.394218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch[10] avg_epoch_loss=0.361949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.323226645589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch [10]#011Speed: 528.64 samples/sec#011loss=0.323227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1496.3538646697998, \"sum\": 1496.3538646697998, \"min\": 1496.3538646697998}}, \"EndTime\": 1591047045.804147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047044.307295}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.014327707 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.36194902523\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e017a7af-40bf-4007-80b7-cf44632c9840-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.041860580444336, \"sum\": 31.041860580444336, \"min\": 31.041860580444336}}, \"EndTime\": 1591047045.835785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047045.804226}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch[0] avg_epoch_loss=0.405998\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.405998051167\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch[5] avg_epoch_loss=0.416228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.416227628787\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch [5]#011Speed: 953.08 samples/sec#011loss=0.416228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.9329643249512, \"sum\": 1516.9329643249512, \"min\": 1516.9329643249512}}, \"EndTime\": 1591047047.352857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047045.835858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=410.004385983 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.453421983123\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] Epoch[63] Batch[0] avg_epoch_loss=0.408436\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.408436208963\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch[5] avg_epoch_loss=0.402242\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.402241925399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch [5]#011Speed: 1045.92 samples/sec#011loss=0.402242\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch[10] avg_epoch_loss=0.415737\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.431930136681\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch [10]#011Speed: 532.80 samples/sec#011loss=0.431930\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1521.9471454620361, \"sum\": 1521.9471454620361, \"min\": 1521.9471454620361}}, \"EndTime\": 1591047048.875357, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047047.352942}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.104771633 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.41573656689\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch[0] avg_epoch_loss=0.516642\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.516642093658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch[5] avg_epoch_loss=0.459528\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.459528103471\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch [5]#011Speed: 1046.48 samples/sec#011loss=0.459528\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] Epoch[64] Batch[10] avg_epoch_loss=0.398084\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.324351853132\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] Epoch[64] Batch [10]#011Speed: 540.92 samples/sec#011loss=0.324352\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1527.1978378295898, \"sum\": 1527.1978378295898, \"min\": 1527.1978378295898}}, \"EndTime\": 1591047050.403065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047048.875437}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.823050706 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.398084353317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch[0] avg_epoch_loss=0.355190\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.355190455914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch[5] avg_epoch_loss=0.404535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.404534791907\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch [5]#011Speed: 1057.11 samples/sec#011loss=0.404535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1437.5288486480713, \"sum\": 1437.5288486480713, \"min\": 1437.5288486480713}}, \"EndTime\": 1591047051.841138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047050.403137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.519210951 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.416255965829\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch[0] avg_epoch_loss=0.345558\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.34555798769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch[5] avg_epoch_loss=0.396587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.396586852769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch [5]#011Speed: 969.49 samples/sec#011loss=0.396587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] Epoch[66] Batch[10] avg_epoch_loss=0.376723\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.352887293696\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] Epoch[66] Batch [10]#011Speed: 528.33 samples/sec#011loss=0.352887\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1586.1620903015137, \"sum\": 1586.1620903015137, \"min\": 1586.1620903015137}}, \"EndTime\": 1591047053.427831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047051.841221}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=409.132306593 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.376723416827\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[0] avg_epoch_loss=0.474371\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.474370598793\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[5] avg_epoch_loss=0.440296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.440295691291\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch [5]#011Speed: 1049.52 samples/sec#011loss=0.440296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[10] avg_epoch_loss=0.369463\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=0.284464401007\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch [10]#011Speed: 541.68 samples/sec#011loss=0.284464\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.0057830810547, \"sum\": 1486.0057830810547, \"min\": 1486.0057830810547}}, \"EndTime\": 1591047054.914367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047053.427911}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=444.113986193 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.369463286617\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch[0] avg_epoch_loss=0.355923\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.355922937393\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch[5] avg_epoch_loss=0.388219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.388218899568\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch [5]#011Speed: 1043.86 samples/sec#011loss=0.388219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[68] Batch[10] avg_epoch_loss=0.378072\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=0.365894705057\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[68] Batch [10]#011Speed: 538.87 samples/sec#011loss=0.365895\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1485.0718975067139, \"sum\": 1485.0718975067139, \"min\": 1485.0718975067139}}, \"EndTime\": 1591047056.39997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047054.914433}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.7532507 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.378071538427\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[69] Batch[0] avg_epoch_loss=0.525512\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.52551150322\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Epoch[69] Batch[5] avg_epoch_loss=0.379894\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.379894350966\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Epoch[69] Batch [5]#011Speed: 1037.18 samples/sec#011loss=0.379894\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1425.5170822143555, \"sum\": 1425.5170822143555, \"min\": 1425.5170822143555}}, \"EndTime\": 1591047057.826033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047056.400049}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.497100325 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.349440449476\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e243123e-7368-44e8-b7bc-42f404be0891-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.690202713012695, \"sum\": 20.690202713012695, \"min\": 20.690202713012695}}, \"EndTime\": 1591047057.847335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047057.826099}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch[0] avg_epoch_loss=0.400950\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.400950103998\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch[5] avg_epoch_loss=0.397989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.397989327709\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch [5]#011Speed: 1032.68 samples/sec#011loss=0.397989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.9488487243652, \"sum\": 1435.9488487243652, \"min\": 1435.9488487243652}}, \"EndTime\": 1591047059.283412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047057.847402}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=445.660663605 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.385269227624\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] Epoch[71] Batch[0] avg_epoch_loss=0.390674\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.390673756599\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch[5] avg_epoch_loss=0.448717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.448717311025\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch [5]#011Speed: 1038.02 samples/sec#011loss=0.448717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch[10] avg_epoch_loss=0.421969\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.389870655537\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch [10]#011Speed: 535.95 samples/sec#011loss=0.389871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1515.0949954986572, \"sum\": 1515.0949954986572, \"min\": 1515.0949954986572}}, \"EndTime\": 1591047060.799046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047059.283496}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.642831922 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.421968831257\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch[0] avg_epoch_loss=0.471578\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.471578240395\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch[5] avg_epoch_loss=0.369161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.369160860777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch [5]#011Speed: 1013.41 samples/sec#011loss=0.369161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1482.5160503387451, \"sum\": 1482.5160503387451, \"min\": 1482.5160503387451}}, \"EndTime\": 1591047062.2821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047060.799125}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=416.829398046 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.382378193736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] Epoch[73] Batch[0] avg_epoch_loss=0.337137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.337137371302\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch[5] avg_epoch_loss=0.374672\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.374671916167\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch [5]#011Speed: 1020.31 samples/sec#011loss=0.374672\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch[10] avg_epoch_loss=0.477594\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.601100707054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch [10]#011Speed: 544.68 samples/sec#011loss=0.601101\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1480.7050228118896, \"sum\": 1480.7050228118896, \"min\": 1480.7050228118896}}, \"EndTime\": 1591047063.763321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047062.28217}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.21770767 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.477594093843\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch[0] avg_epoch_loss=0.290773\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.290773153305\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch[5] avg_epoch_loss=0.304497\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.30449689428\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch [5]#011Speed: 1044.18 samples/sec#011loss=0.304497\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[74] Batch[10] avg_epoch_loss=0.342296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.387655472755\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[74] Batch [10]#011Speed: 518.28 samples/sec#011loss=0.387655\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.5639953613281, \"sum\": 1508.5639953613281, \"min\": 1508.5639953613281}}, \"EndTime\": 1591047065.272417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047063.7634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.84105976 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.342296248133\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_013f5611-ef0a-4b87-92e5-f572d50a8871-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.178951263427734, \"sum\": 31.178951263427734, \"min\": 31.178951263427734}}, \"EndTime\": 1591047065.304184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047065.272494}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[75] Batch[0] avg_epoch_loss=0.473744\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.473743975163\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch[5] avg_epoch_loss=0.422812\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.422811823587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch [5]#011Speed: 1022.76 samples/sec#011loss=0.422812\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch[10] avg_epoch_loss=0.432398\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.443900996447\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch [10]#011Speed: 534.88 samples/sec#011loss=0.443901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1540.1380062103271, \"sum\": 1540.1380062103271, \"min\": 1540.1380062103271}}, \"EndTime\": 1591047066.844462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047065.304257}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=418.106353266 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.43239781125\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch[0] avg_epoch_loss=0.473007\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.473006844521\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch[5] avg_epoch_loss=0.423262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.423261702061\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch [5]#011Speed: 1039.57 samples/sec#011loss=0.423262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1423.961877822876, \"sum\": 1423.961877822876, \"min\": 1423.961877822876}}, \"EndTime\": 1591047068.268999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047066.844563}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.479410076 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.389875084162\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] Epoch[77] Batch[0] avg_epoch_loss=0.322719\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.322719097137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] Epoch[77] Batch[5] avg_epoch_loss=0.378475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.378475437562\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] Epoch[77] Batch [5]#011Speed: 1053.76 samples/sec#011loss=0.378475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1431.480884552002, \"sum\": 1431.480884552002, \"min\": 1431.480884552002}}, \"EndTime\": 1591047069.701046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047068.26907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=435.875043342 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.39159155488\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch[0] avg_epoch_loss=0.406873\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.406873047352\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch[5] avg_epoch_loss=0.348675\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.348674642543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch [5]#011Speed: 1059.96 samples/sec#011loss=0.348675\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[78] Batch[10] avg_epoch_loss=0.352660\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.357442069054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[78] Batch [10]#011Speed: 487.22 samples/sec#011loss=0.357442\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1570.1401233673096, \"sum\": 1570.1401233673096, \"min\": 1570.1401233673096}}, \"EndTime\": 1591047071.271717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047069.701129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=427.955697814 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.352659836411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[79] Batch[0] avg_epoch_loss=0.286215\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.286214739084\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Epoch[79] Batch[5] avg_epoch_loss=0.309093\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.309093082945\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Epoch[79] Batch [5]#011Speed: 877.88 samples/sec#011loss=0.309093\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1513.5161876678467, \"sum\": 1513.5161876678467, \"min\": 1513.5161876678467}}, \"EndTime\": 1591047072.785735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047071.271796}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=416.876420812 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.340787410736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_83b90d2e-54a0-495a-9cf0-e31a1ab772d5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.57298469543457, \"sum\": 32.57298469543457, \"min\": 32.57298469543457}}, \"EndTime\": 1591047072.818917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047072.785819}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch[0] avg_epoch_loss=0.305913\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.305913299322\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch[5] avg_epoch_loss=0.338112\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.338111894826\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch [5]#011Speed: 1041.21 samples/sec#011loss=0.338112\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[80] Batch[10] avg_epoch_loss=0.285901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.223248907924\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[80] Batch [10]#011Speed: 535.74 samples/sec#011loss=0.223249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1511.6100311279297, \"sum\": 1511.6100311279297, \"min\": 1511.6100311279297}}, \"EndTime\": 1591047074.330673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047072.818994}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=440.556610936 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.285901446234\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c91fb5b5-91b5-4826-8464-e399c6197e87-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.08001708984375, \"sum\": 21.08001708984375, \"min\": 21.08001708984375}}, \"EndTime\": 1591047074.352346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047074.330748}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[81] Batch[0] avg_epoch_loss=0.331740\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.331739842892\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch[5] avg_epoch_loss=0.388631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.388631403446\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch [5]#011Speed: 1038.01 samples/sec#011loss=0.388631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch[10] avg_epoch_loss=0.359602\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.324767175317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch [10]#011Speed: 531.54 samples/sec#011loss=0.324767\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1504.5628547668457, \"sum\": 1504.5628547668457, \"min\": 1504.5628547668457}}, \"EndTime\": 1591047075.857043, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047074.352412}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=457.897367626 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.359602208842\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch[0] avg_epoch_loss=0.272171\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.272170960903\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch[5] avg_epoch_loss=0.355211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.355210602283\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch [5]#011Speed: 1042.88 samples/sec#011loss=0.355211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1430.3970336914062, \"sum\": 1430.3970336914062, \"min\": 1430.3970336914062}}, \"EndTime\": 1591047077.288031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047075.857141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=443.18615664 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.377050116658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] Epoch[83] Batch[0] avg_epoch_loss=0.376422\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.376422405243\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] Epoch[83] Batch[5] avg_epoch_loss=0.416822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.416822135448\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] Epoch[83] Batch [5]#011Speed: 827.71 samples/sec#011loss=0.416822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.9649028778076, \"sum\": 1526.9649028778076, \"min\": 1526.9649028778076}}, \"EndTime\": 1591047078.815676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047077.288144}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=407.969533878 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.415880054235\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch[0] avg_epoch_loss=0.268475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.268475234509\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch[5] avg_epoch_loss=0.294922\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.29492200911\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch [5]#011Speed: 1057.35 samples/sec#011loss=0.294922\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1401.9479751586914, \"sum\": 1401.9479751586914, \"min\": 1401.9479751586914}}, \"EndTime\": 1591047080.218232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047078.815753}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.195004693 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.26953766644\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c04774ce-bc6e-4d12-b743-04909a761792-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.13509178161621, \"sum\": 21.13509178161621, \"min\": 21.13509178161621}}, \"EndTime\": 1591047080.239971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047080.218316}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] Epoch[85] Batch[0] avg_epoch_loss=0.503598\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.503598093987\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] Epoch[85] Batch[5] avg_epoch_loss=0.325381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.325381316245\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] Epoch[85] Batch [5]#011Speed: 1013.55 samples/sec#011loss=0.325381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1439.296007156372, \"sum\": 1439.296007156372, \"min\": 1439.296007156372}}, \"EndTime\": 1591047081.679405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047080.240044}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.764469938 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.304662819207\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch[0] avg_epoch_loss=0.396947\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.396947056055\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch[5] avg_epoch_loss=0.346896\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.346895505985\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch [5]#011Speed: 1037.63 samples/sec#011loss=0.346896\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1418.9798831939697, \"sum\": 1418.9798831939697, \"min\": 1418.9798831939697}}, \"EndTime\": 1591047083.098941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047081.679482}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.766821576 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.350696727633\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] Epoch[87] Batch[0] avg_epoch_loss=0.254176\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.254176408052\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] Epoch[87] Batch[5] avg_epoch_loss=0.363797\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.363797277212\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] Epoch[87] Batch [5]#011Speed: 541.97 samples/sec#011loss=0.363797\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1421.9858646392822, \"sum\": 1421.9858646392822, \"min\": 1421.9858646392822}}, \"EndTime\": 1591047084.521496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047083.099012}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=419.803441417 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.396937271953\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch[0] avg_epoch_loss=0.289145\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.289145261049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch[5] avg_epoch_loss=0.392511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.392511074742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch [5]#011Speed: 981.72 samples/sec#011loss=0.392511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[88] Batch[10] avg_epoch_loss=0.340901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.278968140483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[88] Batch [10]#011Speed: 529.18 samples/sec#011loss=0.278968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.72096824646, \"sum\": 1510.72096824646, \"min\": 1510.72096824646}}, \"EndTime\": 1591047086.032762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047084.521568}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.096602424 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.340900650079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch[0] avg_epoch_loss=0.335540\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.335540384054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch[5] avg_epoch_loss=0.382849\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.382848918438\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch [5]#011Speed: 1029.79 samples/sec#011loss=0.382849\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] Epoch[89] Batch[10] avg_epoch_loss=0.352780\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.316697520018\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] Epoch[89] Batch [10]#011Speed: 502.08 samples/sec#011loss=0.316698\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1552.7589321136475, \"sum\": 1552.7589321136475, \"min\": 1552.7589321136475}}, \"EndTime\": 1591047087.586005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047086.032839}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.02116707 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.352780100974\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch[0] avg_epoch_loss=0.313433\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.313433468342\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch[5] avg_epoch_loss=0.367483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.367483228445\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch [5]#011Speed: 1022.45 samples/sec#011loss=0.367483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.2710132598877, \"sum\": 1422.2710132598877, \"min\": 1422.2710132598877}}, \"EndTime\": 1591047089.008802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047087.586078}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.6702916 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.355760231614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch[0] avg_epoch_loss=0.386939\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.386939287186\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch[5] avg_epoch_loss=0.337555\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.337554509441\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch [5]#011Speed: 1041.99 samples/sec#011loss=0.337555\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] Epoch[91] Batch[10] avg_epoch_loss=0.315931\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.289982205629\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] Epoch[91] Batch [10]#011Speed: 536.53 samples/sec#011loss=0.289982\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1495.9590435028076, \"sum\": 1495.9590435028076, \"min\": 1495.9590435028076}}, \"EndTime\": 1591047090.505342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047089.008873}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.468213817 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.315930734981\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch[0] avg_epoch_loss=0.166290\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.166289806366\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch[5] avg_epoch_loss=0.299475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.299474984407\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch [5]#011Speed: 1017.66 samples/sec#011loss=0.299475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[92] Batch[10] avg_epoch_loss=0.379207\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.474884578586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[92] Batch [10]#011Speed: 491.61 samples/sec#011loss=0.474885\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1586.287021636963, \"sum\": 1586.287021636963, \"min\": 1586.287021636963}}, \"EndTime\": 1591047092.09218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047090.505424}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=417.926620594 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.379206618125\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[93] Batch[0] avg_epoch_loss=0.199889\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.199889168143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch[5] avg_epoch_loss=0.356010\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.356009580195\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch [5]#011Speed: 1051.38 samples/sec#011loss=0.356010\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch[10] avg_epoch_loss=0.448569\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.559640912712\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch [10]#011Speed: 501.86 samples/sec#011loss=0.559641\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1549.9329566955566, \"sum\": 1549.9329566955566, \"min\": 1549.9329566955566}}, \"EndTime\": 1591047093.642651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047092.092259}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=421.277051504 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.448569276793\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch[0] avg_epoch_loss=0.254592\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.254591703415\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch[5] avg_epoch_loss=0.293480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.293480438491\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch [5]#011Speed: 1026.93 samples/sec#011loss=0.293480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1451.9438743591309, \"sum\": 1451.9438743591309, \"min\": 1451.9438743591309}}, \"EndTime\": 1591047095.095101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047093.64273}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.380910331 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.320347319543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch[0] avg_epoch_loss=0.343025\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.343025177717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch[5] avg_epoch_loss=0.346083\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.346083333095\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch [5]#011Speed: 1041.85 samples/sec#011loss=0.346083\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1432.0080280303955, \"sum\": 1432.0080280303955, \"min\": 1432.0080280303955}}, \"EndTime\": 1591047096.527696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047095.095165}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=436.417442713 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.301443825662\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch[0] avg_epoch_loss=0.244632\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.244631811976\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch[5] avg_epoch_loss=0.376419\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.376419280966\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch [5]#011Speed: 1009.08 samples/sec#011loss=0.376419\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[96] Batch[10] avg_epoch_loss=0.365389\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.352151852846\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[96] Batch [10]#011Speed: 537.39 samples/sec#011loss=0.352152\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.0839138031006, \"sum\": 1510.0839138031006, \"min\": 1510.0839138031006}}, \"EndTime\": 1591047098.038372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047096.527763}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.417838138 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.365388631821\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch[0] avg_epoch_loss=0.251817\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.251817017794\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch[5] avg_epoch_loss=0.284140\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.284139735003\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch [5]#011Speed: 1048.45 samples/sec#011loss=0.284140\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.9750118255615, \"sum\": 1426.9750118255615, \"min\": 1426.9750118255615}}, \"EndTime\": 1591047099.465906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047098.038456}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=444.95802743 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.292433904111\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[0] avg_epoch_loss=0.298185\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.298184663057\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[5] avg_epoch_loss=0.278262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.278261783222\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch [5]#011Speed: 1058.38 samples/sec#011loss=0.278262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[10] avg_epoch_loss=0.250298\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=0.216742414236\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch [10]#011Speed: 534.39 samples/sec#011loss=0.216742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1488.5509014129639, \"sum\": 1488.5509014129639, \"min\": 1488.5509014129639}}, \"EndTime\": 1591047100.95501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047099.465992}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.944986536 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.250298433683\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b067c683-1682-40eb-b98c-1634f320691e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.255006790161133, \"sum\": 31.255006790161133, \"min\": 31.255006790161133}}, \"EndTime\": 1591047100.986819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047100.955089}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch[0] avg_epoch_loss=0.244077\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.244077175856\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch[5] avg_epoch_loss=0.288435\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.288434584936\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch [5]#011Speed: 1021.09 samples/sec#011loss=0.288435\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1450.3068923950195, \"sum\": 1450.3068923950195, \"min\": 1450.3068923950195}}, \"EndTime\": 1591047102.437268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047100.986893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=410.913189961 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.248870485276\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_8df91455-93ef-442c-9291-5871817a62bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.274877548217773, \"sum\": 20.274877548217773, \"min\": 20.274877548217773}}, \"EndTime\": 1591047102.45819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.437347}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Final loss: 0.248870485276 (occurred at epoch 99)\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #quality_metric: host=algo-1, train final_loss <loss>=0.248870485276\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 WARNING 140176366790464] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 265.9471035003662, \"sum\": 265.9471035003662, \"min\": 265.9471035003662}}, \"EndTime\": 1591047102.72481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.458267}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 325.5000114440918, \"sum\": 325.5000114440918, \"min\": 325.5000114440918}}, \"EndTime\": 1591047102.784321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.724895}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 9.264945983886719, \"sum\": 9.264945983886719, \"min\": 9.264945983886719}}, \"EndTime\": 1591047102.793689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.784379}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.031948089599609375, \"sum\": 0.031948089599609375, \"min\": 0.031948089599609375}}, \"EndTime\": 1591047102.794466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.793744}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:45 INFO 140176366790464] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:48 INFO 140176366790464] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:51 INFO 140176366790464] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:54 INFO 140176366790464] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:57 INFO 140176366790464] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:00 INFO 140176366790464] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:03 INFO 140176366790464] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 22959.881067276, \"sum\": 22959.881067276, \"min\": 22959.881067276}}, \"EndTime\": 1591047125.754312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.794517}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, RMSE): 0.724123434059\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, mean_absolute_QuantileLoss): 1720.1710181790002\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, mean_wQuantileLoss): 0.4346061187920667\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.1]): 0.5391333572234263\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.2]): 0.5227500572602042\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.3]): 0.49925785692074076\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.4]): 0.47178476693552457\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.5]): 0.4414293351376853\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.6]): 0.4089470922154905\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.7]): 0.3755974119798784\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.8]): 0.343263300519414\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.9]): 0.30929189093623677\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.434606118792\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #quality_metric: host=algo-1, test RMSE <loss>=0.724123434059\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 178027.48608589172, \"sum\": 178027.48608589172, \"min\": 178027.48608589172}, \"setuptime\": {\"count\": 1, \"max\": 8.664131164550781, \"sum\": 8.664131164550781, \"min\": 8.664131164550781}}, \"EndTime\": 1591047125.772278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047125.754382}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-01 21:32:14 Uploading - Uploading generated training model\n",
      "2020-06-01 21:32:14 Completed - Training job completed\n",
      "Training seconds: 235\n",
      "Billable seconds: 235\n",
      "CPU times: user 900 ms, sys: 76.3 ms, total: 976 ms\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_request(instance, num_samples, quantiles):\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = sagemaker.predictor.RealTimePredictor(endpoint='MLEND-Capstone-Project-2020-05-31-23-12-35-007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prediction(ticker,date):\n",
    "    try:\n",
    "        date_pred = pd.Timestamp(date, freq='D')\n",
    "        date_start = date_pred-50\n",
    "        pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "        result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "\n",
    "        pred = {\n",
    "                \"start\": str(date_pred),\n",
    "                \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
    "                \"dynamic_feat\": pred_df[['Adj Close','Volume']][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "        req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "        res = predictor.predict(req)\n",
    "\n",
    "        prediction_data = json.loads(res.decode('utf-8'))\n",
    "        pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "        result_df['prediction'] = pred\n",
    "        return result_df\n",
    "    except:\n",
    "        print('{} did not trade today.'.format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03886</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target  prediction\n",
       "Date       Ticker                                         \n",
       "2019-01-23 AAPL      0.03886  0.026907       1           1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('AAPL', '2019-01-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_prediction_accuracy(ticker, date_range):\n",
    "    ticker = str(ticker)\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "\n",
    "    for date in date_range:\n",
    "        target.append(get_stock_prediction(ticker, date)['target'].values[0])\n",
    "        prediction.append(int(get_stock_prediction(ticker, date)['prediction'].values[0]))\n",
    "    target = list(np.array(target).reshape(252))\n",
    "    prediction = list(np.array(prediction).reshape(252))\n",
    "    data = {'target': list(target), 'prediction': list(prediction)}\n",
    "    prediction_df = pd.DataFrame(data=data,index=date_index, columns=['target','prediction'])\n",
    "    \n",
    "    return accuracy_score(target, prediction), prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.746031746031746,             target  prediction\n",
       " 2019-01-02      -1          -1\n",
       " 2019-01-03       1          -1\n",
       " 2019-01-04       1           1\n",
       " 2019-01-07       1           1\n",
       " 2019-01-08       1           1\n",
       " 2019-01-09       0           1\n",
       " 2019-01-10      -1           0\n",
       " 2019-01-11       1          -1\n",
       " 2019-01-14       1           1\n",
       " 2019-01-15       1           1\n",
       " 2019-01-16       0           1\n",
       " 2019-01-17       0           0\n",
       " 2019-01-18      -1           0\n",
       " 2019-01-22       1          -1\n",
       " 2019-01-23       1           1\n",
       " 2019-01-24       1           1\n",
       " 2019-01-25       1           1\n",
       " 2019-01-28       1           1\n",
       " 2019-01-29       1           1\n",
       " 2019-01-30       1           1\n",
       " 2019-01-31       1           1\n",
       " 2019-02-01       1           1\n",
       " 2019-02-04       0           1\n",
       " 2019-02-05       0           0\n",
       " 2019-02-06       0           0\n",
       " 2019-02-07       0           0\n",
       " 2019-02-08       0           0\n",
       " 2019-02-11       0           0\n",
       " 2019-02-12       0           0\n",
       " 2019-02-13       1           0\n",
       " ...            ...         ...\n",
       " 2019-11-18       0           0\n",
       " 2019-11-19       0           0\n",
       " 2019-11-20       0           0\n",
       " 2019-11-21       0           0\n",
       " 2019-11-22       0           0\n",
       " 2019-11-25      -1           0\n",
       " 2019-11-26       1          -1\n",
       " 2019-11-27      -1           1\n",
       " 2019-11-29      -1          -1\n",
       " 2019-12-02       1          -1\n",
       " 2019-12-03       1           1\n",
       " 2019-12-04       1           1\n",
       " 2019-12-05       1           1\n",
       " 2019-12-06       1           1\n",
       " 2019-12-09       1           1\n",
       " 2019-12-10       1           1\n",
       " 2019-12-11       1           1\n",
       " 2019-12-12       1           1\n",
       " 2019-12-13       1           1\n",
       " 2019-12-16       1           1\n",
       " 2019-12-17       1           1\n",
       " 2019-12-18       1           1\n",
       " 2019-12-19       1           1\n",
       " 2019-12-20       1           1\n",
       " 2019-12-23       1           1\n",
       " 2019-12-24       1           1\n",
       " 2019-12-26       0           1\n",
       " 2019-12-27       0           0\n",
       " 2019-12-30       0           0\n",
       " 2019-12-31       0           0\n",
       " \n",
       " [252 rows x 2 columns])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prediction_accuracy('AAPL', date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': '2010-01-04 00:00:00',\n",
       " 'target': [-1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  -1,\n",
       "  -1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  -1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...],\n",
       " 'dynamic_feat': [[0.005248434396220181,\n",
       "   0.005191410188869483,\n",
       "   0.005172959364883671,\n",
       "   0.00516625037521779,\n",
       "   0.0051645727603331065,\n",
       "   0.0051679275001448545,\n",
       "   0.005105869958182486,\n",
       "   0.005146122916262532,\n",
       "   0.005223277032356581,\n",
       "   0.005102516688243592,\n",
       "   0.0051645727603331065,\n",
       "   0.005136059676742521,\n",
       "   0.005117611302544799,\n",
       "   0.004891186698492885,\n",
       "   0.00493814815628119,\n",
       "   0.004911314157448143,\n",
       "   0.004891186698492885,\n",
       "   0.004810678332544703,\n",
       "   0.004699981227951725,\n",
       "   0.004884476728911767,\n",
       "   0.004946534760831754,\n",
       "   0.004953243750497635,\n",
       "   0.004874414469306992,\n",
       "   0.004891186698492885,\n",
       "   0.004896218073274081,\n",
       "   0.004931439166615311,\n",
       "   0.0049046046778246435,\n",
       "   0.004923053052022364,\n",
       "   0.005027042146864699,\n",
       "   0.005072325989768322,\n",
       "   0.005136059676742521,\n",
       "   0.005194762968850758,\n",
       "   0.0052316597172462,\n",
       "   0.005223277032356581,\n",
       "   0.005172959364883671,\n",
       "   0.005236693541815486,\n",
       "   0.0052132128129213345,\n",
       "   0.005275269374968465,\n",
       "   0.005380933634907394,\n",
       "   0.005426219927599108,\n",
       "   0.005439636927015631,\n",
       "   0.005409447698413216,\n",
       "   0.005560399230959093,\n",
       "   0.005533560822507481,\n",
       "   0.00557884858507205,\n",
       "   0.00564426037688855,\n",
       "   0.005630840437726317,\n",
       "   0.005604005948935651,\n",
       "   0.005585556104865075,\n",
       "   0.005604005948935651,\n",
       "   0.005597295489396916,\n",
       "   0.005610714448643913,\n",
       "   0.005604005948935651,\n",
       "   0.005654322146535707,\n",
       "   0.00571973295843697,\n",
       "   0.005684511375138122,\n",
       "   0.005793532334719271,\n",
       "   0.005753280356554463,\n",
       "   0.0057683730110251975,\n",
       "   0.005805273679081583,\n",
       "   0.0057666973559709865,\n",
       "   0.005793532334719271,\n",
       "   0.005838816177622894,\n",
       "   0.005795207989773483,\n",
       "   0.005749925616742712,\n",
       "   0.005699607949269802,\n",
       "   0.0057398609073498474,\n",
       "   0.0057365061675380994,\n",
       "   0.005684511375138122,\n",
       "   0.0057700491560370255,\n",
       "   0.005974673095867564,\n",
       "   0.005914292678832259,\n",
       "   0.0058958418548464495,\n",
       "   0.0060853687305876866,\n",
       "   0.006058533261881784,\n",
       "   0.006110526094451289,\n",
       "   0.006144071532738307,\n",
       "   0.00617426125129834,\n",
       "   0.006073628856098228,\n",
       "   0.006144071532738307,\n",
       "   0.006243027292968973,\n",
       "   0.00608033784576411,\n",
       "   0.006165875136705396,\n",
       "   0.005929385823260612,\n",
       "   0.005828753918018121,\n",
       "   0.005580522770253406,\n",
       "   0.005344035906596716,\n",
       "   0.005733151427726349,\n",
       "   0.0057130244587287075,\n",
       "   0.005837140032611064,\n",
       "   0.00583210816787225,\n",
       "   0.005677802875429861,\n",
       "   0.005733151427726349,\n",
       "   0.005377582324798971,\n",
       "   0.005464796740667323,\n",
       "   0.005204827188286006,\n",
       "   0.00525514387584368,\n",
       "   0.005265207115363693,\n",
       "   0.005238371646657788,\n",
       "   0.005280302219622518,\n",
       "   0.00549163122945799,\n",
       "   0.005426219927599108,\n",
       "   0.005278622644907361,\n",
       "   0.0054027396886625716,\n",
       "   0.0054329303871378425,\n",
       "   0.0050874225639000025,\n",
       "   0.005047169605819957,\n",
       "   0.0050035609280129284,\n",
       "   0.004949889010685885,\n",
       "   0.005105869958182486,\n",
       "   0.00515283484567412,\n",
       "   0.005275269374968465,\n",
       "   0.005399382499062732,\n",
       "   0.005471506710248438,\n",
       "   0.005444669771669682,\n",
       "   0.005468150010606217,\n",
       "   0.005437960292046185,\n",
       "   0.0053691942503755535,\n",
       "   0.005360808625740226,\n",
       "   0.0051578637706672275,\n",
       "   0.005146122916262532,\n",
       "   0.00512767454206481,\n",
       "   0.004897894708243528,\n",
       "   0.004767071614568146,\n",
       "   0.004741914250704544,\n",
       "   0.004659730229702153,\n",
       "   0.004653019770163419,\n",
       "   0.00479055479325039,\n",
       "   0.004830805301542344,\n",
       "   0.00480396983283644,\n",
       "   0.004728495291457547,\n",
       "   0.0048207425519799525,\n",
       "   0.004810678332544703,\n",
       "   0.004762040239786949,\n",
       "   0.0045322604059656645,\n",
       "   0.004617800146695043,\n",
       "   0.004614443937010439,\n",
       "   0.004624507666488069,\n",
       "   0.004815710687241136,\n",
       "   0.004912988832587117,\n",
       "   0.004970015489725908,\n",
       "   0.004840869031019972,\n",
       "   0.004825774416718765,\n",
       "   0.0047201081969493654,\n",
       "   0.004683210468638689,\n",
       "   0.004832482916427027,\n",
       "   0.004668115854337481,\n",
       "   0.004743590395716374,\n",
       "   0.004772102009434105,\n",
       "   0.004817387812168203,\n",
       "   0.00500020520828594,\n",
       "   0.004939825771165874,\n",
       "   0.004731850521226915,\n",
       "   0.004616120082022268,\n",
       "   0.004585930853419852,\n",
       "   0.00455406498984799,\n",
       "   0.004909635562648222,\n",
       "   0.004936470541396507,\n",
       "   0.004785521458638722,\n",
       "   0.0047888737486623785,\n",
       "   0.004688241353462266,\n",
       "   0.004587608468304537,\n",
       "   0.0046429580005162615,\n",
       "   0.004612766812083374,\n",
       "   0.004688241353462266,\n",
       "   0.004641279405716341,\n",
       "   0.004522196676488037,\n",
       "   0.004743590395716374,\n",
       "   0.004897894708243528,\n",
       "   0.005005238052939993,\n",
       "   0.004860996000017613,\n",
       "   0.0048358376562387785,\n",
       "   0.004850932270539984,\n",
       "   0.004847579490558708,\n",
       "   0.004884476728911767,\n",
       "   0.004891186698492885,\n",
       "   0.004931439166615311,\n",
       "   0.005037104406469472,\n",
       "   0.005122643657241232,\n",
       "   0.005203149573401322,\n",
       "   0.005223277032356581,\n",
       "   0.0051578637706672275,\n",
       "   0.00512432029221068,\n",
       "   0.005380933634907394,\n",
       "   0.005365841470394277,\n",
       "   0.005422866167702594,\n",
       "   0.005463119615740258,\n",
       "   0.005595619834342705,\n",
       "   0.00565599976142039,\n",
       "   0.005464796740667323,\n",
       "   0.005540272261961453,\n",
       "   0.005478213250126229,\n",
       "   0.005567106750752119,\n",
       "   0.005667740125867468,\n",
       "   0.005659354011274521,\n",
       "   0.005647614626742682,\n",
       "   0.005753280356554463,\n",
       "   0.0057398609073498474,\n",
       "   0.0057817905003993385,\n",
       "   0.005811980218959373,\n",
       "   0.005704639324051,\n",
       "   0.005811980218959373,\n",
       "   0.005827075813175818,\n",
       "   0.005827075813175818,\n",
       "   0.0058522341569546546,\n",
       "   0.005835462417726381,\n",
       "   0.005800240834427532,\n",
       "   0.005855589386724022,\n",
       "   0.005835462417726381,\n",
       "   0.005827075813175818,\n",
       "   0.005879070605575794,\n",
       "   0.005887455250295885,\n",
       "   0.006035052532987631,\n",
       "   0.006100462854931276,\n",
       "   0.006066918886517111,\n",
       "   0.005949513772173489,\n",
       "   0.005912614573989958,\n",
       "   0.005991443855180601,\n",
       "   0.006097108605077145,\n",
       "   0.0060702760761169525,\n",
       "   0.00587571439589119,\n",
       "   0.0059042289493546314,\n",
       "   0.006075304511152439,\n",
       "   0.006113881814178272,\n",
       "   0.006083691115703003,\n",
       "   0.005941126677665308,\n",
       "   0.006093754845180633,\n",
       "   0.006095433929938171,\n",
       "   0.005978026365806458,\n",
       "   0.005872361615909914,\n",
       "   0.0060568566269123365,\n",
       "   0.006157490002027686,\n",
       "   0.006284958355891318,\n",
       "   0.006226256533655935,\n",
       "   0.006301731075034829,\n",
       "   0.006315147584493733,\n",
       "   0.006380558886352615,\n",
       "   0.006462742907355009,\n",
       "   0.0064895798459337645,\n",
       "   0.006523122834432692,\n",
       "   0.006452679177877378,\n",
       "   0.006664010147543323,\n",
       "   0.006798187981030454,\n",
       "   0.006813282595331661,\n",
       "   0.0068468285235763,\n",
       "   0.006885403376814041,\n",
       "   0.006887080011783489,\n",
       "   0.007029644939778802,\n",
       "   0.00696087938806579,\n",
       "   0.0069910686166682065,\n",
       "   0.006974296877439931,\n",
       "   0.0069474614087340295,\n",
       "   0.007022934970197686,\n",
       "   0.0069575246482540395,\n",
       "   0.006942430523910451,\n",
       "   0.00695584605345412,\n",
       "   0.006979327272305892,\n",
       "   0.007079962607251713,\n",
       "   0.007200721481491848,\n",
       "   0.007232589794851802,\n",
       "   0.007205752856273044,\n",
       "   0.007254393398818889,\n",
       "   0.007437211284894247,\n",
       "   0.007115182720677706,\n",
       "   0.0070917024817411705,\n",
       "   0.0070615103133930485,\n",
       "   0.007172209867774115,\n",
       "   0.007158791888442356,\n",
       "   0.0071386644294870955,\n",
       "   0.007118537950447074,\n",
       "   0.00687198343765181,\n",
       "   0.0070145493455623575,\n",
       "   0.007051449523661126,\n",
       "   0.006913916950362246,\n",
       "   0.0068736635023245824,\n",
       "   0.0072091085760000295,\n",
       "   0.007452304919280219,\n",
       "   0.007407019116546122,\n",
       "   0.00728290501253662,\n",
       "   0.007378507012870773,\n",
       "   0.007549584534499055,\n",
       "   0.007511007721430839,\n",
       "   0.007152082898776475,\n",
       "   0.0073063862313883935,\n",
       "   0.0073650895234966315,\n",
       "   0.0073650895234966315,\n",
       "   0.0071252464501553375,\n",
       "   0.006783092386814011,\n",
       "   0.006900498971030487,\n",
       "   0.007103443826103484,\n",
       "   0.0070564808984423235,\n",
       "   0.006821669199882225,\n",
       "   0.006952492783515226,\n",
       "   0.007180594512494206,\n",
       "   0.007839743315482789,\n",
       "   0.007692147502663896,\n",
       "   0.007708919731849787,\n",
       "   0.007722338691096784,\n",
       "   0.007526102335732045,\n",
       "   0.007603255471910859,\n",
       "   0.007519393346066165,\n",
       "   0.00734831680435312,\n",
       "   0.007098411471407051,\n",
       "   0.007118537950447074,\n",
       "   0.007200721481491848,\n",
       "   0.007370121388235445,\n",
       "   0.007242651564498958,\n",
       "   0.007323157480659045,\n",
       "   0.007363411908611947,\n",
       "   0.007385217472409508,\n",
       "   0.00737179900312013,\n",
       "   0.007472431888277859,\n",
       "   0.007561324408988513,\n",
       "   0.007509330596503773,\n",
       "   0.00756971052358146,\n",
       "   0.007517716711096719,\n",
       "   0.007603255471910859,\n",
       "   0.0075948698472755325,\n",
       "   0.0075244252108049815,\n",
       "   0.007527780930531969,\n",
       "   0.00754454973001453,\n",
       "   0.007536166065209677,\n",
       "   0.00781290735681927,\n",
       "   0.007819616836442765,\n",
       "   0.007965534054461737,\n",
       "   0.007811230721849818,\n",
       "   0.007838066680513341,\n",
       "   0.008156737565172422,\n",
       "   0.008274143169473662,\n",
       "   0.008079585408908845,\n",
       "   0.008284209838697001,\n",
       "   0.008389875078551164,\n",
       "   0.00843851023156321,\n",
       "   0.008369744679850195,\n",
       "   0.008458636710603231,\n",
       "   0.008485473649181988,\n",
       "   0.008300981087967655,\n",
       "   0.008245629595925456,\n",
       "   0.008339554471332545,\n",
       "   0.00836471575485709,\n",
       "   0.0084972149935443,\n",
       "   0.008389875078551164,\n",
       "   0.008440190786193603,\n",
       "   0.008817564962960923,\n",
       "   0.008773957755026748,\n",
       "   0.008581075159558523,\n",
       "   0.0086749970952199,\n",
       "   0.008579397544673836,\n",
       "   0.008502244898452645,\n",
       "   0.008233890211393617,\n",
       "   0.00816512563959584,\n",
       "   0.008220469292316148,\n",
       "   0.008282531243897079,\n",
       "   0.00831775233723831,\n",
       "   0.008363036180141937,\n",
       "   0.008049396670264048,\n",
       "   0.008079585408908845,\n",
       "   0.007953794669929895,\n",
       "   0.007891737617925144,\n",
       "   0.007999079492748756,\n",
       "   0.008005790442245109,\n",
       "   0.008168479399492353,\n",
       "   0.008081264003708765,\n",
       "   0.007970567879031025,\n",
       "   0.008212088567257003,\n",
       "   0.00794373241032512,\n",
       "   0.007891737617925144,\n",
       "   0.007953794669929895,\n",
       "   0.008113130357238247,\n",
       "   0.008354646145888045,\n",
       "   0.008208731867614779,\n",
       "   0.008280852649097161,\n",
       "   0.008131579711351205,\n",
       "   0.00819195914847127,\n",
       "   0.008465347170141966,\n",
       "   0.008483797014212542,\n",
       "   0.00857101338991137,\n",
       "   0.008720285837699704,\n",
       "   0.008663259670518534,\n",
       "   0.008690096609097291,\n",
       "   0.008784022464419613,\n",
       "   0.008423415617261999,\n",
       "   0.008232211616593698,\n",
       "   0.008061137034711126,\n",
       "   0.008072876419242966,\n",
       "   0.00793702391061686,\n",
       "   0.007895089907948804,\n",
       "   0.007623381950950882,\n",
       "   0.007871607709181796,\n",
       "   0.007868255419158138,\n",
       "   0.007868255419158138,\n",
       "   0.007863223554419324,\n",
       "   0.007781039533416931,\n",
       "   0.007519393346066165,\n",
       "   0.007120215565331758,\n",
       "   0.007073253617585835,\n",
       "   0.007069897407901228,\n",
       "   0.006840118553995182,\n",
       "   0.006529832314056191,\n",
       "   0.0065734414818208395,\n",
       "   0.006189356845514785,\n",
       "   0.0061356844382301256,\n",
       "   0.005459764385970889,\n",
       "   0.0057365061675380994,\n",
       "   0.005340681166784966,\n",
       "   0.005786822365138155,\n",
       "   0.006071950261298308,\n",
       "   0.006288311625830214,\n",
       "   0.006142394897768861,\n",
       "   0.006003185199542913,\n",
       "   0.005404416323632019,\n",
       "   0.005211535198036649,\n",
       "   0.005114257052690668,\n",
       "   0.005342357311796796,\n",
       "   0.005546980271712096,\n",
       "   0.00553859415711915,\n",
       "   0.005847203272131076,\n",
       "   0.006157490002027686,\n",
       "   0.006140716302968939,\n",
       "   0.006182646875933668,\n",
       "   0.006009894679166413,\n",
       "   0.0057683730110251975,\n",
       "   0.005687866604907489,\n",
       "   0.005984736335387575,\n",
       "   0.005724765313133404,\n",
       "   0.005593942709415639,\n",
       "   0.005602328334050968,\n",
       "   0.00583210816787225,\n",
       "   0.0061340092731335324,\n",
       "   0.006211160449481872,\n",
       "   0.006097108605077145,\n",
       "   0.005862297886432285,\n",
       "   0.005877392990691109,\n",
       "   0.005407771553401387,\n",
       "   0.0052467577612507355,\n",
       "   0.005312168573151998,\n",
       "   0.005431251792337922,\n",
       "   0.005627487167787422,\n",
       "   0.005305459583486118,\n",
       "   0.005453054906347392,\n",
       "   0.005240047301711999,\n",
       "   0.004929761551730628,\n",
       "   0.005199795323547191,\n",
       "   0.005385967459476681,\n",
       "   0.005624131448060437,\n",
       "   0.005260175740582495,\n",
       "   0.005585556104865075,\n",
       "   0.005689542749919319,\n",
       "   0.005664385386055719,\n",
       "   0.005444669771669682,\n",
       "   0.005590587479646272,\n",
       "   0.005545302166869793,\n",
       "   0.005614069678413279,\n",
       "   0.005531885657410888,\n",
       "   0.00565599976142039,\n",
       "   0.005914292678832259,\n",
       "   0.006295021595411329,\n",
       "   0.006024987823594766,\n",
       "   0.006001508074615847,\n",
       "   0.006707618825350353,\n",
       "   0.006856891273138693,\n",
       "   0.006216193784093542,\n",
       "   0.005917646928686392,\n",
       "   0.006063564146705362,\n",
       "   0.006226256533655935,\n",
       "   0.006189356845514785,\n",
       "   0.006328564583910258,\n",
       "   0.00649796596052671,\n",
       "   0.006048469042446536,\n",
       "   0.006180969751006603,\n",
       "   0.006281602146206716,\n",
       "   0.006296699700253631,\n",
       "   0.006414103834682017,\n",
       "   0.006469453366893743,\n",
       "   0.006239673043114841,\n",
       "   0.006172584126371275,\n",
       "   0.006001508074615847,\n",
       "   0.005966286981274618,\n",
       "   0.005634195667495684,\n",
       "   0.0056727724805639,\n",
       "   0.0058958418548464495,\n",
       "   0.005872361615909914,\n",
       "   0.006288311625830214,\n",
       "   0.0063419820732844,\n",
       "   0.006164199481651185,\n",
       "   0.006214514209378384,\n",
       "   0.006201095740089008,\n",
       "   0.006147426272550056,\n",
       "   0.005771727750836946,\n",
       "   0.005924354938437034,\n",
       "   0.00565599976142039,\n",
       "   0.005558721126116792,\n",
       "   0.005521821927933259,\n",
       "   0.0056090378136744645,\n",
       "   0.005610714448643913,\n",
       "   0.005572136655660462,\n",
       "   0.005798563219542848,\n",
       "   0.0057616645113169355,\n",
       "   0.005842170427477025,\n",
       "   0.005929385823260612,\n",
       "   0.005910935489232421,\n",
       "   0.00571973295843697,\n",
       "   0.005879070605575794,\n",
       "   0.005857265531735851,\n",
       "   0.006117235574074786,\n",
       "   0.006068596011444177,\n",
       "   0.006204452439731229,\n",
       "   0.00627153939664432,\n",
       "   0.0064359069486914855,\n",
       "   0.006556668272719713,\n",
       "   0.006526478064202061,\n",
       "   0.006627110949359791,\n",
       "   0.006549960262969068,\n",
       "   0.0067059407205080504,\n",
       "   0.006920624470155272,\n",
       "   0.007017904085374107,\n",
       "   0.0068753416071668854,\n",
       "   0.0068753416071668854,\n",
       "   0.007143695804268294,\n",
       "   0.007386893617421336,\n",
       "   0.00715375757391545,\n",
       "   0.0071571137836000534,\n",
       "   0.0072376201897177625,\n",
       "   0.007121891710343586,\n",
       "   0.007410374836273106,\n",
       "   0.0073315445751672296,\n",
       "   0.007593190762517994,\n",
       "   0.007489203137548514,\n",
       "   0.0074791418578589765,\n",
       "   0.007541197929948491,\n",
       "   0.007561324408988513,\n",
       "   0.007401987741764927,\n",
       "   0.007351672524080107,\n",
       "   0.007341608794602476,\n",
       "   0.007318128555665942,\n",
       "   0.007349994909195423,\n",
       "   0.00723426593986363,\n",
       "   0.007145370969364888,\n",
       "   0.00716382375318117,\n",
       "   0.00727955125264011,\n",
       "   0.007214139460823606,\n",
       "   0.0072694870332048606,\n",
       "   0.007316450450823641,\n",
       "   0.0073147738158541925,\n",
       "   0.007405341991619058,\n",
       "   0.0073315445751672296,\n",
       "   0.007173886502743563,\n",
       "   0.007090024866856487,\n",
       "   0.007287937367233054,\n",
       "   0.007425468960616698,\n",
       "   0.007470754273393177,\n",
       "   0.0073147738158541925,\n",
       "   0.007564678658842644,\n",
       "   0.007541197929948491,\n",
       "   0.007690470377736828,\n",
       "   0.007650217419656784,\n",
       "   0.00760661021172261,\n",
       "   0.007517716711096719,\n",
       "   0.007499267356983762,\n",
       "   0.007329867940197783,\n",
       "   0.007428824190386066,\n",
       "   0.007554614929365014,\n",
       "   0.007658603534249731,\n",
       "   0.0074774642429742905,\n",
       "   0.00745565916913435,\n",
       "   0.007487592646857524,\n",
       "   0.007548163657406301,\n",
       "   0.0075414335995628405,\n",
       "   0.007358039523328489,\n",
       "   0.007531337532882409,\n",
       "   0.007326073708402515,\n",
       "   0.007115757440963844,\n",
       "   0.00714435969684094,\n",
       "   0.007322706229692694,\n",
       "   0.0071056628441562705,\n",
       "   0.0070635999826346284,\n",
       "   0.0071746464270093764,\n",
       "   0.007119122959843194,\n",
       "   0.007051822871366188,\n",
       "   0.0070821081316617684,\n",
       "   0.006942456981621833,\n",
       "   0.0066951302956203,\n",
       "   0.0069273163113045145,\n",
       "   0.0070635999826346284,\n",
       "   0.007159503796861586,\n",
       "   0.007095568737306312,\n",
       "   0.0072284859098418405,\n",
       "   0.007240263511067898,\n",
       "   0.007100614810816055,\n",
       "   0.006898713075512182,\n",
       "   0.006873476828472053,\n",
       "   0.006774208475280982,\n",
       "   0.006717001023781079,\n",
       "   0.006685035208855107,\n",
       "   0.006636241309574767,\n",
       "   0.006543703014227158,\n",
       "   0.006814588822341758,\n",
       "   0.006688399747819219,\n",
       "   0.006666527549785585,\n",
       "   0.006515100758350062,\n",
       "   0.006727096600503892,\n",
       "   0.006811222813504788,\n",
       "   0.0068583332184090256,\n",
       "   0.006900396079930666,\n",
       "   0.006898713075512182,\n",
       "   0.007095568737306312,\n",
       "   0.00695591905713923,\n",
       "   0.006839826539254741,\n",
       "   0.006429292520845918,\n",
       "   0.006365358441205878,\n",
       "   0.006441070122071976,\n",
       "   0.006690082752237703,\n",
       "   0.006722050526994149,\n",
       "   0.006661478536530133,\n",
       "   0.006557163619871702,\n",
       "   0.006639606828454115,\n",
       "   0.006560527668878197,\n",
       "   0.006619417144881346,\n",
       "   0.006730461629425623,\n",
       "   0.006754018301750596,\n",
       "   0.006922269747837156,\n",
       "   0.0068364615103330105,\n",
       "   0.00658071882232382,\n",
       "   0.0065655761921760295,\n",
       "   0.006335071711037443,\n",
       "   0.006390595668161246,\n",
       "   0.006451165208837171,\n",
       "   0.0063451672877602545,\n",
       "   0.006625469591338854,\n",
       "   0.0065697408319306495,\n",
       "   0.006676132678932587,\n",
       "   0.006647423122337107,\n",
       "   0.006410996033509837,\n",
       "   0.00635188901632421,\n",
       "   0.006329934505410718,\n",
       "   0.006279272397732218,\n",
       "   0.00616612550504829,\n",
       "   0.006253940118998925,\n",
       "   0.006218476006679072,\n",
       "   0.006304601736719805,\n",
       "   0.006476858096377452,\n",
       "   0.006493745465603619,\n",
       "   0.006341756202822414,\n",
       "   0.006194831631940443,\n",
       "   0.006012444908569094,\n",
       "   0.006101949876302484,\n",
       "   0.0061813225204917005,\n",
       "   0.006405928401864893,\n",
       "   0.006446461125744925,\n",
       "   0.006465037868838611,\n",
       "   0.006446461125744925,\n",
       "   0.006373842057364846,\n",
       "   0.006603515080425362,\n",
       "   0.006623779727513714,\n",
       "   0.006796036087171361,\n",
       "   0.006865274447985928,\n",
       "   0.006888917842809321,\n",
       "   0.006843320427030056,\n",
       "   0.006817990108127234,\n",
       "   0.006836566361263303,\n",
       "   0.006834877967311018,\n",
       "   0.006272515882177375,\n",
       "   0.006284337089631454,\n",
       "   0.006193144217903395,\n",
       "   0.006269138114357571,\n",
       "   0.006324867363723394,\n",
       "   0.006326556737590914,\n",
       "   0.006390730896463866,\n",
       "   0.006321490085861207,\n",
       "   0.006238739183894571,\n",
       "   0.006274204766087277,\n",
       "   0.006253940118998925,\n",
       "   0.006274204766087277,\n",
       "   0.006255628512951209,\n",
       "   0.0062032760514899535,\n",
       "   0.006328246111458435,\n",
       "   0.0064869899299640115,\n",
       "   0.006459970237193668,\n",
       "   0.006387352638686444,\n",
       "   0.006329934505410718,\n",
       "   0.006480234884282021,\n",
       "   0.00670822049330549,\n",
       "   0.006691332144164089,\n",
       "   0.006677822542757728,\n",
       "   0.006733549832293076,\n",
       "   0.006623779727513714,\n",
       "   0.006584938827289294,\n",
       "   0.00653258881561613,\n",
       "   0.0064701025607378435,\n",
       "   0.006461658141188335,\n",
       "   0.006514011092607206,\n",
       "   0.006515672048932874,\n",
       "   0.006585163717836043,\n",
       "   0.006586858481237364,\n",
       "   0.006605501368609506,\n",
       "   0.006642790573057117,\n",
       "   0.006585163717836043,\n",
       "   0.006551264040191073,\n",
       "   0.006464824737274698,\n",
       "   0.006339400976551775,\n",
       "   0.0063766882211689115,\n",
       "   0.00630719655226574,\n",
       "   0.006300417988618077,\n",
       "   0.006424147475897301,\n",
       "   0.006449570886747579,\n",
       "   0.006208892925624886,\n",
       "   0.006058045754051685,\n",
       "   0.006100418758745639,\n",
       "   0.006017367022801433,\n",
       "   0.0060749933880648865,\n",
       "   0.006119060666202545,\n",
       "   0.006090248708464862,\n",
       "   0.006098721545556228,\n",
       "   0.006286857431619424,\n",
       "   0.006269909307648603,\n",
       "   0.006373300164239126,\n",
       "   0.006486858621322336,\n",
       "   0.006368214894119929,\n",
       "   0.006249570676959904,\n",
       "   0.006254656437036718,\n",
       "   0.006239401116636746,\n",
       "   0.006156351340523011,\n",
       "   0.006124147896152215,\n",
       "   0.006058045754051685,\n",
       "   0.006068214824417225,\n",
       "   0.006356349100522596,\n",
       "   0.0061478745837707,\n",
       "   0.006073298624663567,\n",
       "   0.006249570676959904,\n",
       "   0.00634957102683255,\n",
       "   0.006327536162869676,\n",
       "   0.006434317526178078,\n",
       "   0.006497028671603112,\n",
       "   0.006488553874681274,\n",
       "   0.006425841259383384,\n",
       "   0.006461434720514439,\n",
       "   0.006551264040191073,\n",
       "   0.006493636695012382,\n",
       "   0.00640719886196886,\n",
       "   0.00653431591622025,\n",
       "   0.0067275365824450255,\n",
       "   0.006864824666892429,\n",
       "   0.0067444861762887024,\n",
       "   0.006752959993295304,\n",
       "   0.006903807654826125,\n",
       "   0.0070139746252762215,\n",
       "   0.00687838375401823,\n",
       "   0.0070088922949027325,\n",
       "   0.006956349729885619,\n",
       "   0.006959739746645879,\n",
       "   0.006929232045591639,\n",
       "   0.006910488716907769,\n",
       "   0.006772730273146903,\n",
       "   0.006961511923350874,\n",
       "   0.007121378724717342,\n",
       "   0.007146890817896512,\n",
       "   0.007288051527358092,\n",
       "   0.007235327188064631,\n",
       "   0.007177503369881043,\n",
       "   0.007371386458805612,\n",
       "   0.007425808011288486,\n",
       "   0.007384992581862758,\n",
       "   0.007403699653682883,\n",
       "   0.0073526779171126325,\n",
       "   0.007320363742320126,\n",
       "   0.007459823808888969,\n",
       "   0.007507444749631359,\n",
       "   0.0075907811509517325,\n",
       "   0.007532954882980058,\n",
       "   0.007594180476906739,\n",
       "   0.007701328328513546,\n",
       "   0.007682619786820565,\n",
       "   0.0075907811509517325,\n",
       "   0.007549962781780299,\n",
       "   0.007614589171534837,\n",
       "   0.007701328328513546,\n",
       "   0.007526151821451483,\n",
       "   0.0076520057649631795,\n",
       "   0.0076673135108283,\n",
       "   0.00765370787772877,\n",
       "   0.0076656133578931794,\n",
       "   0.00758397563963507,\n",
       "   0.007587378395293402,\n",
       "   0.0076094877328142406,\n",
       "   0.007580575823722449,\n",
       "   0.007184304961536762,\n",
       "   0.0073135611707491706,\n",
       "   0.007182604318644025,\n",
       "   0.0070788616724836405,\n",
       "   0.007107773091617814,\n",
       "   0.007021035894469579,\n",
       "   0.006966613852029089,\n",
       "   0.0070958676114534075,\n",
       "   0.0070533490893468515,\n",
       "   0.007129883409053891,\n",
       "   0.007146890817896512,\n",
       "   0.00725403621971523,\n",
       "   0.0073526779171126325,\n",
       "   0.007354380519835843,\n",
       "   0.007316963436449884,\n",
       "   0.007279547332979162,\n",
       "   0.007247233648144273,\n",
       "   0.007311861017814052,\n",
       "   0.0073662850200850154,\n",
       "   0.0073135611707491706,\n",
       "   0.007235327188064631,\n",
       "   0.007141788889218297,\n",
       "   0.007260838301328566,\n",
       "   0.007082262468311499,\n",
       "   0.007014232832941007,\n",
       "   0.006993824628270525,\n",
       "   0.007111175357318528,\n",
       "   0.0071570956551681815,\n",
       "   0.007165249529850105,\n",
       "   0.00698766635096749,\n",
       "   0.006941563788883872,\n",
       "   0.006917656796861894,\n",
       "   0.0070696284211688425,\n",
       "   0.007091826441018573,\n",
       "   0.0071123164686112935,\n",
       "   0.007110608476354282,\n",
       "   0.007214768566405364,\n",
       "   0.007487973833884475,\n",
       "   0.007639943498360951,\n",
       "   0.007347956195546133,\n",
       "   0.007407719755940134,\n",
       "   0.0072028155603519935,\n",
       "   0.007141342537828125,\n",
       "   0.007170373506621137,\n",
       "   0.007144759012299766,\n",
       "   0.0072745316368417485,\n",
       "   0.007315513161900042,\n",
       "   0.007295023134307322,\n",
       "   0.0070508449159602795,\n",
       "   0.00707816544270819,\n",
       "   0.00707474896823655,\n",
       "   0.007052553888132526,\n",
       "   0.00708157995734936,\n",
       "   0.007093533453360348,\n",
       "   0.007170373506621137,\n",
       "   0.007238674578512106,\n",
       "   0.0073308797026793455,\n",
       "   0.007368446223138853,\n",
       "   0.007448701770956047,\n",
       "   0.007347956195546133,\n",
       "   0.007506756849135421,\n",
       "   0.00779874366199262,\n",
       "   0.007680924043504008,\n",
       "   0.007778254614315138,\n",
       "   0.00791144371332876,\n",
       "   0.007981452777476741,\n",
       "   0.007894364770673883,\n",
       "   0.007808989165746597,\n",
       "   0.007783376631255699,\n",
       "   0.007926810254108064,\n",
       "   0.007861924676773501,\n",
       "   0.007861924676773501,\n",
       "   0.007759472089021811,\n",
       "   0.00776971710281817,\n",
       "   0.007754349582123631,\n",
       "   0.0076023784477742985,\n",
       "   0.007684341007933267,\n",
       "   0.0077082460401247735,\n",
       "   0.0076365275138469265,\n",
       "   0.007580180427924569,\n",
       "   0.0074982183577232174,\n",
       "   0.0075955464787462535,\n",
       "   0.007511878376118363,\n",
       "   0.007576766403241017,\n",
       "   0.0076484819897731544,\n",
       "   0.007429917775789866,\n",
       "   0.007225013580201723,\n",
       "   0.0073377092219193,\n",
       "   0.007223306077902331,\n",
       "   0.007252335086864871,\n",
       "   0.007228429564715747,\n",
       "   0.007354785714786088,\n",
       "   0.007328688122253181,\n",
       "   0.007470968384872325,\n",
       "   0.007385257158976581,\n",
       "   0.00739897107270973,\n",
       "   0.007580677734907031,\n",
       "   0.0076029629672128,\n",
       "   0.007644101768666533,\n",
       "   0.00776581312051643,\n",
       "   0.007836094601100125,\n",
       "   0.007841239156091124,\n",
       "   0.00790294833818696,\n",
       "   0.007868664043811709,\n",
       "   0.007817237602248834,\n",
       "   0.007866951151978515,\n",
       "   0.007918376613626155,\n",
       "   0.008021229986709523,\n",
       "   0.007964661439943739,\n",
       "   0.008014374009758185,\n",
       "   0.00799722941278247,\n",
       "   0.00781552471041564,\n",
       "   0.007770955225719338,\n",
       "   0.007686958851487259,\n",
       "   0.007666387000972303,\n",
       "   0.007890949276117477,\n",
       "   0.007949233654462163,\n",
       "   0.007873806149014615,\n",
       "   0.007897806722941669,\n",
       "   0.007932091997232155,\n",
       "   0.007858379343448275,\n",
       "   0.007921805337038249,\n",
       "   0.007961232226574027,\n",
       "   0.008041801837224482,\n",
       "   0.007971517416895077,\n",
       "   0.008048656344302962,\n",
       "   0.008019515625003474,\n",
       "   0.007959517374910359,\n",
       "   0.008060655896330064,\n",
       "   0.008036658262148718,\n",
       "   0.008221791197970025,\n",
       "   0.008172079118113199,\n",
       "   0.00815836863408338,\n",
       "   0.008050369236136156,\n",
       "   0.008010945286346091,\n",
       "   0.008021229986709523,\n",
       "   0.007993803629116083,\n",
       "   0.008043514239100056,\n",
       "   0.008204650030697636,\n",
       "   0.008172079118113199,\n",
       "   0.00812922497503818,\n",
       "   0.008218363454473165,\n",
       "   0.008321216337598917,\n",
       "   0.008391498798097847,\n",
       "   0.008333216379583633,\n",
       "   0.008305790511947813,\n",
       "   0.008353787740140974,\n",
       "   0.008376071992531505,\n",
       "   0.00845321189985463,\n",
       "   0.008737771445177683,\n",
       "   0.008938334616268657,\n",
       "   0.008856048194124064,\n",
       "   0.00884404913205458,\n",
       "   0.008883477001505595,\n",
       "   0.008928044526371423,\n",
       "   0.008877789573473642,\n",
       "   0.008812437066528943,\n",
       "   0.008924220897119067,\n",
       "   0.008867471066034553,\n",
       "   0.008779762772887065,\n",
       "   0.0089035868219866,\n",
       "   0.008791802011481241,\n",
       "   0.008619825907581195,\n",
       "   0.008588873325009641,\n",
       "   0.008767725494123366,\n",
       "   0.008848551842565751,\n",
       "   0.008831355310082509,\n",
       "   0.008748808230485038,\n",
       "   0.00886575229470994,\n",
       "   0.009053201260192434,\n",
       "   0.009111672802347268,\n",
       "   0.008939700618108172,\n",
       "   0.008772880828181964,\n",
       "   0.008709252972053294,\n",
       "   0.008790081280326155,\n",
       "   0.00891906458314523,\n",
       "   0.008853709136454821,\n",
       "   0.008889824892406868,\n",
       "   0.0087952405340457,\n",
       "   0.008728171215606859,\n",
       "   0.00878664373767693,\n",
       "   0.008805559041484789,\n",
       "   0.008805559041484789,\n",
       "   0.008776324250322607,\n",
       "   0.008606069857492885,\n",
       "   0.008731609738171322,\n",
       "   0.008821037782558655,\n",
       "   0.008803842229990646,\n",
       "   0.008803842229990646,\n",
       "   0.008690337668245438,\n",
       "   0.009445305522793029,\n",
       "   0.009293966433771164,\n",
       "   0.009254413135169894,\n",
       "   0.009204538389383829,\n",
       "   0.009302566169885642,\n",
       "   0.009259570429058969,\n",
       "   0.009132310797140684,\n",
       "   0.009225178344007712,\n",
       "   0.009252693383930049,\n",
       "   0.009211419354173692,\n",
       "   0.009149507329623928,\n",
       "   0.00910135429490818,\n",
       "   0.009213141065244014,\n",
       "   0.00919938011557952,\n",
       "   0.009428108010394549,\n",
       "   0.00946766326882629,\n",
       "   0.0095158153236268,\n",
       "   0.009429828741549634,\n",
       "   0.009502056333792778,\n",
       "   0.0094848588213943,\n",
       "   0.009498619771058793,\n",
       "   0.009692946761585441,\n",
       "   0.009954351889788054,\n",
       "   0.009866643596640568,\n",
       "   0.009794412084736477,\n",
       "   ...],\n",
       "  [0.004438468323027897,\n",
       "   0.004869460988126006,\n",
       "   0.003773308792925066,\n",
       "   0.0036004464176657438,\n",
       "   0.004343545242131795,\n",
       "   0.005562190089518514,\n",
       "   0.003339756927116523,\n",
       "   0.003977114231319638,\n",
       "   0.007170066099501258,\n",
       "   0.005381999094042948,\n",
       "   0.004145439841683191,\n",
       "   0.0053382600273555285,\n",
       "   0.007074328727470019,\n",
       "   0.004959033012991201,\n",
       "   0.004197670801637051,\n",
       "   0.0031371147617721487,\n",
       "   0.005074313372265756,\n",
       "   0.005743195376129218,\n",
       "   0.0068900662763187615,\n",
       "   0.0064973452945721434,\n",
       "   0.006845512918496204,\n",
       "   0.007079214474280847,\n",
       "   0.007715175850823725,\n",
       "   0.011105302501013792,\n",
       "   0.008054618926871305,\n",
       "   0.005991554172346347,\n",
       "   0.0037870354149173954,\n",
       "   0.008122437745697809,\n",
       "   0.01332854995455092,\n",
       "   0.00763048957276936,\n",
       "   0.005689684815820141,\n",
       "   0.003854970561048919,\n",
       "   0.004869577315431028,\n",
       "   0.004697412904001822,\n",
       "   0.005079199119076585,\n",
       "   0.004590042801468609,\n",
       "   0.005558700270367922,\n",
       "   0.00544190765612811,\n",
       "   0.0054947202526070705,\n",
       "   0.007000809870697547,\n",
       "   0.007467747673046755,\n",
       "   0.007096081933508709,\n",
       "   0.008951618775878464,\n",
       "   0.008163617611674794,\n",
       "   0.008239811996462721,\n",
       "   0.0054635445348617815,\n",
       "   0.004585971345792918,\n",
       "   0.004671355587677403,\n",
       "   0.0042101178232741635,\n",
       "   0.003822398915643394,\n",
       "   0.004567009995074703,\n",
       "   0.005228563378721921,\n",
       "   0.006842953717785771,\n",
       "   0.004590973419908767,\n",
       "   0.006044134114215265,\n",
       "   0.0044105497698231615,\n",
       "   0.008074627223334699,\n",
       "   0.005474130319618577,\n",
       "   0.006255617154741141,\n",
       "   0.006956721822095069,\n",
       "   0.006157553236609506,\n",
       "   0.003611962820862697,\n",
       "   0.004341218696031401,\n",
       "   0.004070292402640444,\n",
       "   0.003803321237620157,\n",
       "   0.0048855141562187294,\n",
       "   0.004546769044001269,\n",
       "   0.004809552426040844,\n",
       "   0.00420686065873361,\n",
       "   0.01353840441280652,\n",
       "   0.014876633729753525,\n",
       "   0.009383076750196652,\n",
       "   0.004660420821005547,\n",
       "   0.008131045966269271,\n",
       "   0.0036554692329400775,\n",
       "   0.005423178960019934,\n",
       "   0.0042501344162009505,\n",
       "   0.0045990000039551285,\n",
       "   0.007092824768968156,\n",
       "   0.0051002543612851566,\n",
       "   0.0037658638454038033,\n",
       "   0.0048806284094079005,\n",
       "   0.00302346298476787,\n",
       "   0.005280561684065742,\n",
       "   0.005362572434104653,\n",
       "   0.010500400514911185,\n",
       "   0.012976310874951172,\n",
       "   0.006597852086109194,\n",
       "   0.005490881451541418,\n",
       "   0.0046214511738239365,\n",
       "   0.007154012931408534,\n",
       "   0.0054971631260124845,\n",
       "   0.0067757165354843645,\n",
       "   0.019213199333584136,\n",
       "   0.008380335380926557,\n",
       "   0.007664340818530102,\n",
       "   0.009613055832220664,\n",
       "   0.0061501082890882425,\n",
       "   0.00622130059976032,\n",
       "   0.006198849429891511,\n",
       "   0.004349477934687802,\n",
       "   0.0039907245260069465,\n",
       "   0.004516989253916217,\n",
       "   0.004155327662609868,\n",
       "   0.004386586344989097,\n",
       "   0.007033032534188013,\n",
       "   0.0066313543499548775,\n",
       "   0.00976218743725596,\n",
       "   0.007040128499794217,\n",
       "   0.005466918026707354,\n",
       "   0.003148398510359063,\n",
       "   0.006771296097893615,\n",
       "   0.0041604460640307355,\n",
       "   0.004979739273284713,\n",
       "   0.0036935082616815296,\n",
       "   0.00433703091305069,\n",
       "   0.004764766413608248,\n",
       "   0.0033209119037033268,\n",
       "   0.003749461695396021,\n",
       "   0.006754428638665754,\n",
       "   0.006234794567142608,\n",
       "   0.0036111485297275593,\n",
       "   0.0077080798852175204,\n",
       "   0.0061278897738294745,\n",
       "   0.00813523374924998,\n",
       "   0.005390490987309389,\n",
       "   0.006144291923837256,\n",
       "   0.005367923490135561,\n",
       "   0.005953631470909914,\n",
       "   0.0038581113982844506,\n",
       "   0.006095667110339008,\n",
       "   0.00665601573861906,\n",
       "   0.006845163936581146,\n",
       "   0.006722554957090347,\n",
       "   0.01032323402936613,\n",
       "   0.009212540921037723,\n",
       "   0.007092243132443056,\n",
       "   0.007241491064783373,\n",
       "   0.005935019102106758,\n",
       "   0.006042156550029931,\n",
       "   0.003456898523271393,\n",
       "   0.00543341576286167,\n",
       "   0.005066868424744493,\n",
       "   0.007169484462976159,\n",
       "   0.0060079563223541296,\n",
       "   0.004713931381314624,\n",
       "   0.006895533659654689,\n",
       "   0.003825888734793985,\n",
       "   0.0033634876973405482,\n",
       "   0.005493091670336793,\n",
       "   0.005999231774477649,\n",
       "   0.005670258155881846,\n",
       "   0.004597720403599911,\n",
       "   0.005296149542938386,\n",
       "   0.003979440777420033,\n",
       "   0.0053928175334097845,\n",
       "   0.011872946386839008,\n",
       "   0.004950308465114721,\n",
       "   0.0055020488728233135,\n",
       "   0.005486809995865727,\n",
       "   0.005078384827941446,\n",
       "   0.004772676670349589,\n",
       "   0.0071060860817404045,\n",
       "   0.004799315623199108,\n",
       "   0.005441442346908032,\n",
       "   0.004216632152355267,\n",
       "   0.008686625175043513,\n",
       "   0.005919314915929093,\n",
       "   0.004423811082595411,\n",
       "   0.0036816428765695172,\n",
       "   0.0035896279782989095,\n",
       "   0.004564218139754228,\n",
       "   0.003416649275734566,\n",
       "   0.002886080437539566,\n",
       "   0.004810599371786022,\n",
       "   0.0042770060236605074,\n",
       "   0.006088454817427784,\n",
       "   0.007264872853092341,\n",
       "   0.007751004660769803,\n",
       "   0.005985505152485322,\n",
       "   0.004744409135229794,\n",
       "   0.0062776030153898705,\n",
       "   0.004552818063862294,\n",
       "   0.005913963859898186,\n",
       "   0.00449814423050302,\n",
       "   0.004649253399723653,\n",
       "   0.005501118254383154,\n",
       "   0.009880375979156008,\n",
       "   0.00958362502405067,\n",
       "   0.00795190191653888,\n",
       "   0.006868778379500151,\n",
       "   0.005260553387602348,\n",
       "   0.00607763637806095,\n",
       "   0.004656582019939896,\n",
       "   0.003117804429138873,\n",
       "   0.005597786244854553,\n",
       "   0.005201575443957343,\n",
       "   0.00473742949692861,\n",
       "   0.004523736237607361,\n",
       "   0.003361393805850193,\n",
       "   0.006491761583931197,\n",
       "   0.004055867816817997,\n",
       "   0.004974853526473884,\n",
       "   0.004096349718964864,\n",
       "   0.00431108992403129,\n",
       "   0.005204134644667778,\n",
       "   0.0026943730388670466,\n",
       "   0.0038964994089409634,\n",
       "   0.003144908691208471,\n",
       "   0.0033565080590393643,\n",
       "   0.003408390037078165,\n",
       "   0.0039002218827015947,\n",
       "   0.004486278845391008,\n",
       "   0.0037959926174039134,\n",
       "   0.003031140586899173,\n",
       "   0.005189477404235291,\n",
       "   0.0053989828805758285,\n",
       "   0.006159414473489821,\n",
       "   0.011505352102976651,\n",
       "   0.007854419634932346,\n",
       "   0.008020418699195503,\n",
       "   0.004520944382286889,\n",
       "   0.004810250389870963,\n",
       "   0.003472602709449057,\n",
       "   0.0037175880138206148,\n",
       "   0.0037228227425465023,\n",
       "   0.003520645886422207,\n",
       "   0.001273318680745993,\n",
       "   0.00559883319059973,\n",
       "   0.007080377747331046,\n",
       "   0.006713016118078729,\n",
       "   0.0049291368956011295,\n",
       "   0.005310806783370872,\n",
       "   0.0026862301275156646,\n",
       "   0.006694287421970552,\n",
       "   0.0054640098440818605,\n",
       "   0.0055491614313563035,\n",
       "   0.009019204940094928,\n",
       "   0.007638516156815722,\n",
       "   0.005209136718783626,\n",
       "   0.005353033595093035,\n",
       "   0.007004997653678258,\n",
       "   0.010886607167576695,\n",
       "   0.006543527234664979,\n",
       "   0.005879298323002304,\n",
       "   0.004478019606734607,\n",
       "   0.0026643605941719553,\n",
       "   0.0060582097181226535,\n",
       "   0.004911222490628091,\n",
       "   0.0031140819553782412,\n",
       "   0.002579209006897511,\n",
       "   0.0023855240440396564,\n",
       "   0.005809385612685446,\n",
       "   0.005836373547450024,\n",
       "   0.0052568309138417155,\n",
       "   0.0054662200628772355,\n",
       "   0.00443311726699699,\n",
       "   0.00511595854746282,\n",
       "   0.005391770587664606,\n",
       "   0.004005730748354492,\n",
       "   0.002726363047747473,\n",
       "   0.0036032382729862173,\n",
       "   0.004848754727832493,\n",
       "   0.007377128702436383,\n",
       "   0.007927589509789758,\n",
       "   0.004886212120048849,\n",
       "   0.0033622080969853314,\n",
       "   0.0050946706506442094,\n",
       "   0.006386950682108418,\n",
       "   0.005187267185439916,\n",
       "   0.005902912765921311,\n",
       "   0.006002488939018202,\n",
       "   0.005510191784174694,\n",
       "   0.006457445028950376,\n",
       "   0.00572888711761179,\n",
       "   0.008451295036988594,\n",
       "   0.0074318025357956565,\n",
       "   0.0060733322677752185,\n",
       "   0.00614626948802259,\n",
       "   0.005327208933378655,\n",
       "   0.007214154148103737,\n",
       "   0.00565967237112505,\n",
       "   0.01191563850778125,\n",
       "   0.00676489809611753,\n",
       "   0.004263628383583239,\n",
       "   0.007139704672891109,\n",
       "   0.006738608125183071,\n",
       "   0.011504421484536495,\n",
       "   0.007245678847764084,\n",
       "   0.006988595503670475,\n",
       "   0.004993000586056962,\n",
       "   0.006653572865213646,\n",
       "   0.005492277379201655,\n",
       "   0.007087124731022189,\n",
       "   0.021711444536187917,\n",
       "   0.012046157744013391,\n",
       "   0.0063343707402395,\n",
       "   0.0056646744452408995,\n",
       "   0.007896530119349491,\n",
       "   0.007222413386760137,\n",
       "   0.009210563356852388,\n",
       "   0.011237799301431269,\n",
       "   0.00820060969467107,\n",
       "   0.007075957309740295,\n",
       "   0.006315642044131322,\n",
       "   0.0050978114878797425,\n",
       "   0.003933724146547277,\n",
       "   0.0061164896979375395,\n",
       "   0.004474529787584014,\n",
       "   0.003390242977495087,\n",
       "   0.0048506159647128105,\n",
       "   0.003733292199998278,\n",
       "   0.00408855578952854,\n",
       "   0.003569387027225475,\n",
       "   0.0035976545623452702,\n",
       "   0.003470159836043643,\n",
       "   0.005101184979725314,\n",
       "   0.0038247254617437883,\n",
       "   0.003418975821834961,\n",
       "   0.0032548379944521183,\n",
       "   0.0025674599490905178,\n",
       "   0.004507217760294559,\n",
       "   0.008291112337976421,\n",
       "   0.007560693189757521,\n",
       "   0.00797016530342698,\n",
       "   0.007446110794313085,\n",
       "   0.00712516375976364,\n",
       "   0.005895118836484989,\n",
       "   0.004983578074350364,\n",
       "   0.005431321871371315,\n",
       "   0.007806027476044136,\n",
       "   0.004426253956000825,\n",
       "   0.0036660550176968727,\n",
       "   0.003853341978778642,\n",
       "   0.0042550201630117785,\n",
       "   0.003669312182237425,\n",
       "   0.0060667016113890955,\n",
       "   0.005631055854090196,\n",
       "   0.004203138184972978,\n",
       "   0.003292993350498591,\n",
       "   0.00677781042697472,\n",
       "   0.004908663289917657,\n",
       "   0.005892326981164516,\n",
       "   0.0164086643368634,\n",
       "   0.00843570717811595,\n",
       "   0.007350722404196903,\n",
       "   0.00646035321157587,\n",
       "   0.00839254974795363,\n",
       "   0.005449236276344354,\n",
       "   0.006937178834851753,\n",
       "   0.005589759660808191,\n",
       "   0.0060760077957906725,\n",
       "   0.005315343548266642,\n",
       "   0.0034827231849857737,\n",
       "   0.004297130647428923,\n",
       "   0.00676466544150749,\n",
       "   0.005414919721363533,\n",
       "   0.0047019496688975905,\n",
       "   0.003352669257973713,\n",
       "   0.006713830409213867,\n",
       "   0.007663991836615044,\n",
       "   0.0053360498085601536,\n",
       "   0.006373922023946209,\n",
       "   0.006546319089985452,\n",
       "   0.007806609112569234,\n",
       "   0.005695850162986187,\n",
       "   0.00493809409808765,\n",
       "   0.006967423934156884,\n",
       "   0.006198267793366412,\n",
       "   0.005743893339959337,\n",
       "   0.0037228227425465023,\n",
       "   0.0056535070239590035,\n",
       "   0.0043035286492050075,\n",
       "   0.004980553564419852,\n",
       "   0.005385139931278481,\n",
       "   0.005676074521132833,\n",
       "   0.005685148050924372,\n",
       "   0.0045658467220245046,\n",
       "   0.0030345140787447445,\n",
       "   0.004096117064354824,\n",
       "   0.003937562947612929,\n",
       "   0.012913726784850556,\n",
       "   0.0048503833101027705,\n",
       "   0.0068246903308976726,\n",
       "   0.008471070678841947,\n",
       "   0.006079846596856325,\n",
       "   0.0061476654156828285,\n",
       "   0.010759461423190128,\n",
       "   0.005801591683249124,\n",
       "   0.0062208352905402395,\n",
       "   0.006907282717461682,\n",
       "   0.003366628534576081,\n",
       "   0.00299728934113843,\n",
       "   0.008340318787999769,\n",
       "   0.010641854517815177,\n",
       "   0.01218470356429189,\n",
       "   0.006239680313953437,\n",
       "   0.00921370419408792,\n",
       "   0.009236620673176809,\n",
       "   0.007662712236259825,\n",
       "   0.010024505510075457,\n",
       "   0.014638162754463072,\n",
       "   0.015418021007315359,\n",
       "   0.013287602743183977,\n",
       "   0.016524758987273094,\n",
       "   0.013123697570411172,\n",
       "   0.015336475566496525,\n",
       "   0.010560541731606385,\n",
       "   0.012536709989281605,\n",
       "   0.00765224277880805,\n",
       "   0.011964612303194556,\n",
       "   0.009860367682692614,\n",
       "   0.011064704271561908,\n",
       "   0.013875288288143665,\n",
       "   0.009228594089130446,\n",
       "   0.011035040808781874,\n",
       "   0.011952979572692585,\n",
       "   0.012568932652772069,\n",
       "   0.0063383258686101705,\n",
       "   0.008781082946719535,\n",
       "   0.007900019938500081,\n",
       "   0.009220800159694124,\n",
       "   0.01229951861434637,\n",
       "   0.010330562649582374,\n",
       "   0.010080924253010028,\n",
       "   0.0072803443846599656,\n",
       "   0.008603334824649384,\n",
       "   0.008935565607785741,\n",
       "   0.011187313251052705,\n",
       "   0.007915375142762686,\n",
       "   0.0081341868035048,\n",
       "   0.009686458361688114,\n",
       "   0.007007789508998732,\n",
       "   0.013574814859277695,\n",
       "   0.010272398997072506,\n",
       "   0.007062579669663025,\n",
       "   0.008915092002102267,\n",
       "   0.00774786382353427,\n",
       "   0.007550921696135863,\n",
       "   0.007758333280986045,\n",
       "   0.0068239923670675535,\n",
       "   0.008451411364293613,\n",
       "   0.008781780910549654,\n",
       "   0.007984008252724327,\n",
       "   0.008142911351381283,\n",
       "   0.0131556875792916,\n",
       "   0.0062691111221234285,\n",
       "   0.006367756676780164,\n",
       "   0.012361753722531923,\n",
       "   0.01373418326715473,\n",
       "   0.009465552809455642,\n",
       "   0.008129882693219072,\n",
       "   0.009841406331974398,\n",
       "   0.005680611286028602,\n",
       "   0.007838599121449661,\n",
       "   0.007009418091269008,\n",
       "   0.00993528246712532,\n",
       "   0.00857448565300449,\n",
       "   0.010126524556577764,\n",
       "   0.01450927210050121,\n",
       "   0.010273678597427723,\n",
       "   0.012154458464986759,\n",
       "   0.012350004664724929,\n",
       "   0.006569351896379359,\n",
       "   0.007558250316352106,\n",
       "   0.005144807719107714,\n",
       "   0.006387532318633518,\n",
       "   0.005844981768021484,\n",
       "   0.010713395810402314,\n",
       "   0.006204433140532458,\n",
       "   0.006369152604440399,\n",
       "   0.004241177213714431,\n",
       "   0.006812824945785659,\n",
       "   0.01346383861028887,\n",
       "   0.007319430359146595,\n",
       "   0.006192567755420445,\n",
       "   0.006512933153444788,\n",
       "   0.0043149287250969426,\n",
       "   0.006921939957894169,\n",
       "   0.003718053323040693,\n",
       "   0.005005563934999094,\n",
       "   0.0045863203277079784,\n",
       "   0.00544248929265321,\n",
       "   0.005170167071602016,\n",
       "   0.005682937832128997,\n",
       "   0.005322206859262806,\n",
       "   0.0040385350483700564,\n",
       "   0.004907848998782519,\n",
       "   0.007029542715037421,\n",
       "   0.004134737729621376,\n",
       "   0.006363336239189413,\n",
       "   0.004962406504836773,\n",
       "   0.005325115041888299,\n",
       "   0.0052083224276484865,\n",
       "   0.005078384827941446,\n",
       "   0.005797985536793512,\n",
       "   0.005970847912052835,\n",
       "   0.00436366986590021,\n",
       "   0.0030668530695402304,\n",
       "   0.0022531435709272013,\n",
       "   0.002273966158525733,\n",
       "   0.002985540283331437,\n",
       "   0.0031050084255867024,\n",
       "   0.0022461639326260166,\n",
       "   0.004834911778535146,\n",
       "   0.005411313574907921,\n",
       "   0.007959812173280224,\n",
       "   0.005480528321394662,\n",
       "   0.005152717975849056,\n",
       "   0.005187732494659996,\n",
       "   0.004574571269900985,\n",
       "   0.004622032810349035,\n",
       "   0.005884184069813134,\n",
       "   0.0067618735861870154,\n",
       "   0.005156440449609687,\n",
       "   0.005028015104867902,\n",
       "   0.006352866781737637,\n",
       "   0.005517985713611015,\n",
       "   0.008878797882936109,\n",
       "   0.007576048394020125,\n",
       "   0.009409017739216052,\n",
       "   0.0039484977142847846,\n",
       "   0.004753017355801254,\n",
       "   0.005464940462522018,\n",
       "   0.006748844928024807,\n",
       "   0.0036209200233492172,\n",
       "   0.005671072447016984,\n",
       "   0.004853873129253361,\n",
       "   0.004142647986362717,\n",
       "   0.0035283234885535096,\n",
       "   0.0036444181389632025,\n",
       "   0.004302714358069871,\n",
       "   0.005573939147325508,\n",
       "   0.004256881399892096,\n",
       "   0.0040604045817137675,\n",
       "   0.011518380761138864,\n",
       "   0.006615882818387252,\n",
       "   0.004610981716372161,\n",
       "   0.0040216675891421964,\n",
       "   0.0039051076295124228,\n",
       "   0.004774654234534924,\n",
       "   0.003340105909031582,\n",
       "   0.0034438698651091835,\n",
       "   0.004866669132805533,\n",
       "   0.003867650237296069,\n",
       "   0.003461551615472183,\n",
       "   0.005805663138924814,\n",
       "   0.006108928423111257,\n",
       "   0.005682123540993859,\n",
       "   0.006222347545505497,\n",
       "   0.004977063745269259,\n",
       "   0.004293059191753232,\n",
       "   0.0038049498198904335,\n",
       "   0.004456964364526035,\n",
       "   0.003457363832491473,\n",
       "   0.003706885901758799,\n",
       "   0.003882307477728556,\n",
       "   0.0028112819804118772,\n",
       "   0.0025573394735538013,\n",
       "   0.005047558092111217,\n",
       "   0.005479481375649484,\n",
       "   0.005638384474306439,\n",
       "   0.0061864024082544,\n",
       "   0.005023943649192212,\n",
       "   0.0037214268148862656,\n",
       "   0.003776798612075658,\n",
       "   0.003677920402808885,\n",
       "   0.0044817420804952384,\n",
       "   0.003888007515674523,\n",
       "   0.008065321038933121,\n",
       "   0.006029011564562701,\n",
       "   0.007045479555825125,\n",
       "   0.003953499788400632,\n",
       "   0.002541751614681157,\n",
       "   0.0038901014071648773,\n",
       "   0.004009918531335203,\n",
       "   0.0033747714459274624,\n",
       "   0.002798020667639628,\n",
       "   0.004420902899969918,\n",
       "   0.0032371562440891193,\n",
       "   0.0035582196059435814,\n",
       "   0.01245446658463265,\n",
       "   0.005945255904948494,\n",
       "   0.0039831632511806644,\n",
       "   0.0047939645671682,\n",
       "   0.003031140586899173,\n",
       "   0.004205813712988432,\n",
       "   0.0038946381720606476,\n",
       "   0.0027896451016782074,\n",
       "   0.0035714809187158297,\n",
       "   0.0042552528176218185,\n",
       "   0.00697847502813376,\n",
       "   0.007008603800133868,\n",
       "   0.003829145899334538,\n",
       "   0.006280162216100304,\n",
       "   0.006428828511915522,\n",
       "   0.01138030025008044,\n",
       "   0.005788795679696954,\n",
       "   0.008026816700971589,\n",
       "   0.008394178330223904,\n",
       "   0.0045253648198776385,\n",
       "   0.007189143777524496,\n",
       "   0.005604300573935657,\n",
       "   0.004177429850563618,\n",
       "   0.0030634795776946578,\n",
       "   0.006626584930449067,\n",
       "   0.004999514915138068,\n",
       "   0.0068484211011216965,\n",
       "   0.008815981138225454,\n",
       "   0.008389990547243193,\n",
       "   0.006051928043651588,\n",
       "   0.006011678796114761,\n",
       "   0.007368985791085002,\n",
       "   0.0049376287888675706,\n",
       "   0.003318818012212971,\n",
       "   0.0029301684861420445,\n",
       "   0.003342548782436997,\n",
       "   0.0037645842450485865,\n",
       "   0.0046957843217315465,\n",
       "   0.004268746785004107,\n",
       "   0.0049017999789214926,\n",
       "   0.005234263416667888,\n",
       "   0.005046045837145961,\n",
       "   0.005976431622693782,\n",
       "   0.004851662910457986,\n",
       "   0.004096815028184942,\n",
       "   0.0025073187323953164,\n",
       "   0.0032548379944521183,\n",
       "   0.004302714358069871,\n",
       "   0.002825590238929305,\n",
       "   0.0015785615291177713,\n",
       "   0.0026108500338628787,\n",
       "   0.005425738160730368,\n",
       "   0.0036915306974961938,\n",
       "   0.00358032179389733,\n",
       "   0.0031399066170926227,\n",
       "   0.00471137218060419,\n",
       "   0.005499257017502839,\n",
       "   0.002848157736103133,\n",
       "   0.003358950932444779,\n",
       "   0.004561426284433755,\n",
       "   0.0034531760495107623,\n",
       "   0.0029915893031924627,\n",
       "   0.0035761340109166195,\n",
       "   0.006274811160069396,\n",
       "   0.0028917804754855325,\n",
       "   0.004968804506612858,\n",
       "   0.004692294502580954,\n",
       "   0.005336166135865174,\n",
       "   0.003552984877217693,\n",
       "   0.0030872103479186834,\n",
       "   0.0036170812222835655,\n",
       "   0.002967742205663418,\n",
       "   0.003265307451903894,\n",
       "   0.0035798564846772508,\n",
       "   0.003146071964258668,\n",
       "   0.0035633380073644494,\n",
       "   0.0031867865210155745,\n",
       "   0.0030260221854783048,\n",
       "   0.003243321591255165,\n",
       "   0.004615751135877971,\n",
       "   0.020809675267674944,\n",
       "   0.008745486791383497,\n",
       "   0.006461051175405988,\n",
       "   0.008742578608758003,\n",
       "   0.007128769906219253,\n",
       "   0.007305354755239206,\n",
       "   0.004679149517113724,\n",
       "   0.004973224944203608,\n",
       "   0.008067182275813434,\n",
       "   0.008172574814161313,\n",
       "   0.003932095564277001,\n",
       "   0.0037344554730484756,\n",
       "   0.006261084538077067,\n",
       "   0.006624840020873771,\n",
       "   0.007780551796244816,\n",
       "   0.00649083096549104,\n",
       "   0.0046782188986735675,\n",
       "   0.004762323540202833,\n",
       "   0.0076414243394412155,\n",
       "   0.008759329740680845,\n",
       "   0.010952913731437943,\n",
       "   0.005930133355295929,\n",
       "   0.005844516458801405,\n",
       "   0.004795476822133457,\n",
       "   0.00722590320591073,\n",
       "   0.0057324932640674025,\n",
       "   0.003327077250869372,\n",
       "   0.005762854690677553,\n",
       "   0.006082173142956719,\n",
       "   0.005553116559726975,\n",
       "   0.007914095542407467,\n",
       "   0.006867498779144934,\n",
       "   0.005204483626582836,\n",
       "   0.004071572002995661,\n",
       "   0.009052474549330572,\n",
       "   0.004550840499676959,\n",
       "   0.004386004708463997,\n",
       "   0.0049462370094390294,\n",
       "   0.0048977285232458005,\n",
       "   0.004888654993454263,\n",
       "   0.0043408697141163405,\n",
       "   0.004301434757714652,\n",
       "   0.0038160009138673077,\n",
       "   0.00628202345298062,\n",
       "   0.010522153720949872,\n",
       "   0.0064875738009504864,\n",
       "   0.003962922300107231,\n",
       "   0.005364201016374929,\n",
       "   0.006653223883298586,\n",
       "   0.0053487294848073045,\n",
       "   0.0060667016113890955,\n",
       "   0.0055556757604374105,\n",
       "   0.005191571295725646,\n",
       "   0.005450515876699571,\n",
       "   0.0044081068964177475,\n",
       "   0.006058675027342733,\n",
       "   0.005252410476250967,\n",
       "   0.004980669891724871,\n",
       "   0.0063690362771353795,\n",
       "   0.0033718632633019693,\n",
       "   0.0057578526165617035,\n",
       "   0.005154579212729371,\n",
       "   0.008498872904741666,\n",
       "   0.010738289853676536,\n",
       "   0.008328337075582736,\n",
       "   0.01710930369499725,\n",
       "   0.006511304571174513,\n",
       "   0.003653026359534663,\n",
       "   0.0053389579911856475,\n",
       "   0.005586735150877678,\n",
       "   0.005616980250182808,\n",
       "   0.006457445028950376,\n",
       "   0.003913017886253765,\n",
       "   0.004414388570888814,\n",
       "   0.0035588012424686796,\n",
       "   0.006387067009413439,\n",
       "   0.00580356924743446,\n",
       "   0.005409568665332625,\n",
       "   0.004958102394551043,\n",
       "   0.007251611540320091,\n",
       "   0.010057542464701063,\n",
       "   0.005647458004097978,\n",
       "   0.004280030533591022,\n",
       "   0.005618027195927986,\n",
       "   0.00756011155323242,\n",
       "   0.007715408505433764,\n",
       "   0.006193847355775662,\n",
       "   0.007668993910730891,\n",
       "   0.0018147059583078285,\n",
       "   0.004162539955521092,\n",
       "   0.0032846177845371697,\n",
       "   0.0037811027223613883,\n",
       "   0.00547657319302399,\n",
       "   0.010225402765844535,\n",
       "   0.00669079760281996,\n",
       "   0.007483102877309359,\n",
       "   0.0041755686136833005,\n",
       "   0.0045331587493139605,\n",
       "   0.006674744434727237,\n",
       "   0.0064221978555293975,\n",
       "   0.003774472065975264,\n",
       "   0.003756325006392185,\n",
       "   0.005547416521781009,\n",
       "   0.003512502975070826,\n",
       "   0.007723318762175106,\n",
       "   0.003879515622408082,\n",
       "   0.003680828585434379,\n",
       "   0.0043518044807881965,\n",
       "   0.0050608194048834656,\n",
       "   0.005150740411663721,\n",
       "   0.003856250161404136,\n",
       "   0.004799548277809147,\n",
       "   0.00478547267390176,\n",
       "   0.0055020488728233135,\n",
       "   0.004598651022040069,\n",
       "   0.005644666148777505,\n",
       "   0.003513433593510984,\n",
       "   0.004482556371630376,\n",
       "   0.0051102585095168535,\n",
       "   0.002967509551053379,\n",
       "   0.004741268297994261,\n",
       "   0.003860321617079826,\n",
       "   0.003337663035626168,\n",
       "   0.006253988572470864,\n",
       "   0.02383534847123819,\n",
       "   0.006693938440055493,\n",
       "   0.006293539856177572,\n",
       "   0.005553930850862113,\n",
       "   0.005455866932730479,\n",
       "   0.005891047380809297,\n",
       "   0.010059752683496436,\n",
       "   0.005796240627218216,\n",
       "   0.0056254721434492476,\n",
       "   0.005024059976497231,\n",
       "   0.0039600141174817366,\n",
       "   0.0053487294848073045,\n",
       "   0.004873881425716756,\n",
       "   0.003926046544415975,\n",
       "   0.005295567906413288,\n",
       "   0.008899969452449702,\n",
       "   0.006127191809999356,\n",
       "   0.003644534466268223,\n",
       "   0.004618310336588404,\n",
       "   0.004194995273621597,\n",
       "   0.003004617961354673,\n",
       "   0.01015839823815317,\n",
       "   0.005269626917393887,\n",
       "   0.007170298754111298,\n",
       "   0.008858789586472716,\n",
       "   0.004549560899321742,\n",
       "   0.004084716988462891,\n",
       "   0.005480295666784623,\n",
       "   0.00349319264243755,\n",
       "   0.004132062201605922,\n",
       "   0.003828796917419479,\n",
       "   0.008584955110456266,\n",
       "   0.006372758750896011,\n",
       "   0.008064390420492963,\n",
       "   0.0038953361358907658,\n",
       "   0.0026986771491527765,\n",
       "   0.003527741852028411,\n",
       "   0.009149956830937106,\n",
       "   0.009892474018878059,\n",
       "   0.01260278389853281,\n",
       "   0.007326526324752798,\n",
       "   0.0073067506828994435,\n",
       "   0.006467449177182073,\n",
       "   0.004999165933223007,\n",
       "   0.0038062294202456507,\n",
       "   0.005728421808391712,\n",
       "   0.004912153109068247,\n",
       "   0.003447476011564795,\n",
       "   0.00858879391152192,\n",
       "   0.004132062201605922,\n",
       "   0.0056127924672020975,\n",
       "   0.004433582576217069,\n",
       "   0.0055905739519433294,\n",
       "   0.0077085451944376,\n",
       "   0.004585040727352761,\n",
       "   0.005730981009102146,\n",
       "   0.0034472433569547557,\n",
       "   0.005136781135061353,\n",
       "   0.007581748431966091,\n",
       "   0.006923568540164445,\n",
       "   0.009892008709657979,\n",
       "   0.016732519554038337,\n",
       "   0.007953995808029236,\n",
       "   0.00528184128442096,\n",
       "   0.009266982099786958,\n",
       "   0.00653864148785415,\n",
       "   0.008816562774750555,\n",
       "   0.006143128650787059,\n",
       "   0.003619058786468901,\n",
       "   0.007796023327812438,\n",
       "   0.005888953489318943,\n",
       "   0.005026502849902646,\n",
       "   0.007494386625896274,\n",
       "   0.006000395047527847,\n",
       "   0.005470989482383044,\n",
       "   0.004057612726393293,\n",
       "   0.006147316433767769,\n",
       "   0.004523387255692303,\n",
       "   0.004440911196433312,\n",
       "   0.006365779112594827,\n",
       "   0.004440794869128292,\n",
       "   0.004219540334980761,\n",
       "   0.002728340611932809,\n",
       "   0.003252744102961763,\n",
       "   0.004100072192725495,\n",
       "   0.009441822039231615,\n",
       "   0.0102582070658601,\n",
       "   0.0059416497584928825,\n",
       "   0.0046692616961870465,\n",
       "   0.004774188925314846,\n",
       "   0.006007025703913971,\n",
       "   0.0056025556643603605,\n",
       "   0.00903595607201777,\n",
       "   0.006966144333801667,\n",
       "   0.004856781311878856,\n",
       "   0.0031558434578803263,\n",
       "   0.003411530874313698,\n",
       "   0.004179523742053972,\n",
       "   0.005653158042043946,\n",
       "   0.005330117116004148,\n",
       "   0.0034663210349779924,\n",
       "   0.003217845911455843,\n",
       "   0.003956291643721107,\n",
       "   0.0032960178604291034,\n",
       "   0.001944178248794791,\n",
       "   0.0032494869384212107,\n",
       "   0.0019180046051653513,\n",
       "   0.002983795373756141,\n",
       "   0.0047684888873688784,\n",
       "   0.0029784443177252326,\n",
       "   0.0023096786411667906,\n",
       "   0.006004582830508557,\n",
       "   0.003190857976691265,\n",
       "   0.004540254714920164,\n",
       "   0.004565497740109445,\n",
       "   0.004451845963105165,\n",
       "   0.003901617810361832,\n",
       "   0.0030498692830073486,\n",
       "   0.0037092124478591937,\n",
       "   0.002768008222944537,\n",
       "   0.0027684735321646162,\n",
       "   0.003184808956830239,\n",
       "   0.0020281665630190373,\n",
       "   0.0033796571927382914,\n",
       "   0.0032321541699732704,\n",
       "   0.010909407319360563,\n",
       "   0.00589465352726491,\n",
       "   0.0028094207435315622,\n",
       "   0.003731081981202904,\n",
       "   0.0031696864071776736,\n",
       "   0.005119448366613412,\n",
       "   0.0027661469860642213,\n",
       "   0.0032722870902050787,\n",
       "   0.003469112890298465,\n",
       "   0.002586304972503715,\n",
       "   0.0030909328216793147,\n",
       "   0.0022094045042397813,\n",
       "   0.0025460557249668875,\n",
       "   0.0041013517930807136,\n",
       "   0.0021355366655522517,\n",
       "   0.002167526674432677,\n",
       "   0.002714032353415381,\n",
       "   0.002961809513107412,\n",
       "   0.0029001560414469536,\n",
       "   0.0023880832447500905,\n",
       "   0.003481908893850636,\n",
       "   0.003024626257818067,\n",
       "   0.00256687831256542,\n",
       "   0.0033242853955488985,\n",
       "   0.029510608701235886,\n",
       "   0.01738767493590947,\n",
       "   0.0044183436992594844,\n",
       "   0.0035839279403529416,\n",
       "   0.00465937387526037,\n",
       "   0.004358784119089382,\n",
       "   0.003431539170777092,\n",
       "   0.0043095776690660345,\n",
       "   0.004873881425716756,\n",
       "   0.003286246366807446,\n",
       "   0.003054871357123198,\n",
       "   0.0030376549159802767,\n",
       "   0.002247559860286253,\n",
       "   0.003916973014624436,\n",
       "   0.003952685497265494,\n",
       "   0.00394570585896431,\n",
       "   0.002440779513924029,\n",
       "   0.003432120807302191,\n",
       "   0.0031905089947762057,\n",
       "   0.004046096323196339,\n",
       "   0.0057705322928088545,\n",
       "   0.004684965882364711,\n",
       "   0.004157305226795203,\n",
       "   0.006961142259685819,\n",
       "   0.0038193744057128795,\n",
       "   0.002904227497122644,\n",
       "   0.0039002218827015947,\n",
       "   0.002892594766620671,\n",
       "   0.0019417353753893765,\n",
       "   0.0022116147230351567,\n",
       "   0.003082557255717894,\n",
       "   0.003140255599007681,\n",
       "   0.0022928111819389297,\n",
       "   0.002566296676040321,\n",
       "   0.0023234052631591192,\n",
       "   0.005771230256638974,\n",
       "   0.003025789530868265,\n",
       "   0.003227268423162441,\n",
       "   0.0038333336823152494,\n",
       "   0.003431539170777092,\n",
       "   0.004610167425237023,\n",
       "   0.017911962099633406,\n",
       "   0.006718832483329716,\n",
       "   0.0040216675891421964,\n",
       "   0.0037752863571104017,\n",
       "   0.003931630255056923,\n",
       "   0.003107334971687097,\n",
       "   0.002846180171917797,\n",
       "   0.005197969297501732,\n",
       "   0.0019348720643932126,\n",
       "   0.0014154706474801065,\n",
       "   0.002372960695097525,\n",
       "   0.0040280655909182795,\n",
       "   0.003928605745126409,\n",
       "   0.0029441277627444127,\n",
       "   0.004965431014767286,\n",
       "   0.003445149465464401,\n",
       "   0.003784127232291901,\n",
       "   0.003812860076631776,\n",
       "   0.004414039588973753,\n",
       "   0.003289154549432939,\n",
       "   0.0027410202881799593,\n",
       "   0.005216930648219948,\n",
       "   0.006726626412766039,\n",
       "   0.0030787184546522428,\n",
       "   0.004803852388094877,\n",
       "   ...]]}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
