{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  390748\n",
      " 0:  389090\n",
      " 1:  397485\n"
     ]
    }
   ],
   "source": [
    "stock_data_preprocessed = pd.read_csv('stock_data.csv',parse_dates=True, index_col=[0,1])\n",
    "stock_data_preprocessed = stock_data_preprocessed[['Adj Close','Volume','target']]\n",
    "get_target_distribution(stock_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-03-16</th>\n",
       "      <th>A</th>\n",
       "      <td>0.005604</td>\n",
       "      <td>0.003822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAL</th>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAP</th>\n",
       "      <td>0.010572</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.007148</td>\n",
       "      <td>0.129969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABC</th>\n",
       "      <td>0.006258</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2019-12-31</th>\n",
       "      <th>YUM</th>\n",
       "      <td>0.025759</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBH</th>\n",
       "      <td>0.038353</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZBRA</th>\n",
       "      <td>0.065616</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZION</th>\n",
       "      <td>0.013238</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZTS</th>\n",
       "      <td>0.033893</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1177323 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target\n",
       "Date       Ticker                             \n",
       "2010-03-16 A        0.005604  0.003822       1\n",
       "           AAL      0.001880  0.006881      -1\n",
       "           AAP      0.010572  0.000981       1\n",
       "           AAPL     0.007148  0.129969       0\n",
       "           ABC      0.006258  0.004898       1\n",
       "...                      ...       ...     ...\n",
       "2019-12-31 YUM      0.025759  0.001479       0\n",
       "           ZBH      0.038353  0.000712       0\n",
       "           ZBRA     0.065616  0.000423       0\n",
       "           ZION     0.013238  0.001450       0\n",
       "           ZTS      0.033893  0.001313       0\n",
       "\n",
       "[1177323 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.3 s, sys: 450 ms, total: 18.8 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"price_train.json\", training_data)\n",
    "write_json_dataset(\"price_test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "CPU times: user 96.5 ms, sys: 4.01 ms, total: 100 ms\n",
      "Wall time: 198 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"price_train.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"price_test.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "price_estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='deep-ar-price-volume',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-12 01:34:52 Starting - Starting the training job...\n",
      "2020-06-12 01:34:54 Starting - Launching requested ML instances.........\n",
      "2020-06-12 01:36:26 Starting - Preparing the instances for training......\n",
      "2020-06-12 01:37:39 Downloading - Downloading input data...\n",
      "2020-06-12 01:38:08 Training - Training image download completed. Training in progress.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:11 INFO 140607820867392] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:11 INFO 140607820867392] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:11 INFO 140607820867392] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:11 INFO 140607820867392] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] Real time series\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] min/mean/max target: -1.0/0.00129573177972/1.0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] mean abs(target): 0.671014271099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:12 INFO 140607820867392] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] Real time series\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] number of observations: 10562640\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] mean target length: 2151\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] min/mean/max target: -1.0/0.00289700302197/1.0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] mean abs(target): 0.671493300917\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] nvidia-smi took: 0.0252120494843 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:15 INFO 140607820867392] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 227.03790664672852, \"sum\": 227.03790664672852, \"min\": 227.03790664672852}}, \"EndTime\": 1591925896.177471, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925895.949528}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:16 INFO 140607820867392] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 439.41617012023926, \"sum\": 439.41617012023926, \"min\": 439.41617012023926}}, \"EndTime\": 1591925896.389076, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925896.177557}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:17 INFO 140607820867392] Epoch[0] Batch[0] avg_epoch_loss=1.376866\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.37686598301\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:17 INFO 140607820867392] Epoch[0] Batch[5] avg_epoch_loss=1.361090\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.36108980576\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:17 INFO 140607820867392] Epoch[0] Batch [5]#011Speed: 1096.97 samples/sec#011loss=1.361090\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] Epoch[0] Batch[10] avg_epoch_loss=1.299936\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=1.2265504241\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] Epoch[0] Batch [10]#011Speed: 495.18 samples/sec#011loss=1.226550\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 1664.7720336914062, \"sum\": 1664.7720336914062, \"min\": 1664.7720336914062}}, \"EndTime\": 1591925898.05399, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925896.389136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=391.015188132 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.29993554137\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_69d66bf3-9bdb-477d-ad3d-97ab45d0f690-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.964860916137695, \"sum\": 20.964860916137695, \"min\": 20.964860916137695}}, \"EndTime\": 1591925898.075572, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925898.054081}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] Epoch[1] Batch[0] avg_epoch_loss=1.249982\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.24998152256\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] Epoch[1] Batch[5] avg_epoch_loss=1.233278\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.2332777977\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] Epoch[1] Batch [5]#011Speed: 802.04 samples/sec#011loss=1.233278\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] Epoch[1] Batch[10] avg_epoch_loss=1.238702\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=1.24521036148\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] Epoch[1] Batch [10]#011Speed: 460.21 samples/sec#011loss=1.245210\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1695.422887802124, \"sum\": 1695.422887802124, \"min\": 1695.422887802124}}, \"EndTime\": 1591925899.771116, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925898.075638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=387.488109626 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.23870169033\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:19 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_ba8b48ac-2197-4b33-91ac-554306689b77-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.394004821777344, \"sum\": 31.394004821777344, \"min\": 31.394004821777344}}, \"EndTime\": 1591925899.803046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925899.771194}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:20 INFO 140607820867392] Epoch[2] Batch[0] avg_epoch_loss=1.270680\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.2706798315\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:20 INFO 140607820867392] Epoch[2] Batch[5] avg_epoch_loss=1.208919\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.2089189291\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:20 INFO 140607820867392] Epoch[2] Batch [5]#011Speed: 1014.38 samples/sec#011loss=1.208919\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] Epoch[2] Batch[10] avg_epoch_loss=1.209519\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.21023917198\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] Epoch[2] Batch [10]#011Speed: 546.22 samples/sec#011loss=1.210239\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1591.2389755249023, \"sum\": 1591.2389755249023, \"min\": 1591.2389755249023}}, \"EndTime\": 1591925901.394422, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925899.803115}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=419.139641511 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.2095190395\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:21 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_9b7ab7e4-3465-45b0-8a96-31b0ff03072d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.5690860748291, \"sum\": 20.5690860748291, \"min\": 20.5690860748291}}, \"EndTime\": 1591925901.41556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925901.394497}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] Epoch[3] Batch[0] avg_epoch_loss=1.175195\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.1751948595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] Epoch[3] Batch[5] avg_epoch_loss=1.214402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.21440164248\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] Epoch[3] Batch [5]#011Speed: 1020.46 samples/sec#011loss=1.214402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] Epoch[3] Batch[10] avg_epoch_loss=1.245351\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.28249082565\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] Epoch[3] Batch [10]#011Speed: 539.61 samples/sec#011loss=1.282491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1506.5510272979736, \"sum\": 1506.5510272979736, \"min\": 1506.5510272979736}}, \"EndTime\": 1591925902.922219, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925901.415612}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=428.761538694 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.2453512712\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:22 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:23 INFO 140607820867392] Epoch[4] Batch[0] avg_epoch_loss=1.136562\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.13656151295\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:23 INFO 140607820867392] Epoch[4] Batch[5] avg_epoch_loss=1.163901\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.16390083234\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:23 INFO 140607820867392] Epoch[4] Batch [5]#011Speed: 1081.69 samples/sec#011loss=1.163901\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] Epoch[4] Batch[10] avg_epoch_loss=1.161249\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.15806779861\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] Epoch[4] Batch [10]#011Speed: 520.08 samples/sec#011loss=1.158068\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.054111480713, \"sum\": 1510.054111480713, \"min\": 1510.054111480713}}, \"EndTime\": 1591925904.432795, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925902.922298}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=425.780421667 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.16124945337\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:24 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_6d2cfc83-350f-444a-9127-b81ec62ec505-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.43906593322754, \"sum\": 31.43906593322754, \"min\": 31.43906593322754}}, \"EndTime\": 1591925904.464801, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925904.432873}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:25 INFO 140607820867392] Epoch[5] Batch[0] avg_epoch_loss=1.092379\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.09237933159\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:25 INFO 140607820867392] Epoch[5] Batch[5] avg_epoch_loss=1.161284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.16128438711\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:25 INFO 140607820867392] Epoch[5] Batch [5]#011Speed: 1068.02 samples/sec#011loss=1.161284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Epoch[5] Batch[10] avg_epoch_loss=1.157576\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=1.15312681198\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Epoch[5] Batch [10]#011Speed: 542.80 samples/sec#011loss=1.153127\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] processed a total of 690 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1556.7059516906738, \"sum\": 1556.7059516906738, \"min\": 1556.7059516906738}}, \"EndTime\": 1591925906.021643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925904.464875}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=443.210574314 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.15757639842\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_3df10921-4362-4b9b-b870-f1983c17dacd-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.38811683654785, \"sum\": 30.38811683654785, \"min\": 30.38811683654785}}, \"EndTime\": 1591925906.05258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925906.021721}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Epoch[6] Batch[0] avg_epoch_loss=1.169877\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.16987729073\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Epoch[6] Batch[5] avg_epoch_loss=1.137002\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.13700193167\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:26 INFO 140607820867392] Epoch[6] Batch [5]#011Speed: 1047.20 samples/sec#011loss=1.137002\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] Epoch[6] Batch[10] avg_epoch_loss=1.125392\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.11146037579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] Epoch[6] Batch [10]#011Speed: 544.25 samples/sec#011loss=1.111460\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1489.116907119751, \"sum\": 1489.116907119751, \"min\": 1489.116907119751}}, \"EndTime\": 1591925907.541843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925906.052655}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=433.780892588 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.12539213354\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:27 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_521a6aa9-0191-463c-b3bc-59340352ca94-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.57790756225586, \"sum\": 20.57790756225586, \"min\": 20.57790756225586}}, \"EndTime\": 1591925907.563026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925907.541919}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:28 INFO 140607820867392] Epoch[7] Batch[0] avg_epoch_loss=1.086701\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.08670139313\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:28 INFO 140607820867392] Epoch[7] Batch[5] avg_epoch_loss=1.077214\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.07721408208\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:28 INFO 140607820867392] Epoch[7] Batch [5]#011Speed: 1061.47 samples/sec#011loss=1.077214\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] Epoch[7] Batch[10] avg_epoch_loss=1.078403\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.0798289299\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] Epoch[7] Batch [10]#011Speed: 530.17 samples/sec#011loss=1.079829\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1495.3091144561768, \"sum\": 1495.3091144561768, \"min\": 1495.3091144561768}}, \"EndTime\": 1591925909.058477, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925907.563105}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=439.340551466 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.07840264927\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_25d25d4a-8fab-493f-b816-cf9f01e9529c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.235933303833008, \"sum\": 31.235933303833008, \"min\": 31.235933303833008}}, \"EndTime\": 1591925909.090262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925909.058557}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] Epoch[8] Batch[0] avg_epoch_loss=1.120533\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.12053263187\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] Epoch[8] Batch[5] avg_epoch_loss=1.095729\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.09572937091\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] Epoch[8] Batch [5]#011Speed: 1045.95 samples/sec#011loss=1.095729\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1453.9220333099365, \"sum\": 1453.9220333099365, \"min\": 1453.9220333099365}}, \"EndTime\": 1591925910.544358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925909.090371}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=429.147707071 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.08421280384\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:30 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] Epoch[9] Batch[0] avg_epoch_loss=1.017381\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.01738083363\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] Epoch[9] Batch[5] avg_epoch_loss=1.016563\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.01656312744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] Epoch[9] Batch [5]#011Speed: 1073.06 samples/sec#011loss=1.016563\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1404.3400287628174, \"sum\": 1404.3400287628174, \"min\": 1404.3400287628174}}, \"EndTime\": 1591925911.94928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925910.544442}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=443.585894823 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.04188092351\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:31 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_7d1a59e0-def8-4c08-aea4-ef4fb61be2b3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.776987075805664, \"sum\": 20.776987075805664, \"min\": 20.776987075805664}}, \"EndTime\": 1591925911.970624, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925911.949364}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:32 INFO 140607820867392] Epoch[10] Batch[0] avg_epoch_loss=1.016767\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.01676690578\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:32 INFO 140607820867392] Epoch[10] Batch[5] avg_epoch_loss=1.060038\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.06003848712\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:32 INFO 140607820867392] Epoch[10] Batch [5]#011Speed: 1060.41 samples/sec#011loss=1.060038\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:33 INFO 140607820867392] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1423.4910011291504, \"sum\": 1423.4910011291504, \"min\": 1423.4910011291504}}, \"EndTime\": 1591925913.39426, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925911.970702}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:33 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=446.048166274 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:33 INFO 140607820867392] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.06158893108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:33 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] Epoch[11] Batch[0] avg_epoch_loss=1.028359\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.02835929394\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] Epoch[11] Batch[5] avg_epoch_loss=1.037872\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.03787163893\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] Epoch[11] Batch [5]#011Speed: 1026.30 samples/sec#011loss=1.037872\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] Epoch[11] Batch[10] avg_epoch_loss=1.056967\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=1.07988060713\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] Epoch[11] Batch [10]#011Speed: 553.88 samples/sec#011loss=1.079881\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1541.8260097503662, \"sum\": 1541.8260097503662, \"min\": 1541.8260097503662}}, \"EndTime\": 1591925914.936679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925913.394344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=442.297915621 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.05696662448\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:34 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:35 INFO 140607820867392] Epoch[12] Batch[0] avg_epoch_loss=1.028469\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.02846860886\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:35 INFO 140607820867392] Epoch[12] Batch[5] avg_epoch_loss=1.036434\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.03643426299\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:35 INFO 140607820867392] Epoch[12] Batch [5]#011Speed: 1057.75 samples/sec#011loss=1.036434\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] Epoch[12] Batch[10] avg_epoch_loss=1.047821\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.0614846468\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] Epoch[12] Batch [10]#011Speed: 555.76 samples/sec#011loss=1.061485\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1463.0179405212402, \"sum\": 1463.0179405212402, \"min\": 1463.0179405212402}}, \"EndTime\": 1591925916.400232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925914.936762}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=440.836964572 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.04782080108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] Epoch[13] Batch[0] avg_epoch_loss=1.015449\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.01544868946\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] Epoch[13] Batch[5] avg_epoch_loss=1.008965\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.00896544258\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] Epoch[13] Batch [5]#011Speed: 1058.86 samples/sec#011loss=1.008965\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1387.9292011260986, \"sum\": 1387.9292011260986, \"min\": 1387.9292011260986}}, \"EndTime\": 1591925917.788683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925916.400303}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=461.081474565 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.01761246324\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:37 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_649bb145-e384-42f0-9c40-2c5cd08caa2e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.936012268066406, \"sum\": 20.936012268066406, \"min\": 20.936012268066406}}, \"EndTime\": 1591925917.810212, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925917.788763}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:38 INFO 140607820867392] Epoch[14] Batch[0] avg_epoch_loss=0.997236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.997236311436\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:38 INFO 140607820867392] Epoch[14] Batch[5] avg_epoch_loss=0.991126\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=0.991126040618\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:38 INFO 140607820867392] Epoch[14] Batch [5]#011Speed: 552.15 samples/sec#011loss=0.991126\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1399.1940021514893, \"sum\": 1399.1940021514893, \"min\": 1399.1940021514893}}, \"EndTime\": 1591925919.209539, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925917.810286}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=433.786444464 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=14, train loss <loss>=0.988469606638\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_32dad5f6-a366-472f-8d60-5e1e5d2c5dd5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.9200382232666, \"sum\": 20.9200382232666, \"min\": 20.9200382232666}}, \"EndTime\": 1591925919.231081, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925919.209613}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] Epoch[15] Batch[0] avg_epoch_loss=1.017886\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.01788592339\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] Epoch[15] Batch[5] avg_epoch_loss=0.967007\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.967007150253\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] Epoch[15] Batch [5]#011Speed: 1092.31 samples/sec#011loss=0.967007\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1444.0879821777344, \"sum\": 1444.0879821777344, \"min\": 1444.0879821777344}}, \"EndTime\": 1591925920.67529, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925919.231143}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=411.297782632 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=15, train loss <loss>=0.902330583334\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:40 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_d4d422d9-2c8c-4327-81b3-ae6f15dacad4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.218217849731445, \"sum\": 32.218217849731445, \"min\": 32.218217849731445}}, \"EndTime\": 1591925920.708185, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925920.675373}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:41 INFO 140607820867392] Epoch[16] Batch[0] avg_epoch_loss=0.954796\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.954796016216\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:41 INFO 140607820867392] Epoch[16] Batch[5] avg_epoch_loss=0.948208\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.948208242655\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:41 INFO 140607820867392] Epoch[16] Batch [5]#011Speed: 1036.41 samples/sec#011loss=0.948208\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] Epoch[16] Batch[10] avg_epoch_loss=0.962291\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=0.979189693928\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] Epoch[16] Batch [10]#011Speed: 551.10 samples/sec#011loss=0.979190\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.0858917236328, \"sum\": 1486.0858917236328, \"min\": 1486.0858917236328}}, \"EndTime\": 1591925922.194406, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925920.708256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=447.44503454 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.962290720506\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] Epoch[17] Batch[0] avg_epoch_loss=0.992996\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=0.992995679379\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] Epoch[17] Batch[5] avg_epoch_loss=0.958691\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.958691348632\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] Epoch[17] Batch [5]#011Speed: 1075.92 samples/sec#011loss=0.958691\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] Epoch[17] Batch[10] avg_epoch_loss=0.945181\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.928968524933\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] Epoch[17] Batch [10]#011Speed: 547.95 samples/sec#011loss=0.928969\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1477.841854095459, \"sum\": 1477.841854095459, \"min\": 1477.841854095459}}, \"EndTime\": 1591925923.672741, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925922.1945}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=457.388798695 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.945180974223\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:43 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:44 INFO 140607820867392] Epoch[18] Batch[0] avg_epoch_loss=0.944270\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.944270312786\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:44 INFO 140607820867392] Epoch[18] Batch[5] avg_epoch_loss=0.927467\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.927467385928\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:44 INFO 140607820867392] Epoch[18] Batch [5]#011Speed: 1061.04 samples/sec#011loss=0.927467\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] Epoch[18] Batch[10] avg_epoch_loss=0.946698\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.969774794579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] Epoch[18] Batch [10]#011Speed: 514.59 samples/sec#011loss=0.969775\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1528.0380249023438, \"sum\": 1528.0380249023438, \"min\": 1528.0380249023438}}, \"EndTime\": 1591925925.201276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925923.672818}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=437.128748357 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.946698026224\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] Epoch[19] Batch[0] avg_epoch_loss=0.854769\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:45 INFO 140607820867392] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.854769051075\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] Epoch[19] Batch[5] avg_epoch_loss=0.917155\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.917154898246\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] Epoch[19] Batch [5]#011Speed: 1057.76 samples/sec#011loss=0.917155\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] Epoch[19] Batch[10] avg_epoch_loss=0.934290\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=0.954851603508\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] Epoch[19] Batch [10]#011Speed: 550.52 samples/sec#011loss=0.954852\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1501.272201538086, \"sum\": 1501.272201538086, \"min\": 1501.272201538086}}, \"EndTime\": 1591925926.70304, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925925.201355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=450.9169946 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.934289764274\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:46 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:47 INFO 140607820867392] Epoch[20] Batch[0] avg_epoch_loss=0.917773\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.91777318716\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:47 INFO 140607820867392] Epoch[20] Batch[5] avg_epoch_loss=0.887389\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.887389183044\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:47 INFO 140607820867392] Epoch[20] Batch [5]#011Speed: 1021.44 samples/sec#011loss=0.887389\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] Epoch[20] Batch[10] avg_epoch_loss=0.885001\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=0.882135689259\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] Epoch[20] Batch [10]#011Speed: 533.25 samples/sec#011loss=0.882136\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1499.3791580200195, \"sum\": 1499.3791580200195, \"min\": 1499.3791580200195}}, \"EndTime\": 1591925928.202916, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925926.703116}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=430.145080544 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.885001231324\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_f54813f2-e8d8-4a47-b59a-3dfd135d5530-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.64592933654785, \"sum\": 19.64592933654785, \"min\": 19.64592933654785}}, \"EndTime\": 1591925928.223116, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925928.202996}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] Epoch[21] Batch[0] avg_epoch_loss=0.877205\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:48 INFO 140607820867392] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.877204835415\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] Epoch[21] Batch[5] avg_epoch_loss=0.905895\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.905894577503\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] Epoch[21] Batch [5]#011Speed: 1076.81 samples/sec#011loss=0.905895\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] Epoch[21] Batch[10] avg_epoch_loss=0.908056\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=21, batch=10 train loss <loss>=0.910649859905\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] Epoch[21] Batch [10]#011Speed: 533.40 samples/sec#011loss=0.910650\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.0349941253662, \"sum\": 1481.0349941253662, \"min\": 1481.0349941253662}}, \"EndTime\": 1591925929.704275, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925928.223175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=447.625110706 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.908056069504\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:49 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:50 INFO 140607820867392] Epoch[22] Batch[0] avg_epoch_loss=0.875809\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:50 INFO 140607820867392] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.875808894634\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:50 INFO 140607820867392] Epoch[22] Batch[5] avg_epoch_loss=0.883509\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:50 INFO 140607820867392] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.883509159088\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:50 INFO 140607820867392] Epoch[22] Batch [5]#011Speed: 1062.42 samples/sec#011loss=0.883509\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] Epoch[22] Batch[10] avg_epoch_loss=0.885730\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=0.88839546442\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] Epoch[22] Batch [10]#011Speed: 504.68 samples/sec#011loss=0.888395\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1544.461965560913, \"sum\": 1544.461965560913, \"min\": 1544.461965560913}}, \"EndTime\": 1591925931.249233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925929.704354}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=445.429979577 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.885730206966\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] Epoch[23] Batch[0] avg_epoch_loss=0.881608\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.881608009338\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] Epoch[23] Batch[5] avg_epoch_loss=0.890598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.890597651402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] Epoch[23] Batch [5]#011Speed: 783.94 samples/sec#011loss=0.890598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] Epoch[23] Batch[10] avg_epoch_loss=0.911688\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.936995911598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] Epoch[23] Batch [10]#011Speed: 541.53 samples/sec#011loss=0.936996\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1606.0941219329834, \"sum\": 1606.0941219329834, \"min\": 1606.0941219329834}}, \"EndTime\": 1591925932.855823, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925931.249312}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=417.753905638 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.911687769673\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:52 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:53 INFO 140607820867392] Epoch[24] Batch[0] avg_epoch_loss=0.898844\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:53 INFO 140607820867392] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.898843765259\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:53 INFO 140607820867392] Epoch[24] Batch[5] avg_epoch_loss=0.907290\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:53 INFO 140607820867392] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.907289922237\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:53 INFO 140607820867392] Epoch[24] Batch [5]#011Speed: 1096.28 samples/sec#011loss=0.907290\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] Epoch[24] Batch[10] avg_epoch_loss=0.898607\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.888186919689\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] Epoch[24] Batch [10]#011Speed: 541.77 samples/sec#011loss=0.888187\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1487.8861904144287, \"sum\": 1487.8861904144287, \"min\": 1487.8861904144287}}, \"EndTime\": 1591925934.344199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925932.855902}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=438.171435859 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.898606739261\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] Epoch[25] Batch[0] avg_epoch_loss=0.842480\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.842479586601\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] Epoch[25] Batch[5] avg_epoch_loss=0.868864\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.86886370182\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] Epoch[25] Batch [5]#011Speed: 1073.97 samples/sec#011loss=0.868864\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.914981842041, \"sum\": 1422.914981842041, \"min\": 1422.914981842041}}, \"EndTime\": 1591925935.767615, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925934.34428}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=447.633863179 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.876238065958\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:55 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_c3682fea-9bff-481d-8865-99a83dafc25c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.110048294067383, \"sum\": 31.110048294067383, \"min\": 31.110048294067383}}, \"EndTime\": 1591925935.799356, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925935.767699}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:56 INFO 140607820867392] Epoch[26] Batch[0] avg_epoch_loss=0.894261\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:56 INFO 140607820867392] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.894261300564\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:56 INFO 140607820867392] Epoch[26] Batch[5] avg_epoch_loss=0.865705\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:56 INFO 140607820867392] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.865705182155\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:56 INFO 140607820867392] Epoch[26] Batch [5]#011Speed: 1086.97 samples/sec#011loss=0.865705\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] Epoch[26] Batch[10] avg_epoch_loss=0.889807\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] #quality_metric: host=algo-1, epoch=26, batch=10 train loss <loss>=0.91872831583\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] Epoch[26] Batch [10]#011Speed: 550.69 samples/sec#011loss=0.918728\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1451.4050483703613, \"sum\": 1451.4050483703613, \"min\": 1451.4050483703613}}, \"EndTime\": 1591925937.250895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925935.799428}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=451.943460571 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.889806606553\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] Epoch[27] Batch[0] avg_epoch_loss=0.850240\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:57 INFO 140607820867392] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.850240468979\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] Epoch[27] Batch[5] avg_epoch_loss=0.860920\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.860920439164\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] Epoch[27] Batch [5]#011Speed: 1061.81 samples/sec#011loss=0.860920\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1381.2360763549805, \"sum\": 1381.2360763549805, \"min\": 1381.2360763549805}}, \"EndTime\": 1591925938.632642, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925937.250962}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=448.113964957 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.868084400892\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:58 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_d24521b7-2ccb-4ab5-931c-3981225229b7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.51091194152832, \"sum\": 20.51091194152832, \"min\": 20.51091194152832}}, \"EndTime\": 1591925938.653782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925938.632718}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:59 INFO 140607820867392] Epoch[28] Batch[0] avg_epoch_loss=0.857677\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:59 INFO 140607820867392] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.857677102089\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:59 INFO 140607820867392] Epoch[28] Batch[5] avg_epoch_loss=0.827671\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:59 INFO 140607820867392] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.827670911948\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:38:59 INFO 140607820867392] Epoch[28] Batch [5]#011Speed: 934.39 samples/sec#011loss=0.827671\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] Epoch[28] Batch[10] avg_epoch_loss=0.830513\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=0.833923137188\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] Epoch[28] Batch [10]#011Speed: 536.09 samples/sec#011loss=0.833923\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1513.8812065124512, \"sum\": 1513.8812065124512, \"min\": 1513.8812065124512}}, \"EndTime\": 1591925940.167796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925938.653852}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=445.178914322 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.830512832512\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_6fec5549-01af-4f28-9a61-a0b04b3d47ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.411014556884766, \"sum\": 20.411014556884766, \"min\": 20.411014556884766}}, \"EndTime\": 1591925940.188751, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925940.167876}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] Epoch[29] Batch[0] avg_epoch_loss=0.946090\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.946090340614\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] Epoch[29] Batch[5] avg_epoch_loss=0.866763\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.866763194402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] Epoch[29] Batch [5]#011Speed: 1050.15 samples/sec#011loss=0.866763\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] Epoch[29] Batch[10] avg_epoch_loss=0.839491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.806763422489\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] Epoch[29] Batch [10]#011Speed: 540.41 samples/sec#011loss=0.806763\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1495.7759380340576, \"sum\": 1495.7759380340576, \"min\": 1495.7759380340576}}, \"EndTime\": 1591925941.68467, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925940.18883}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=439.204286121 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.839490570805\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:01 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:02 INFO 140607820867392] Epoch[30] Batch[0] avg_epoch_loss=0.861135\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.861134588718\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:02 INFO 140607820867392] Epoch[30] Batch[5] avg_epoch_loss=0.861964\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.861963897943\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:02 INFO 140607820867392] Epoch[30] Batch [5]#011Speed: 999.69 samples/sec#011loss=0.861964\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1438.5240077972412, \"sum\": 1438.5240077972412, \"min\": 1438.5240077972412}}, \"EndTime\": 1591925943.123664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925941.684745}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=444.154367558 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.840910762548\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] Epoch[31] Batch[0] avg_epoch_loss=0.773898\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:03 INFO 140607820867392] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.773898005486\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] Epoch[31] Batch[5] avg_epoch_loss=0.792520\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.792520344257\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] Epoch[31] Batch [5]#011Speed: 1087.39 samples/sec#011loss=0.792520\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1428.8651943206787, \"sum\": 1428.8651943206787, \"min\": 1428.8651943206787}}, \"EndTime\": 1591925944.553162, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925943.123792}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=444.370724402 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.779789978266\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:04 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_367a617c-2c5d-4799-9a96-9fc267769689-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.701087951660156, \"sum\": 31.701087951660156, \"min\": 31.701087951660156}}, \"EndTime\": 1591925944.585459, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925944.553246}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:05 INFO 140607820867392] Epoch[32] Batch[0] avg_epoch_loss=0.841314\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:05 INFO 140607820867392] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.841313719749\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:05 INFO 140607820867392] Epoch[32] Batch[5] avg_epoch_loss=0.791561\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:05 INFO 140607820867392] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.791561265786\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:05 INFO 140607820867392] Epoch[32] Batch [5]#011Speed: 1035.93 samples/sec#011loss=0.791561\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1432.2609901428223, \"sum\": 1432.2609901428223, \"min\": 1432.2609901428223}}, \"EndTime\": 1591925946.017861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925944.585534}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=425.168003814 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.790506333113\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] Epoch[33] Batch[0] avg_epoch_loss=0.761781\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.761780679226\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] Epoch[33] Batch[5] avg_epoch_loss=0.759719\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.759718517462\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:06 INFO 140607820867392] Epoch[33] Batch [5]#011Speed: 1084.31 samples/sec#011loss=0.759719\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1377.2609233856201, \"sum\": 1377.2609233856201, \"min\": 1377.2609233856201}}, \"EndTime\": 1591925947.395696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925946.017935}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=456.656469076 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.743068218231\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_079cb932-329e-4230-8404-bec5e37cbac1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.341873168945312, \"sum\": 20.341873168945312, \"min\": 20.341873168945312}}, \"EndTime\": 1591925947.416657, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925947.3958}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] Epoch[34] Batch[0] avg_epoch_loss=0.696097\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:07 INFO 140607820867392] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.696097493172\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] Epoch[34] Batch[5] avg_epoch_loss=0.760272\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.76027238369\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] Epoch[34] Batch [5]#011Speed: 1080.03 samples/sec#011loss=0.760272\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1377.1979808807373, \"sum\": 1377.1979808807373, \"min\": 1377.1979808807373}}, \"EndTime\": 1591925948.793961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925947.416715}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=447.976911897 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.735593020916\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:08 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_74bae903-61a0-4755-bc0c-1f11ee5110d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.328998565673828, \"sum\": 20.328998565673828, \"min\": 20.328998565673828}}, \"EndTime\": 1591925948.814901, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925948.794037}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:09 INFO 140607820867392] Epoch[35] Batch[0] avg_epoch_loss=0.676235\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:09 INFO 140607820867392] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.676235496998\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:09 INFO 140607820867392] Epoch[35] Batch[5] avg_epoch_loss=0.710041\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:09 INFO 140607820867392] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.710040599108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:09 INFO 140607820867392] Epoch[35] Batch [5]#011Speed: 999.65 samples/sec#011loss=0.710041\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] Epoch[35] Batch[10] avg_epoch_loss=0.731466\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.757176983356\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] Epoch[35] Batch [10]#011Speed: 548.54 samples/sec#011loss=0.757177\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.827974319458, \"sum\": 1481.827974319458, \"min\": 1481.827974319458}}, \"EndTime\": 1591925950.296867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925948.814975}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=436.588672213 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.731466228312\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_4b0c190d-fd2d-4e28-9e66-2f379493a04f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.087875366210938, \"sum\": 31.087875366210938, \"min\": 31.087875366210938}}, \"EndTime\": 1591925950.328504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925950.296945}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] Epoch[36] Batch[0] avg_epoch_loss=0.704108\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:10 INFO 140607820867392] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.70410823822\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] Epoch[36] Batch[5] avg_epoch_loss=0.676580\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.676580299934\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] Epoch[36] Batch [5]#011Speed: 1064.17 samples/sec#011loss=0.676580\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1418.4138774871826, \"sum\": 1418.4138774871826, \"min\": 1418.4138774871826}}, \"EndTime\": 1591925951.747051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925950.328574}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=443.419910638 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.667439705133\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:11 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_f4c03cac-349a-4493-bd21-2dccaa4396d7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.313024520874023, \"sum\": 20.313024520874023, \"min\": 20.313024520874023}}, \"EndTime\": 1591925951.76796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925951.747122}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:12 INFO 140607820867392] Epoch[37] Batch[0] avg_epoch_loss=0.739575\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:12 INFO 140607820867392] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.739574790001\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:12 INFO 140607820867392] Epoch[37] Batch[5] avg_epoch_loss=0.670307\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:12 INFO 140607820867392] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.670306682587\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:12 INFO 140607820867392] Epoch[37] Batch [5]#011Speed: 1077.42 samples/sec#011loss=0.670307\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1385.836124420166, \"sum\": 1385.836124420166, \"min\": 1385.836124420166}}, \"EndTime\": 1591925953.153927, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925951.768033}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=451.677737237 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.655140334368\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_4848bdff-18a1-4748-900a-3c59d87f381a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.60389518737793, \"sum\": 20.60389518737793, \"min\": 20.60389518737793}}, \"EndTime\": 1591925953.175127, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925953.154003}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] Epoch[38] Batch[0] avg_epoch_loss=0.678348\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:13 INFO 140607820867392] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.678347766399\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] Epoch[38] Batch[5] avg_epoch_loss=0.630393\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.630393306414\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] Epoch[38] Batch [5]#011Speed: 1066.00 samples/sec#011loss=0.630393\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] Epoch[38] Batch[10] avg_epoch_loss=0.639077\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=38, batch=10 train loss <loss>=0.649497938156\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] Epoch[38] Batch [10]#011Speed: 514.83 samples/sec#011loss=0.649498\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] processed a total of 696 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.8160037994385, \"sum\": 1508.8160037994385, \"min\": 1508.8160037994385}}, \"EndTime\": 1591925954.684056, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925953.175186}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=461.255177089 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.639077229933\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:14 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_82f9c471-7fe2-4806-9535-f4fc97206248-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.129940032958984, \"sum\": 23.129940032958984, \"min\": 23.129940032958984}}, \"EndTime\": 1591925954.707705, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925954.684132}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:15 INFO 140607820867392] Epoch[39] Batch[0] avg_epoch_loss=0.574923\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:15 INFO 140607820867392] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.574923157692\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:15 INFO 140607820867392] Epoch[39] Batch[5] avg_epoch_loss=0.591197\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:15 INFO 140607820867392] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.591196676095\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:15 INFO 140607820867392] Epoch[39] Batch [5]#011Speed: 1061.36 samples/sec#011loss=0.591197\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] Epoch[39] Batch[10] avg_epoch_loss=0.564159\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] #quality_metric: host=algo-1, epoch=39, batch=10 train loss <loss>=0.531714791059\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] Epoch[39] Batch [10]#011Speed: 513.78 samples/sec#011loss=0.531715\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1515.6991481781006, \"sum\": 1515.6991481781006, \"min\": 1515.6991481781006}}, \"EndTime\": 1591925956.223544, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925954.707788}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=431.446515303 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.564159455624\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_78837cc7-eccd-48af-b316-bf3298280752-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 25.08091926574707, \"sum\": 25.08091926574707, \"min\": 25.08091926574707}}, \"EndTime\": 1591925956.249209, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925956.22364}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] Epoch[40] Batch[0] avg_epoch_loss=0.615049\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:16 INFO 140607820867392] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.615049302578\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] Epoch[40] Batch[5] avg_epoch_loss=0.606860\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.606860488653\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] Epoch[40] Batch [5]#011Speed: 1093.73 samples/sec#011loss=0.606860\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1414.6788120269775, \"sum\": 1414.6788120269775, \"min\": 1414.6788120269775}}, \"EndTime\": 1591925957.664024, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925956.249279}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=442.465388439 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.600312829018\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:17 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:18 INFO 140607820867392] Epoch[41] Batch[0] avg_epoch_loss=0.675327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.675327479839\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:18 INFO 140607820867392] Epoch[41] Batch[5] avg_epoch_loss=0.612162\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.612161715825\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:18 INFO 140607820867392] Epoch[41] Batch [5]#011Speed: 1056.70 samples/sec#011loss=0.612162\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1395.3289985656738, \"sum\": 1395.3289985656738, \"min\": 1395.3289985656738}}, \"EndTime\": 1591925959.059938, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925957.664107}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=451.46823411 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.605769312382\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] Epoch[42] Batch[0] avg_epoch_loss=0.588026\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.588025689125\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] Epoch[42] Batch[5] avg_epoch_loss=0.589593\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.589593132337\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:19 INFO 140607820867392] Epoch[42] Batch [5]#011Speed: 1023.41 samples/sec#011loss=0.589593\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] Epoch[42] Batch[10] avg_epoch_loss=0.589141\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=0.588597655296\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] Epoch[42] Batch [10]#011Speed: 558.08 samples/sec#011loss=0.588598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1487.591028213501, \"sum\": 1487.591028213501, \"min\": 1487.591028213501}}, \"EndTime\": 1591925960.548025, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925959.060019}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=461.110707723 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.589140642773\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:20 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:21 INFO 140607820867392] Epoch[43] Batch[0] avg_epoch_loss=0.561998\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.561998426914\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:21 INFO 140607820867392] Epoch[43] Batch[5] avg_epoch_loss=0.563887\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.563887407382\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:21 INFO 140607820867392] Epoch[43] Batch [5]#011Speed: 1047.67 samples/sec#011loss=0.563887\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] Epoch[43] Batch[10] avg_epoch_loss=0.566333\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.569267433882\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] Epoch[43] Batch [10]#011Speed: 537.34 samples/sec#011loss=0.569267\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1473.1800556182861, \"sum\": 1473.1800556182861, \"min\": 1473.1800556182861}}, \"EndTime\": 1591925962.021703, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925960.548107}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=458.833916447 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.566332873973\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] Epoch[44] Batch[0] avg_epoch_loss=0.658989\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.658988595009\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] Epoch[44] Batch[5] avg_epoch_loss=0.576381\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.576380774379\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:22 INFO 140607820867392] Epoch[44] Batch [5]#011Speed: 1063.81 samples/sec#011loss=0.576381\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1446.9609260559082, \"sum\": 1446.9609260559082, \"min\": 1446.9609260559082}}, \"EndTime\": 1591925963.469179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925962.021786}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=441.579808224 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.558425787091\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:23 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_decc9b07-7890-4dd8-a56f-30863ef4cd25-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.623031616210938, \"sum\": 29.623031616210938, \"min\": 29.623031616210938}}, \"EndTime\": 1591925963.49937, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925963.469258}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] Epoch[45] Batch[0] avg_epoch_loss=0.532726\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.532725811005\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] Epoch[45] Batch[5] avg_epoch_loss=0.522170\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.522169748942\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] Epoch[45] Batch [5]#011Speed: 1089.84 samples/sec#011loss=0.522170\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1382.1780681610107, \"sum\": 1382.1780681610107, \"min\": 1382.1780681610107}}, \"EndTime\": 1591925964.881692, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925963.499443}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=449.975022862 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.509985071421\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:24 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_5e1d2321-26b9-463e-a6ec-b1600bec1f0d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.364999771118164, \"sum\": 20.364999771118164, \"min\": 20.364999771118164}}, \"EndTime\": 1591925964.902681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925964.881772}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:25 INFO 140607820867392] Epoch[46] Batch[0] avg_epoch_loss=0.453659\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.45365908742\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:25 INFO 140607820867392] Epoch[46] Batch[5] avg_epoch_loss=0.559386\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.559386307995\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:25 INFO 140607820867392] Epoch[46] Batch [5]#011Speed: 1078.48 samples/sec#011loss=0.559386\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] Epoch[46] Batch[10] avg_epoch_loss=0.482084\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.389321351051\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] Epoch[46] Batch [10]#011Speed: 535.66 samples/sec#011loss=0.389321\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1531.076192855835, \"sum\": 1531.076192855835, \"min\": 1531.076192855835}}, \"EndTime\": 1591925966.433878, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925964.902746}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=421.245937335 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.482084054839\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:26 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_5d6197a5-fb41-4f92-a42b-bcdc250ddb14-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 24.196147918701172, \"sum\": 24.196147918701172, \"min\": 24.196147918701172}}, \"EndTime\": 1591925966.458555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925966.433943}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] Epoch[47] Batch[0] avg_epoch_loss=0.582614\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.582614421844\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] Epoch[47] Batch[5] avg_epoch_loss=0.523605\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.523604561885\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] Epoch[47] Batch [5]#011Speed: 1074.45 samples/sec#011loss=0.523605\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1382.4567794799805, \"sum\": 1382.4567794799805, \"min\": 1382.4567794799805}}, \"EndTime\": 1591925967.841203, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925966.45868}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=458.569921431 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.566042929888\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:27 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:28 INFO 140607820867392] Epoch[48] Batch[0] avg_epoch_loss=0.504417\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.504416704178\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:28 INFO 140607820867392] Epoch[48] Batch[5] avg_epoch_loss=0.526177\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.526177083453\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:28 INFO 140607820867392] Epoch[48] Batch [5]#011Speed: 1026.94 samples/sec#011loss=0.526177\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1405.038833618164, \"sum\": 1405.038833618164, \"min\": 1405.038833618164}}, \"EndTime\": 1591925969.24679, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925967.841268}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=439.807581968 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.504749292135\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] Epoch[49] Batch[0] avg_epoch_loss=0.427795\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.427794992924\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] Epoch[49] Batch[5] avg_epoch_loss=0.499042\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.499042371909\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] Epoch[49] Batch [5]#011Speed: 1069.74 samples/sec#011loss=0.499042\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] Epoch[49] Batch[10] avg_epoch_loss=0.544913\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.599958348274\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] Epoch[49] Batch [10]#011Speed: 530.23 samples/sec#011loss=0.599958\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1502.7899742126465, \"sum\": 1502.7899742126465, \"min\": 1502.7899742126465}}, \"EndTime\": 1591925970.750115, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925969.246873}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=448.473162706 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.544913270257\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:30 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:31 INFO 140607820867392] Epoch[50] Batch[0] avg_epoch_loss=0.543717\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.543717324734\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:31 INFO 140607820867392] Epoch[50] Batch[5] avg_epoch_loss=0.446105\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.446104700367\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:31 INFO 140607820867392] Epoch[50] Batch [5]#011Speed: 1003.00 samples/sec#011loss=0.446105\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1447.610855102539, \"sum\": 1447.610855102539, \"min\": 1447.610855102539}}, \"EndTime\": 1591925972.198217, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925970.750172}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=441.380987992 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.465177357197\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_5eb9dec6-188b-4e53-a2f7-d1ee47aa7541-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.262096405029297, \"sum\": 22.262096405029297, \"min\": 22.262096405029297}}, \"EndTime\": 1591925972.221035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925972.198297}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] Epoch[51] Batch[0] avg_epoch_loss=0.496605\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.496604949236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] Epoch[51] Batch[5] avg_epoch_loss=0.416693\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.416693016887\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] Epoch[51] Batch [5]#011Speed: 1090.83 samples/sec#011loss=0.416693\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] processed a total of 616 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1417.2990322113037, \"sum\": 1417.2990322113037, \"min\": 1417.2990322113037}}, \"EndTime\": 1591925973.638478, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925972.221112}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=434.591871152 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.477361020446\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:33 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:34 INFO 140607820867392] Epoch[52] Batch[0] avg_epoch_loss=0.335236\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.335236012936\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:34 INFO 140607820867392] Epoch[52] Batch[5] avg_epoch_loss=0.443556\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.443555911382\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:34 INFO 140607820867392] Epoch[52] Batch [5]#011Speed: 1081.01 samples/sec#011loss=0.443556\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] Epoch[52] Batch[10] avg_epoch_loss=0.427441\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.408103203773\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] Epoch[52] Batch [10]#011Speed: 491.17 samples/sec#011loss=0.408103\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1531.1357975006104, \"sum\": 1531.1357975006104, \"min\": 1531.1357975006104}}, \"EndTime\": 1591925975.170172, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925973.638562}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=429.713415921 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.427441044287\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_d67e9747-9f11-4590-a150-6cff8428d65c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.444072723388672, \"sum\": 31.444072723388672, \"min\": 31.444072723388672}}, \"EndTime\": 1591925975.202163, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925975.170253}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] Epoch[53] Batch[0] avg_epoch_loss=0.550946\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.550946116447\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] Epoch[53] Batch[5] avg_epoch_loss=0.449808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.449807758133\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] Epoch[53] Batch [5]#011Speed: 1070.44 samples/sec#011loss=0.449808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] Epoch[53] Batch[10] avg_epoch_loss=0.447647\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=0.445053437352\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] Epoch[53] Batch [10]#011Speed: 547.71 samples/sec#011loss=0.445053\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1485.7051372528076, \"sum\": 1485.7051372528076, \"min\": 1485.7051372528076}}, \"EndTime\": 1591925976.688008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925975.202239}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=450.929146973 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.447646703232\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:36 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:37 INFO 140607820867392] Epoch[54] Batch[0] avg_epoch_loss=0.368548\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.368548214436\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:37 INFO 140607820867392] Epoch[54] Batch[5] avg_epoch_loss=0.444207\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.444206570586\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:37 INFO 140607820867392] Epoch[54] Batch [5]#011Speed: 1058.11 samples/sec#011loss=0.444207\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1445.085048675537, \"sum\": 1445.085048675537, \"min\": 1445.085048675537}}, \"EndTime\": 1591925978.133617, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925976.688086}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=435.923388415 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.422793975472\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_0cf17c89-58e7-4814-9c15-9352a9cf02d3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.355960845947266, \"sum\": 23.355960845947266, \"min\": 23.355960845947266}}, \"EndTime\": 1591925978.157557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925978.133702}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] Epoch[55] Batch[0] avg_epoch_loss=0.474562\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.474561840296\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] Epoch[55] Batch[5] avg_epoch_loss=0.465676\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.465676238139\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] Epoch[55] Batch [5]#011Speed: 1102.25 samples/sec#011loss=0.465676\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1391.3209438323975, \"sum\": 1391.3209438323975, \"min\": 1391.3209438323975}}, \"EndTime\": 1591925979.549004, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925978.157622}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=433.362389962 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.530934303999\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:39 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:40 INFO 140607820867392] Epoch[56] Batch[0] avg_epoch_loss=0.311221\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.311220705509\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:40 INFO 140607820867392] Epoch[56] Batch[5] avg_epoch_loss=0.437056\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.43705628564\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:40 INFO 140607820867392] Epoch[56] Batch [5]#011Speed: 997.15 samples/sec#011loss=0.437056\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1493.1230545043945, \"sum\": 1493.1230545043945, \"min\": 1493.1230545043945}}, \"EndTime\": 1591925981.042648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925979.54909}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=404.487919774 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.414072389901\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_b0907e4b-8e61-42b2-8b08-532a29267fca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 26.050090789794922, \"sum\": 26.050090789794922, \"min\": 26.050090789794922}}, \"EndTime\": 1591925981.069295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925981.042734}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] Epoch[57] Batch[0] avg_epoch_loss=0.390405\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.390405356884\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] Epoch[57] Batch[5] avg_epoch_loss=0.434598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.43459816277\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:41 INFO 140607820867392] Epoch[57] Batch [5]#011Speed: 1012.38 samples/sec#011loss=0.434598\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] Epoch[57] Batch[10] avg_epoch_loss=0.330039\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.204567325115\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] Epoch[57] Batch [10]#011Speed: 536.00 samples/sec#011loss=0.204567\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1506.4117908477783, \"sum\": 1506.4117908477783, \"min\": 1506.4117908477783}}, \"EndTime\": 1591925982.575836, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925981.069368}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=426.145713005 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.330038691109\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:42 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_a99aca01-c13e-4d06-a5b5-e890b5e1428e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.64800262451172, \"sum\": 20.64800262451172, \"min\": 20.64800262451172}}, \"EndTime\": 1591925982.597057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925982.575916}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:43 INFO 140607820867392] Epoch[58] Batch[0] avg_epoch_loss=0.486494\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.486494332552\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:43 INFO 140607820867392] Epoch[58] Batch[5] avg_epoch_loss=0.573846\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.573845639825\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:43 INFO 140607820867392] Epoch[58] Batch [5]#011Speed: 551.36 samples/sec#011loss=0.573846\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1405.717134475708, \"sum\": 1405.717134475708, \"min\": 1405.717134475708}}, \"EndTime\": 1591925984.002917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925982.597131}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=418.966620312 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.543142038584\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] Epoch[59] Batch[0] avg_epoch_loss=0.472987\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.472987383604\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] Epoch[59] Batch[5] avg_epoch_loss=0.447118\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.447118028998\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] Epoch[59] Batch [5]#011Speed: 477.27 samples/sec#011loss=0.447118\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1527.109146118164, \"sum\": 1527.109146118164, \"min\": 1527.109146118164}}, \"EndTime\": 1591925985.530566, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925984.003001}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=401.379205093 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.467519941926\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:45 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:46 INFO 140607820867392] Epoch[60] Batch[0] avg_epoch_loss=0.265827\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:46 INFO 140607820867392] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.265826642513\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:46 INFO 140607820867392] Epoch[60] Batch[5] avg_epoch_loss=0.399808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:46 INFO 140607820867392] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.399808352192\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:46 INFO 140607820867392] Epoch[60] Batch [5]#011Speed: 1073.39 samples/sec#011loss=0.399808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] Epoch[60] Batch[10] avg_epoch_loss=0.353438\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=0.297793617845\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] Epoch[60] Batch [10]#011Speed: 533.08 samples/sec#011loss=0.297794\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1491.192102432251, \"sum\": 1491.192102432251, \"min\": 1491.192102432251}}, \"EndTime\": 1591925987.022297, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925985.530651}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=437.871215482 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.353438018398\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] Epoch[61] Batch[0] avg_epoch_loss=0.474056\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.47405615449\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] Epoch[61] Batch[5] avg_epoch_loss=0.417487\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.417487372955\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:47 INFO 140607820867392] Epoch[61] Batch [5]#011Speed: 1060.38 samples/sec#011loss=0.417487\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] Epoch[61] Batch[10] avg_epoch_loss=0.413322\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.40832297802\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] Epoch[61] Batch [10]#011Speed: 541.24 samples/sec#011loss=0.408323\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1498.8229274749756, \"sum\": 1498.8229274749756, \"min\": 1498.8229274749756}}, \"EndTime\": 1591925988.521619, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925987.022376}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=465.001109126 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.413321738893\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:48 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] Epoch[62] Batch[0] avg_epoch_loss=0.479676\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.479675531387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] Epoch[62] Batch[5] avg_epoch_loss=0.356990\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.356990439196\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] Epoch[62] Batch [5]#011Speed: 1067.05 samples/sec#011loss=0.356990\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1427.5569915771484, \"sum\": 1427.5569915771484, \"min\": 1427.5569915771484}}, \"EndTime\": 1591925989.949527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925988.521687}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=434.979044656 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.380492450297\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:49 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:50 INFO 140607820867392] Epoch[63] Batch[0] avg_epoch_loss=0.359836\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:50 INFO 140607820867392] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.359835833311\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:50 INFO 140607820867392] Epoch[63] Batch[5] avg_epoch_loss=0.379064\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:50 INFO 140607820867392] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.379064296683\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:50 INFO 140607820867392] Epoch[63] Batch [5]#011Speed: 1061.87 samples/sec#011loss=0.379064\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] Epoch[63] Batch[10] avg_epoch_loss=0.368946\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.356802967191\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] Epoch[63] Batch [10]#011Speed: 556.52 samples/sec#011loss=0.356803\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1458.7430953979492, \"sum\": 1458.7430953979492, \"min\": 1458.7430953979492}}, \"EndTime\": 1591925991.408782, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925989.949589}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=459.265213882 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.36894551055\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] Epoch[64] Batch[0] avg_epoch_loss=0.367604\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:51 INFO 140607820867392] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.367603510618\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] Epoch[64] Batch[5] avg_epoch_loss=0.376900\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.376899535457\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] Epoch[64] Batch [5]#011Speed: 939.26 samples/sec#011loss=0.376900\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1491.245985031128, \"sum\": 1491.245985031128, \"min\": 1491.245985031128}}, \"EndTime\": 1591925992.900557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925991.408857}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=419.07884314 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.352469174564\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:52 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:53 INFO 140607820867392] Epoch[65] Batch[0] avg_epoch_loss=0.393551\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:53 INFO 140607820867392] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.393550604582\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:53 INFO 140607820867392] Epoch[65] Batch[5] avg_epoch_loss=0.323284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:53 INFO 140607820867392] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.323283722003\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:53 INFO 140607820867392] Epoch[65] Batch [5]#011Speed: 1082.99 samples/sec#011loss=0.323284\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] Epoch[65] Batch[10] avg_epoch_loss=0.435470\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.570093810558\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] Epoch[65] Batch [10]#011Speed: 552.59 samples/sec#011loss=0.570094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1447.4809169769287, \"sum\": 1447.4809169769287, \"min\": 1447.4809169769287}}, \"EndTime\": 1591925994.348559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925992.900637}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=456.625105368 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.435470125892\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] Epoch[66] Batch[0] avg_epoch_loss=0.449620\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:54 INFO 140607820867392] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.449620485306\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] Epoch[66] Batch[5] avg_epoch_loss=0.335312\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.335311884681\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] Epoch[66] Batch [5]#011Speed: 1022.55 samples/sec#011loss=0.335312\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1406.3711166381836, \"sum\": 1406.3711166381836, \"min\": 1406.3711166381836}}, \"EndTime\": 1591925995.755443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925994.348622}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=452.899840404 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.334571635723\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:55 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:56 INFO 140607820867392] Epoch[67] Batch[0] avg_epoch_loss=0.508651\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:56 INFO 140607820867392] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.508651196957\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:56 INFO 140607820867392] Epoch[67] Batch[5] avg_epoch_loss=0.376050\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:56 INFO 140607820867392] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.37604974707\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:56 INFO 140607820867392] Epoch[67] Batch [5]#011Speed: 1065.16 samples/sec#011loss=0.376050\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] processed a total of 594 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1413.4759902954102, \"sum\": 1413.4759902954102, \"min\": 1413.4759902954102}}, \"EndTime\": 1591925997.169507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925995.755526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=420.208493134 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.347617320716\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] Epoch[68] Batch[0] avg_epoch_loss=0.488184\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:57 INFO 140607820867392] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.48818436265\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] Epoch[68] Batch[5] avg_epoch_loss=0.429723\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.42972295483\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] Epoch[68] Batch [5]#011Speed: 1070.79 samples/sec#011loss=0.429723\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] Epoch[68] Batch[10] avg_epoch_loss=0.313805\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=0.174703150988\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] Epoch[68] Batch [10]#011Speed: 558.89 samples/sec#011loss=0.174703\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1444.6330070495605, \"sum\": 1444.6330070495605, \"min\": 1444.6330070495605}}, \"EndTime\": 1591925998.61463, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925997.169578}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=443.673890353 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.313804862174\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:58 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_8e8a97dd-8353-41bf-b18d-fdfe81c76ca4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.39194107055664, \"sum\": 20.39194107055664, \"min\": 20.39194107055664}}, \"EndTime\": 1591925998.63555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925998.614714}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:59 INFO 140607820867392] Epoch[69] Batch[0] avg_epoch_loss=0.444339\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:59 INFO 140607820867392] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.444338530302\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:59 INFO 140607820867392] Epoch[69] Batch[5] avg_epoch_loss=0.388977\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:59 INFO 140607820867392] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.388976722956\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:39:59 INFO 140607820867392] Epoch[69] Batch [5]#011Speed: 1076.90 samples/sec#011loss=0.388977\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1410.1848602294922, \"sum\": 1410.1848602294922, \"min\": 1410.1848602294922}}, \"EndTime\": 1591926000.045841, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591925998.635599}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=438.905613477 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.395437753201\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] Epoch[70] Batch[0] avg_epoch_loss=0.265395\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.265394747257\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] Epoch[70] Batch[5] avg_epoch_loss=0.340387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.340386810402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:00 INFO 140607820867392] Epoch[70] Batch [5]#011Speed: 1034.40 samples/sec#011loss=0.340387\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] Epoch[70] Batch[10] avg_epoch_loss=0.337394\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.333802318573\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] Epoch[70] Batch [10]#011Speed: 548.40 samples/sec#011loss=0.333802\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1492.8030967712402, \"sum\": 1492.8030967712402, \"min\": 1492.8030967712402}}, \"EndTime\": 1591926001.539266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926000.045948}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=452.133157813 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.337393859571\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:01 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] Epoch[71] Batch[0] avg_epoch_loss=0.375028\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.375027507544\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] Epoch[71] Batch[5] avg_epoch_loss=0.360567\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.360566864411\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] Epoch[71] Batch [5]#011Speed: 1082.36 samples/sec#011loss=0.360567\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] Epoch[71] Batch[10] avg_epoch_loss=0.369778\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.380830341578\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] Epoch[71] Batch [10]#011Speed: 550.70 samples/sec#011loss=0.380830\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1456.2079906463623, \"sum\": 1456.2079906463623, \"min\": 1456.2079906463623}}, \"EndTime\": 1591926002.996041, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926001.53935}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=456.628228119 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.36977753585\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:02 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:03 INFO 140607820867392] Epoch[72] Batch[0] avg_epoch_loss=0.386489\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:03 INFO 140607820867392] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.386488825083\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:03 INFO 140607820867392] Epoch[72] Batch[5] avg_epoch_loss=0.333491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:03 INFO 140607820867392] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.33349079142\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:03 INFO 140607820867392] Epoch[72] Batch [5]#011Speed: 1088.26 samples/sec#011loss=0.333491\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1381.1450004577637, \"sum\": 1381.1450004577637, \"min\": 1381.1450004577637}}, \"EndTime\": 1591926004.377734, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926002.996122}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=451.037485259 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.354007260501\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] Epoch[73] Batch[0] avg_epoch_loss=0.236702\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:04 INFO 140607820867392] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.236701846123\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] Epoch[73] Batch[5] avg_epoch_loss=0.249233\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.249233183761\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] Epoch[73] Batch [5]#011Speed: 1058.23 samples/sec#011loss=0.249233\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1390.733003616333, \"sum\": 1390.733003616333, \"min\": 1390.733003616333}}, \"EndTime\": 1591926005.76902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926004.377809}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=434.992340869 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.264898499101\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:05 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_72171edb-e427-4d2e-8202-ba0d2941ebd2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.22599220275879, \"sum\": 30.22599220275879, \"min\": 30.22599220275879}}, \"EndTime\": 1591926005.799825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926005.769083}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:06 INFO 140607820867392] Epoch[74] Batch[0] avg_epoch_loss=0.609027\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:06 INFO 140607820867392] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.609026670456\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:06 INFO 140607820867392] Epoch[74] Batch[5] avg_epoch_loss=0.349338\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:06 INFO 140607820867392] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.349338057141\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:06 INFO 140607820867392] Epoch[74] Batch [5]#011Speed: 1060.08 samples/sec#011loss=0.349338\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.6331825256348, \"sum\": 1435.6331825256348, \"min\": 1435.6331825256348}}, \"EndTime\": 1591926007.235581, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926005.799889}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=437.400527035 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.313803746551\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] Epoch[75] Batch[0] avg_epoch_loss=0.281056\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:07 INFO 140607820867392] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.281055986881\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] Epoch[75] Batch[5] avg_epoch_loss=0.361040\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.361039588849\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] Epoch[75] Batch [5]#011Speed: 981.17 samples/sec#011loss=0.361040\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1450.9880542755127, \"sum\": 1450.9880542755127, \"min\": 1450.9880542755127}}, \"EndTime\": 1591926008.687136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926007.235667}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=425.193809049 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.355550122261\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:08 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:09 INFO 140607820867392] Epoch[76] Batch[0] avg_epoch_loss=0.287694\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:09 INFO 140607820867392] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.287693500519\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:09 INFO 140607820867392] Epoch[76] Batch[5] avg_epoch_loss=0.285826\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:09 INFO 140607820867392] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.285825550556\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:09 INFO 140607820867392] Epoch[76] Batch [5]#011Speed: 1012.51 samples/sec#011loss=0.285826\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.38498878479, \"sum\": 1435.38498878479, \"min\": 1435.38498878479}}, \"EndTime\": 1591926010.123095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926008.687212}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=444.442923236 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.324596175551\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] Epoch[77] Batch[0] avg_epoch_loss=0.303792\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:10 INFO 140607820867392] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.303791582584\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] Epoch[77] Batch[5] avg_epoch_loss=0.279447\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.279446757088\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] Epoch[77] Batch [5]#011Speed: 1092.36 samples/sec#011loss=0.279447\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] Epoch[77] Batch[10] avg_epoch_loss=0.355423\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.446593558788\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] Epoch[77] Batch [10]#011Speed: 517.55 samples/sec#011loss=0.446594\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.5719470977783, \"sum\": 1520.5719470977783, \"min\": 1520.5719470977783}}, \"EndTime\": 1591926011.6442, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926010.123178}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=440.590377602 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.355422576043\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:11 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:12 INFO 140607820867392] Epoch[78] Batch[0] avg_epoch_loss=0.302574\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:12 INFO 140607820867392] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.302573919296\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:12 INFO 140607820867392] Epoch[78] Batch[5] avg_epoch_loss=0.297907\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:12 INFO 140607820867392] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.297906988611\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:12 INFO 140607820867392] Epoch[78] Batch [5]#011Speed: 1087.59 samples/sec#011loss=0.297907\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1480.219841003418, \"sum\": 1480.219841003418, \"min\": 1480.219841003418}}, \"EndTime\": 1591926013.124918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926011.644278}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=432.332763354 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.300173547119\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] Epoch[79] Batch[0] avg_epoch_loss=0.419486\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:13 INFO 140607820867392] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.419486045837\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] Epoch[79] Batch[5] avg_epoch_loss=0.407815\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.407815009356\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] Epoch[79] Batch [5]#011Speed: 1039.50 samples/sec#011loss=0.407815\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] Epoch[79] Batch[10] avg_epoch_loss=0.336911\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=0.251826816052\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] Epoch[79] Batch [10]#011Speed: 550.57 samples/sec#011loss=0.251827\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1513.2567882537842, \"sum\": 1513.2567882537842, \"min\": 1513.2567882537842}}, \"EndTime\": 1591926014.638712, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926013.125002}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=442.053007807 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.336911285127\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:14 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:15 INFO 140607820867392] Epoch[80] Batch[0] avg_epoch_loss=0.328909\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:15 INFO 140607820867392] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.328909218311\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:15 INFO 140607820867392] Epoch[80] Batch[5] avg_epoch_loss=0.375223\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:15 INFO 140607820867392] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.375223269065\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:15 INFO 140607820867392] Epoch[80] Batch [5]#011Speed: 1044.60 samples/sec#011loss=0.375223\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] processed a total of 626 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.0191345214844, \"sum\": 1486.0191345214844, \"min\": 1486.0191345214844}}, \"EndTime\": 1591926016.125276, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926014.638812}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=421.216938488 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.313206878304\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] Epoch[81] Batch[0] avg_epoch_loss=0.188169\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:16 INFO 140607820867392] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.188168898225\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] Epoch[81] Batch[5] avg_epoch_loss=0.255270\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.255270201713\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] Epoch[81] Batch [5]#011Speed: 946.70 samples/sec#011loss=0.255270\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] Epoch[81] Batch[10] avg_epoch_loss=0.195441\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.123647040129\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] Epoch[81] Batch [10]#011Speed: 524.98 samples/sec#011loss=0.123647\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1558.3109855651855, \"sum\": 1558.3109855651855, \"min\": 1558.3109855651855}}, \"EndTime\": 1591926017.684205, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926016.125389}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=423.504772582 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.195441491902\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:17 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/state_b24e084f-d9b0-4fe1-9cc1-b46f2149d47e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.242929458618164, \"sum\": 20.242929458618164, \"min\": 20.242929458618164}}, \"EndTime\": 1591926017.704971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926017.684282}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:18 INFO 140607820867392] Epoch[82] Batch[0] avg_epoch_loss=0.352126\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.352125883102\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:18 INFO 140607820867392] Epoch[82] Batch[5] avg_epoch_loss=0.323337\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:18 INFO 140607820867392] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.323336601257\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:18 INFO 140607820867392] Epoch[82] Batch [5]#011Speed: 1085.11 samples/sec#011loss=0.323337\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] Epoch[82] Batch[10] avg_epoch_loss=0.439293\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=0.578441403806\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] Epoch[82] Batch [10]#011Speed: 544.77 samples/sec#011loss=0.578441\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.584072113037, \"sum\": 1481.584072113037, \"min\": 1481.584072113037}}, \"EndTime\": 1591926019.186691, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926017.705039}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=433.960727977 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.439293329689\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] Epoch[83] Batch[0] avg_epoch_loss=0.341512\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:19 INFO 140607820867392] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.341512441635\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] Epoch[83] Batch[5] avg_epoch_loss=0.300908\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.300908190509\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] Epoch[83] Batch [5]#011Speed: 1033.60 samples/sec#011loss=0.300908\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1492.7971363067627, \"sum\": 1492.7971363067627, \"min\": 1492.7971363067627}}, \"EndTime\": 1591926020.680019, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926019.186769}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=418.640943654 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.310620991886\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:20 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:21 INFO 140607820867392] Epoch[84] Batch[0] avg_epoch_loss=0.203922\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.203922405839\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:21 INFO 140607820867392] Epoch[84] Batch[5] avg_epoch_loss=0.335832\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:21 INFO 140607820867392] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.335831875602\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:21 INFO 140607820867392] Epoch[84] Batch [5]#011Speed: 1041.87 samples/sec#011loss=0.335832\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] Epoch[84] Batch[10] avg_epoch_loss=0.359469\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=84, batch=10 train loss <loss>=0.38783415556\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] Epoch[84] Batch [10]#011Speed: 555.19 samples/sec#011loss=0.387834\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1481.3110828399658, \"sum\": 1481.3110828399658, \"min\": 1481.3110828399658}}, \"EndTime\": 1591926022.161911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926020.680106}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=434.719778109 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.359469275583\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] Epoch[85] Batch[0] avg_epoch_loss=0.294753\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:22 INFO 140607820867392] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.294752985239\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] Epoch[85] Batch[5] avg_epoch_loss=0.253170\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.25317038099\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] Epoch[85] Batch [5]#011Speed: 1085.53 samples/sec#011loss=0.253170\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] Epoch[85] Batch[10] avg_epoch_loss=0.305025\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.367249906063\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] Epoch[85] Batch [10]#011Speed: 549.84 samples/sec#011loss=0.367250\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1456.125020980835, \"sum\": 1456.125020980835, \"min\": 1456.125020980835}}, \"EndTime\": 1591926023.618554, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926022.16198}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=455.280252026 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.305024710569\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:23 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:24 INFO 140607820867392] Epoch[86] Batch[0] avg_epoch_loss=0.457661\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.45766094327\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:24 INFO 140607820867392] Epoch[86] Batch[5] avg_epoch_loss=0.285483\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:24 INFO 140607820867392] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.285482982794\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:24 INFO 140607820867392] Epoch[86] Batch [5]#011Speed: 1059.46 samples/sec#011loss=0.285483\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] Epoch[86] Batch[10] avg_epoch_loss=0.207993\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.115004244447\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] Epoch[86] Batch [10]#011Speed: 548.09 samples/sec#011loss=0.115004\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1459.130048751831, \"sum\": 1459.130048751831, \"min\": 1459.130048751831}}, \"EndTime\": 1591926025.078245, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926023.61864}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=450.915617238 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.207992647182\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] Epoch[87] Batch[0] avg_epoch_loss=0.282437\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.282436609268\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] Epoch[87] Batch[5] avg_epoch_loss=0.241210\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.241210093101\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:25 INFO 140607820867392] Epoch[87] Batch [5]#011Speed: 1079.48 samples/sec#011loss=0.241210\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] Epoch[87] Batch[10] avg_epoch_loss=0.207542\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=87, batch=10 train loss <loss>=0.167139601707\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] Epoch[87] Batch [10]#011Speed: 546.92 samples/sec#011loss=0.167140\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1466.5939807891846, \"sum\": 1466.5939807891846, \"min\": 1466.5939807891846}}, \"EndTime\": 1591926026.545413, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926025.078328}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=441.809984785 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.207541687922\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:26 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:27 INFO 140607820867392] Epoch[88] Batch[0] avg_epoch_loss=0.453274\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.453273504972\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:27 INFO 140607820867392] Epoch[88] Batch[5] avg_epoch_loss=0.294819\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:27 INFO 140607820867392] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.294818602502\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:27 INFO 140607820867392] Epoch[88] Batch [5]#011Speed: 1019.84 samples/sec#011loss=0.294819\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] Epoch[88] Batch[10] avg_epoch_loss=0.376896\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.475388008356\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] Epoch[88] Batch [10]#011Speed: 557.52 samples/sec#011loss=0.475388\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1460.8571529388428, \"sum\": 1460.8571529388428, \"min\": 1460.8571529388428}}, \"EndTime\": 1591926028.006779, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926026.545476}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=444.229349423 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.376895605163\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] Epoch[89] Batch[0] avg_epoch_loss=0.250754\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.25075379014\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] Epoch[89] Batch[5] avg_epoch_loss=0.293094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.293093793094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:28 INFO 140607820867392] Epoch[89] Batch [5]#011Speed: 1065.61 samples/sec#011loss=0.293094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:29 INFO 140607820867392] processed a total of 610 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1417.1631336212158, \"sum\": 1417.1631336212158, \"min\": 1417.1631336212158}}, \"EndTime\": 1591926029.424448, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926028.006845}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:29 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=430.400099049 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:29 INFO 140607820867392] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:29 INFO 140607820867392] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.290297193825\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:29 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] Epoch[90] Batch[0] avg_epoch_loss=0.391488\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.39148786664\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] Epoch[90] Batch[5] avg_epoch_loss=0.301595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.301594865819\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] Epoch[90] Batch [5]#011Speed: 1013.23 samples/sec#011loss=0.301595\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] Epoch[90] Batch[10] avg_epoch_loss=0.320718\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=90, batch=10 train loss <loss>=0.343665832281\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] Epoch[90] Batch [10]#011Speed: 526.83 samples/sec#011loss=0.343666\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1519.7699069976807, \"sum\": 1519.7699069976807, \"min\": 1519.7699069976807}}, \"EndTime\": 1591926030.944762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926029.424532}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=434.901306695 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.320718032393\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:30 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:31 INFO 140607820867392] Epoch[91] Batch[0] avg_epoch_loss=0.430044\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.430043697357\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:31 INFO 140607820867392] Epoch[91] Batch[5] avg_epoch_loss=0.266579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:31 INFO 140607820867392] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.266578589876\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:31 INFO 140607820867392] Epoch[91] Batch [5]#011Speed: 1056.32 samples/sec#011loss=0.266579\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] Epoch[91] Batch[10] avg_epoch_loss=0.305337\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.351846688986\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] Epoch[91] Batch [10]#011Speed: 481.44 samples/sec#011loss=0.351847\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1579.9319744110107, \"sum\": 1579.9319744110107, \"min\": 1579.9319744110107}}, \"EndTime\": 1591926032.525197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926030.944843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=424.041247312 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.305336816744\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:32 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] Epoch[92] Batch[0] avg_epoch_loss=0.349085\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.3490845263\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] Epoch[92] Batch[5] avg_epoch_loss=0.314660\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.314660042524\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] Epoch[92] Batch [5]#011Speed: 1070.47 samples/sec#011loss=0.314660\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] Epoch[92] Batch[10] avg_epoch_loss=0.361209\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.417067301273\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] Epoch[92] Batch [10]#011Speed: 558.24 samples/sec#011loss=0.417067\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1450.5841732025146, \"sum\": 1450.5841732025146, \"min\": 1450.5841732025146}}, \"EndTime\": 1591926033.976271, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926032.525261}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=448.064771984 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.361208796501\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:33 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:34 INFO 140607820867392] Epoch[93] Batch[0] avg_epoch_loss=0.305804\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.305804312229\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:34 INFO 140607820867392] Epoch[93] Batch[5] avg_epoch_loss=0.227042\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:34 INFO 140607820867392] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.227042145406\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:34 INFO 140607820867392] Epoch[93] Batch [5]#011Speed: 1087.58 samples/sec#011loss=0.227042\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] Epoch[93] Batch[10] avg_epoch_loss=0.259033\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.297422289848\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] Epoch[93] Batch [10]#011Speed: 539.12 samples/sec#011loss=0.297422\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1466.4409160614014, \"sum\": 1466.4409160614014, \"min\": 1466.4409160614014}}, \"EndTime\": 1591926035.443258, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926033.976339}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=456.169797251 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.259033120153\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:35 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] Epoch[94] Batch[0] avg_epoch_loss=0.150135\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.150135025382\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] Epoch[94] Batch[5] avg_epoch_loss=0.220123\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.220123186707\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] Epoch[94] Batch [5]#011Speed: 1098.71 samples/sec#011loss=0.220123\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] Epoch[94] Batch[10] avg_epoch_loss=0.218545\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=94, batch=10 train loss <loss>=0.216651114821\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] Epoch[94] Batch [10]#011Speed: 520.25 samples/sec#011loss=0.216651\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.270025253296, \"sum\": 1508.270025253296, \"min\": 1508.270025253296}}, \"EndTime\": 1591926036.952047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926035.443338}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=442.193626255 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.218544972214\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:36 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:37 INFO 140607820867392] Epoch[95] Batch[0] avg_epoch_loss=0.252367\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.252366840839\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:37 INFO 140607820867392] Epoch[95] Batch[5] avg_epoch_loss=0.236905\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:37 INFO 140607820867392] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.236904571454\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:37 INFO 140607820867392] Epoch[95] Batch [5]#011Speed: 1075.66 samples/sec#011loss=0.236905\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1424.6690273284912, \"sum\": 1424.6690273284912, \"min\": 1424.6690273284912}}, \"EndTime\": 1591926038.377216, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926036.952127}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=423.219937995 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.31530508697\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] Epoch[96] Batch[0] avg_epoch_loss=0.253225\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:38 INFO 140607820867392] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.253225386143\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] Epoch[96] Batch[5] avg_epoch_loss=0.210087\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.210086841136\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] Epoch[96] Batch [5]#011Speed: 1060.32 samples/sec#011loss=0.210087\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1448.6379623413086, \"sum\": 1448.6379623413086, \"min\": 1448.6379623413086}}, \"EndTime\": 1591926039.826391, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926038.3773}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=420.354282113 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.257839740068\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:39 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:40 INFO 140607820867392] Epoch[97] Batch[0] avg_epoch_loss=0.297349\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.297349244356\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:40 INFO 140607820867392] Epoch[97] Batch[5] avg_epoch_loss=0.322094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:40 INFO 140607820867392] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.322093547632\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:40 INFO 140607820867392] Epoch[97] Batch [5]#011Speed: 970.66 samples/sec#011loss=0.322094\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] Epoch[97] Batch[10] avg_epoch_loss=0.317873\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=97, batch=10 train loss <loss>=0.312808051705\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] Epoch[97] Batch [10]#011Speed: 541.36 samples/sec#011loss=0.312808\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1533.3139896392822, \"sum\": 1533.3139896392822, \"min\": 1533.3139896392822}}, \"EndTime\": 1591926041.360268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926039.826493}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=443.450895981 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.317872867666\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] Epoch[98] Batch[0] avg_epoch_loss=0.273144\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:41 INFO 140607820867392] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.273144453764\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] Epoch[98] Batch[5] avg_epoch_loss=0.160213\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.160212606192\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] Epoch[98] Batch [5]#011Speed: 1093.78 samples/sec#011loss=0.160213\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1436.9430541992188, \"sum\": 1436.9430541992188, \"min\": 1436.9430541992188}}, \"EndTime\": 1591926042.797721, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926041.360346}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=441.874052982 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.233288942277\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:42 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:43 INFO 140607820867392] Epoch[99] Batch[0] avg_epoch_loss=0.301733\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.301733374596\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:43 INFO 140607820867392] Epoch[99] Batch[5] avg_epoch_loss=0.220538\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:43 INFO 140607820867392] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.220538058629\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:43 INFO 140607820867392] Epoch[99] Batch [5]#011Speed: 875.94 samples/sec#011loss=0.220538\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Epoch[99] Batch[10] avg_epoch_loss=0.253940\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=99, batch=10 train loss <loss>=0.294021733105\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Epoch[99] Batch [10]#011Speed: 553.61 samples/sec#011loss=0.294022\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1512.948989868164, \"sum\": 1512.948989868164, \"min\": 1512.948989868164}}, \"EndTime\": 1591926044.311221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926042.797802}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] #throughput_metric: host=algo-1, train throughput=432.89665845 records/second\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.253939728845\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Final loss: 0.195441491902 (occurred at epoch 81)\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] #quality_metric: host=algo-1, train final_loss <loss>=0.195441491902\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 WARNING 140607820867392] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 245.02301216125488, \"sum\": 245.02301216125488, \"min\": 245.02301216125488}}, \"EndTime\": 1591926044.557261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926044.311299}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 304.63099479675293, \"sum\": 304.63099479675293, \"min\": 304.63099479675293}}, \"EndTime\": 1591926044.61683, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926044.557344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 8.577823638916016, \"sum\": 8.577823638916016, \"min\": 8.577823638916016}}, \"EndTime\": 1591926044.625525, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926044.616904}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:44 INFO 140607820867392] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.025987625122070312, \"sum\": 0.025987625122070312, \"min\": 0.025987625122070312}}, \"EndTime\": 1591926044.626193, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926044.625569}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:47 INFO 140607820867392] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:50 INFO 140607820867392] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:53 INFO 140607820867392] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:56 INFO 140607820867392] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:40:59 INFO 140607820867392] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:02 INFO 140607820867392] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:05 INFO 140607820867392] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 22767.117977142334, \"sum\": 22767.117977142334, \"min\": 22767.117977142334}}, \"EndTime\": 1591926067.393289, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926044.626247}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, RMSE): 0.719578927792\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, mean_absolute_QuantileLoss): 1726.468723947014\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, mean_wQuantileLoss): 0.4567377576579402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.1]): 0.5615166836451719\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.2]): 0.5466204562840402\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.3]): 0.5237010944363164\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.4]): 0.4962428807978217\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.5]): 0.4652408348874093\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.6]): 0.4309993020855581\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.7]): 0.39538779346875985\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.8]): 0.36232629365031477\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #test_score (algo-1, wQuantileLoss[0.9]): 0.3286044796660699\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.456737757658\u001b[0m\n",
      "\u001b[34m[06/12/2020 01:41:07 INFO 140607820867392] #quality_metric: host=algo-1, test RMSE <loss>=0.719578927792\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 175575.98686218262, \"sum\": 175575.98686218262, \"min\": 175575.98686218262}, \"setuptime\": {\"count\": 1, \"max\": 9.8419189453125, \"sum\": 9.8419189453125, \"min\": 9.8419189453125}}, \"EndTime\": 1591926067.411105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591926067.393357}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-12 01:41:17 Uploading - Uploading generated training model\n",
      "2020-06-12 01:41:17 Completed - Training job completed\n",
      "Training seconds: 218\n",
      "Billable seconds: 218\n",
      "CPU times: user 1.02 s, sys: 50.9 ms, total: 1.07 s\n",
      "Wall time: 6min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "price_estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "price_predictor = price_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predictor = sagemaker.predictor.RealTimePredictor(endpoint='deep-ar-price-volume-2020-06-12-01-34-52-448')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03987</td>\n",
       "      <td>0.043087</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target  prediction\n",
       "Date       Ticker                                         \n",
       "2019-01-02 AAPL      0.03987  0.043087      -1          -1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume']\n",
    "date = '2019-01-02'\n",
    "ticker = 'AAPL'\n",
    "df = stock_data_preprocessed\n",
    "predictor = price_predictor\n",
    "def get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat):\n",
    "\n",
    "    date_pred = pd.Timestamp(date, freq='D')\n",
    "    date_start = date_pred-timedelta(days=50)\n",
    "    pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "    result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "    pred = {\n",
    "            \"start\": str(date_pred),\n",
    "            \"target\": pred_df['target'][date_start:date_pred-timedelta(days=1)].tolist(),\n",
    "            \"dynamic_feat\": pred_df[dynamic_feat][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "    req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "    res = price_predictor.predict(req)\n",
    "    prediction_data = json.loads(res.decode('utf-8'))\n",
    "    pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "    result_df['prediction'] = pred\n",
    "\n",
    "\n",
    "    return result_df\n",
    "\n",
    "get_dynamic_feat_prediction(ticker,date,df,predictor,dynamic_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_dynamic_feat_accuracy(ticker):\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "    df = stock_data_preprocessed\n",
    "    for date in date_index:\n",
    "        target.append(get_dynamic_feat_prediction(ticker, date,df,price_predictor,dynamic_feat)['target'].values[0])\n",
    "        prediction.append(int(get_dynamic_feat_prediction(ticker, date,df,price_predictor,dynamic_feat)['prediction'].values[0]))\n",
    "    return accuracy_score(target, prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.746031746031746"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dynamic_feat_accuracy(ticker='AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "for ticker in tickers:\n",
    "    acc[ticker] = get_dynamic_feat_accuracy(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: RuntimeWarning: Mean of empty slice.\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(list(acc.values())).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
