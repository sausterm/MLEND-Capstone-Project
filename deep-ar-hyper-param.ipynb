{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas_datareader/compat/__init__.py:7: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  from pandas.util.testing import assert_frame_equal\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from helper import *\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data_hyper_param\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output_hyper_param\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1:  3268098\n",
      " 0:  3220281\n",
      " 1:  4107528\n"
     ]
    }
   ],
   "source": [
    "stock_hyper_param_data = pd.read_csv('stock_indicator_data.csv',parse_dates=True, index_col=[0,1])\n",
    "get_target_distribution(stock_hyper_param_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = get_sp500_tickers()\n",
    "\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 1 day\n",
    "prediction_length = 1\n",
    "\n",
    "# we use 50 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp('2018-12-31', freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "    \n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_hyper_param_data.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = []\n",
    "for ts in timeseries:\n",
    "    tickers.append(ts.index[1][1])\n",
    "cat = {}\n",
    "for ticker in enumerate(tickers):\n",
    "    cat[ticker[1]] = ticker[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "dynamic_feat = ['Adj Close','Volume','PC1','PC2','PC3','PC4','PC5','PC6']\n",
    "training_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "        \"cat\" : cat[ts.index[1][1]],\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training].values.T.tolist()\n",
    "        \n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].tolist(),\n",
    "        \"cat\" : cat[ts.index[1][1]], # input stock ticker id\n",
    "        \"dynamic_feat\": ts[dynamic_feat][ts.index[0][0]:end_training + timedelta(days=(2*k * prediction_length))].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train_hyper_param.json\", training_data)\n",
    "write_json_dataset(\"test_hyper_param.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_hyper_param/train/train.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n",
      "File s3://sagemaker-us-east-2-017500148529/s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data_hyper_param/test/test.json already exists.\n",
      "Set override to upload anyway.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "copy_to_s3(\"train_hyper_param.json\", s3_data_path + \"/train/train.json\", s3_bucket)\n",
    "copy_to_s3(\"train_hyper_param.json\", s3_data_path + \"/test/test.json\", s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-03-16 00:00:00\", \"target\": [1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, -1, 1, 1, 1, 1, 1...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator_hyper_param = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": 50,\n",
    "    \"num_layers\":2,\n",
    "    \"dropout_rate\":0.1,\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_hyper_param.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "estimator_hyper_param_tuner = HyperparameterTuner(estimator = estimator_hyper_param, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'test:RMSE', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 30, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'epochs': IntegerParameter(50, 200),\n",
    "                                                    'context_length': IntegerParameter(10, 100),\n",
    "                                                    'mini_batch_size': IntegerParameter(32, 256),\n",
    "                                                    'learning_rate': ContinuousParameter(\"1E-5\", \"1E-3\"),\n",
    "                                                    'num_cells': IntegerParameter(30, 200),\n",
    "                                                    'dropout_rate': ContinuousParameter(0,0.2),\n",
    "                                                    'num_layers': IntegerParameter(1,3)\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.06 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "#estimator_hyper_param_tuner.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator_hyper_param = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"96\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"212\",\n",
    "    \"learning_rate\": \"4.177378470748047e-05\",\n",
    "    \"context_length\": \"94\",\n",
    "    \"prediction_length\": \"1\",\n",
    "    \"num_cells\": \"115\",\n",
    "    \"num_layers\":\"3\",\n",
    "    \"dropout_rate\":\"0.04030803446099004\",\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}\n",
    "estimator_hyper_param.set_hyperparameters(**hyperparameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-06 15:50:33 Starting - Starting the training job.2020-06-06 15:50:33 Starting - Starting the training job.....\n",
      "2020-06-06 15:50:35 Starting - Launching requested ML instances.\n",
      "2020-06-06 15:50:35 Starting - Launching requested ML instances...........\n",
      "2020-06-06 15:51:38 Starting - Preparing the instances for training.\n",
      "2020-06-06 15:51:38 Starting - Preparing the instances for training.....\n",
      "2020-06-06 15:52:16 Downloading - Downloading input data.\n",
      "2020-06-06 15:52:16 Downloading - Downloading input data.....\n",
      "2020-06-06 15:53:01 Training - Training image download completed. Training in progress..\n",
      "2020-06-06 15:53:01 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.04030803446099004', u'learning_rate': u'4.177378470748047e-05', u'num_cells': u'115', u'prediction_length': u'1', u'epochs': u'96', u'time_freq': u'D', u'context_length': u'94', u'num_layers': u'3', u'mini_batch_size': u'212', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Final configuration: {u'dropout_rate': u'0.04030803446099004', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'4.177378470748047e-05', u'num_layers': u'3', u'epochs': u'96', u'embedding_dimension': u'10', u'num_cells': u'115', u'_num_kv_servers': u'auto', u'mini_batch_size': u'212', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'94', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] [cardinality=auto] Inferred value of cardinality=[491] from dataset.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Real time series\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Small number of time series. Doing 5 passes over dataset with prob 0.863543788187 per epoch.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Real time series\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] nvidia-smi took: 0.025251865387 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 667.5670146942139, \"sum\": 667.5670146942139, \"min\": 667.5670146942139}}, \"EndTime\": 1591458786.032263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458785.363845}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:06 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1146.0239887237549, \"sum\": 1146.0239887237549, \"min\": 1146.0239887237549}}, \"EndTime\": 1591458786.51, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458786.032344}\n",
      "\u001b[0m\n",
      "\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.04030803446099004', u'learning_rate': u'4.177378470748047e-05', u'num_cells': u'115', u'prediction_length': u'1', u'epochs': u'96', u'time_freq': u'D', u'context_length': u'94', u'num_layers': u'3', u'mini_batch_size': u'212', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Final configuration: {u'dropout_rate': u'0.04030803446099004', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'4.177378470748047e-05', u'num_layers': u'3', u'epochs': u'96', u'embedding_dimension': u'10', u'num_cells': u'115', u'_num_kv_servers': u'auto', u'mini_batch_size': u'212', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'94', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] [cardinality=auto] `cat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:03 INFO 140027608049472] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] [cardinality=auto] Inferred value of cardinality=[491] from dataset.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=8 from dataset.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Real time series\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:04 INFO 140027608049472] Small number of time series. Doing 5 passes over dataset with prob 0.863543788187 per epoch.\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Real time series\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] number of observations: 1052687\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] mean target length: 2143\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] min/mean/max target: -1.0/0.0748019116794/1.0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] mean abs(target): 0.697209141939\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] nvidia-smi took: 0.025251865387 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:05 INFO 140027608049472] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 667.5670146942139, \"sum\": 667.5670146942139, \"min\": 667.5670146942139}}, \"EndTime\": 1591458786.032263, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458785.363845}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:06 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1146.0239887237549, \"sum\": 1146.0239887237549, \"min\": 1146.0239887237549}}, \"EndTime\": 1591458786.51, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458786.032344}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:12 INFO 140027608049472] Epoch[0] Batch[0] avg_epoch_loss=1.414587\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.41458705686\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] Epoch[0] Batch[5] avg_epoch_loss=1.416837\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.41683655265\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] Epoch[0] Batch [5]#011Speed: 190.14 samples/sec#011loss=1.416837\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:12 INFO 140027608049472] Epoch[0] Batch[0] avg_epoch_loss=1.414587\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.41458705686\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] Epoch[0] Batch[5] avg_epoch_loss=1.416837\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.41683655265\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:17 INFO 140027608049472] Epoch[0] Batch [5]#011Speed: 190.14 samples/sec#011loss=1.416837\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Epoch[0] Batch[10] avg_epoch_loss=1.403895\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=1.38836583551\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Epoch[0] Batch [10]#011Speed: 175.95 samples/sec#011loss=1.388366\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] processed a total of 2145 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}, \"update.time\": {\"count\": 1, \"max\": 17223.159074783325, \"sum\": 17223.159074783325, \"min\": 17223.159074783325}}, \"EndTime\": 1591458803.7333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458786.510062}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=124.540695125 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.40389531759\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_950bce79-bddd-4806-8bec-976f1efb1f58-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.04510688781738, \"sum\": 52.04510688781738, \"min\": 52.04510688781738}}, \"EndTime\": 1591458803.785978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458803.733391}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Epoch[0] Batch[10] avg_epoch_loss=1.403895\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=1.38836583551\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Epoch[0] Batch [10]#011Speed: 175.95 samples/sec#011loss=1.388366\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] processed a total of 2145 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 96, \"sum\": 96.0, \"min\": 96}, \"update.time\": {\"count\": 1, \"max\": 17223.159074783325, \"sum\": 17223.159074783325, \"min\": 17223.159074783325}}, \"EndTime\": 1591458803.7333, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458786.510062}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=124.540695125 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.40389531759\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_950bce79-bddd-4806-8bec-976f1efb1f58-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.04510688781738, \"sum\": 52.04510688781738, \"min\": 52.04510688781738}}, \"EndTime\": 1591458803.785978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458803.733391}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:29 INFO 140027608049472] Epoch[1] Batch[0] avg_epoch_loss=1.367955\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.36795547773\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] Epoch[1] Batch[5] avg_epoch_loss=1.323757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.32375712365\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] Epoch[1] Batch [5]#011Speed: 199.84 samples/sec#011loss=1.323757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:29 INFO 140027608049472] Epoch[1] Batch[0] avg_epoch_loss=1.367955\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.36795547773\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] Epoch[1] Batch[5] avg_epoch_loss=1.323757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.32375712365\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:34 INFO 140027608049472] Epoch[1] Batch [5]#011Speed: 199.84 samples/sec#011loss=1.323757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] processed a total of 2085 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15543.197870254517, \"sum\": 15543.197870254517, \"min\": 15543.197870254517}}, \"EndTime\": 1591458819.329315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458803.786054}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.141224397 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.31335443461\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_caea2a48-e145-40fe-bc80-e99050d83392-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.41396141052246, \"sum\": 55.41396141052246, \"min\": 55.41396141052246}}, \"EndTime\": 1591458819.385346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458819.329399}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:44 INFO 140027608049472] Epoch[2] Batch[0] avg_epoch_loss=1.271975\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.27197452761\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] processed a total of 2085 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15543.197870254517, \"sum\": 15543.197870254517, \"min\": 15543.197870254517}}, \"EndTime\": 1591458819.329315, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458803.786054}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.141224397 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.31335443461\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:39 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_caea2a48-e145-40fe-bc80-e99050d83392-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.41396141052246, \"sum\": 55.41396141052246, \"min\": 55.41396141052246}}, \"EndTime\": 1591458819.385346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458819.329399}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:44 INFO 140027608049472] Epoch[2] Batch[0] avg_epoch_loss=1.271975\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.27197452761\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] Epoch[2] Batch[5] avg_epoch_loss=1.291549\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.29154910681\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] Epoch[2] Batch [5]#011Speed: 198.85 samples/sec#011loss=1.291549\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Epoch[2] Batch[10] avg_epoch_loss=1.305002\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.32114516924\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Epoch[2] Batch [10]#011Speed: 174.27 samples/sec#011loss=1.321145\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] processed a total of 2160 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16688.603162765503, \"sum\": 16688.603162765503, \"min\": 16688.603162765503}}, \"EndTime\": 1591458836.07407, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458819.385412}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.42876957 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.30500186246\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_f586b310-8081-49b4-93d3-64eb8c0db6e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.72382164001465, \"sum\": 43.72382164001465, \"min\": 43.72382164001465}}, \"EndTime\": 1591458836.118394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458836.07415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] Epoch[2] Batch[5] avg_epoch_loss=1.291549\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.29154910681\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:49 INFO 140027608049472] Epoch[2] Batch [5]#011Speed: 198.85 samples/sec#011loss=1.291549\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Epoch[2] Batch[10] avg_epoch_loss=1.305002\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.32114516924\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Epoch[2] Batch [10]#011Speed: 174.27 samples/sec#011loss=1.321145\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] processed a total of 2160 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16688.603162765503, \"sum\": 16688.603162765503, \"min\": 16688.603162765503}}, \"EndTime\": 1591458836.07407, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458819.385412}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.42876957 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.30500186246\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:53:56 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_f586b310-8081-49b4-93d3-64eb8c0db6e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.72382164001465, \"sum\": 43.72382164001465, \"min\": 43.72382164001465}}, \"EndTime\": 1591458836.118394, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458836.07415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:01 INFO 140027608049472] Epoch[3] Batch[0] avg_epoch_loss=1.327851\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.32785077365\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] Epoch[3] Batch[5] avg_epoch_loss=1.276614\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.27661444706\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] Epoch[3] Batch [5]#011Speed: 199.39 samples/sec#011loss=1.276614\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:01 INFO 140027608049472] Epoch[3] Batch[0] avg_epoch_loss=1.327851\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.32785077365\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] Epoch[3] Batch[5] avg_epoch_loss=1.276614\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.27661444706\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:06 INFO 140027608049472] Epoch[3] Batch [5]#011Speed: 199.39 samples/sec#011loss=1.276614\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Epoch[3] Batch[10] avg_epoch_loss=1.296776\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.32096963559\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Epoch[3] Batch [10]#011Speed: 177.01 samples/sec#011loss=1.320970\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] processed a total of 2172 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16553.300142288208, \"sum\": 16553.300142288208, \"min\": 16553.300142288208}}, \"EndTime\": 1591458852.671822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458836.118463}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.211540634 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.29677589639\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_acb80f3e-61c2-4794-9195-948e60d4b075-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.76299476623535, \"sum\": 42.76299476623535, \"min\": 42.76299476623535}}, \"EndTime\": 1591458852.715169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458852.671906}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:17 INFO 140027608049472] Epoch[4] Batch[0] avg_epoch_loss=1.289600\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.28960001244\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Epoch[3] Batch[10] avg_epoch_loss=1.296776\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.32096963559\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Epoch[3] Batch [10]#011Speed: 177.01 samples/sec#011loss=1.320970\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] processed a total of 2172 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16553.300142288208, \"sum\": 16553.300142288208, \"min\": 16553.300142288208}}, \"EndTime\": 1591458852.671822, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458836.118463}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.211540634 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.29677589639\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:12 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_acb80f3e-61c2-4794-9195-948e60d4b075-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.76299476623535, \"sum\": 42.76299476623535, \"min\": 42.76299476623535}}, \"EndTime\": 1591458852.715169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458852.671906}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:17 INFO 140027608049472] Epoch[4] Batch[0] avg_epoch_loss=1.289600\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.28960001244\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] Epoch[4] Batch[5] avg_epoch_loss=1.284735\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.28473509783\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] Epoch[4] Batch [5]#011Speed: 200.42 samples/sec#011loss=1.284735\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] Epoch[4] Batch[5] avg_epoch_loss=1.284735\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.28473509783\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:23 INFO 140027608049472] Epoch[4] Batch [5]#011Speed: 200.42 samples/sec#011loss=1.284735\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Epoch[4] Batch[10] avg_epoch_loss=1.284745\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.2847571319\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Epoch[4] Batch [10]#011Speed: 176.61 samples/sec#011loss=1.284757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] processed a total of 2143 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16538.899898529053, \"sum\": 16538.899898529053, \"min\": 16538.899898529053}}, \"EndTime\": 1591458869.254196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458852.715237}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.572416941 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.28474511331\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_42e0138d-9762-47dd-9f04-68f6a6049e9f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.22607612609863, \"sum\": 42.22607612609863, \"min\": 42.22607612609863}}, \"EndTime\": 1591458869.297026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458869.254274}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:34 INFO 140027608049472] Epoch[5] Batch[0] avg_epoch_loss=1.260194\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.26019416665\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] Epoch[5] Batch[5] avg_epoch_loss=1.271969\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.27196903349\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] Epoch[5] Batch [5]#011Speed: 200.62 samples/sec#011loss=1.271969\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Epoch[4] Batch[10] avg_epoch_loss=1.284745\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=1.2847571319\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Epoch[4] Batch [10]#011Speed: 176.61 samples/sec#011loss=1.284757\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] processed a total of 2143 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16538.899898529053, \"sum\": 16538.899898529053, \"min\": 16538.899898529053}}, \"EndTime\": 1591458869.254196, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458852.715237}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.572416941 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.28474511331\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:29 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_42e0138d-9762-47dd-9f04-68f6a6049e9f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.22607612609863, \"sum\": 42.22607612609863, \"min\": 42.22607612609863}}, \"EndTime\": 1591458869.297026, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458869.254274}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:34 INFO 140027608049472] Epoch[5] Batch[0] avg_epoch_loss=1.260194\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.26019416665\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] Epoch[5] Batch[5] avg_epoch_loss=1.271969\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.27196903349\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:39 INFO 140027608049472] Epoch[5] Batch [5]#011Speed: 200.62 samples/sec#011loss=1.271969\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] processed a total of 2120 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15386.007070541382, \"sum\": 15386.007070541382, \"min\": 15386.007070541382}}, \"EndTime\": 1591458884.683354, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458869.297283}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=137.786596346 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.2689529275\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2716f8e5-643f-43b5-be8a-d04776d56776-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.768001556396484, \"sum\": 42.768001556396484, \"min\": 42.768001556396484}}, \"EndTime\": 1591458884.726737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458884.683423}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] processed a total of 2120 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15386.007070541382, \"sum\": 15386.007070541382, \"min\": 15386.007070541382}}, \"EndTime\": 1591458884.683354, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458869.297283}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=137.786596346 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.2689529275\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:44 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2716f8e5-643f-43b5-be8a-d04776d56776-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.768001556396484, \"sum\": 42.768001556396484, \"min\": 42.768001556396484}}, \"EndTime\": 1591458884.726737, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458884.683423}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:49 INFO 140027608049472] Epoch[6] Batch[0] avg_epoch_loss=1.232570\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.23257014437\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] Epoch[6] Batch[5] avg_epoch_loss=1.252993\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.25299257003\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] Epoch[6] Batch [5]#011Speed: 199.68 samples/sec#011loss=1.252993\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:49 INFO 140027608049472] Epoch[6] Batch[0] avg_epoch_loss=1.232570\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.23257014437\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] Epoch[6] Batch[5] avg_epoch_loss=1.252993\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.25299257003\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:54:55 INFO 140027608049472] Epoch[6] Batch [5]#011Speed: 199.68 samples/sec#011loss=1.252993\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] processed a total of 2118 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15480.45802116394, \"sum\": 15480.45802116394, \"min\": 15480.45802116394}}, \"EndTime\": 1591458900.20732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458884.726806}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.816835486 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.2529993741\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_a837fbb6-08e2-4482-b983-08e00968b8a3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.13804817199707, \"sum\": 46.13804817199707, \"min\": 46.13804817199707}}, \"EndTime\": 1591458900.254047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458900.20738}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:05 INFO 140027608049472] Epoch[7] Batch[0] avg_epoch_loss=1.258232\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.25823197275\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] processed a total of 2118 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15480.45802116394, \"sum\": 15480.45802116394, \"min\": 15480.45802116394}}, \"EndTime\": 1591458900.20732, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458884.726806}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.816835486 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.2529993741\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:00 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_a837fbb6-08e2-4482-b983-08e00968b8a3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 46.13804817199707, \"sum\": 46.13804817199707, \"min\": 46.13804817199707}}, \"EndTime\": 1591458900.254047, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458900.20738}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:05 INFO 140027608049472] Epoch[7] Batch[0] avg_epoch_loss=1.258232\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.25823197275\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] Epoch[7] Batch[5] avg_epoch_loss=1.258412\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.25841215122\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] Epoch[7] Batch [5]#011Speed: 199.28 samples/sec#011loss=1.258412\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Epoch[7] Batch[10] avg_epoch_loss=1.252342\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.24505694407\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Epoch[7] Batch [10]#011Speed: 176.77 samples/sec#011loss=1.245057\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16602.63991355896, \"sum\": 16602.63991355896, \"min\": 16602.63991355896}}, \"EndTime\": 1591458916.856831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458900.254127}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.556825845 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.25234160252\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_6ae0f32b-b046-45df-9a7d-f62beb1253fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.30900573730469, \"sum\": 59.30900573730469, \"min\": 59.30900573730469}}, \"EndTime\": 1591458916.916675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458916.856907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] Epoch[7] Batch[5] avg_epoch_loss=1.258412\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.25841215122\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:10 INFO 140027608049472] Epoch[7] Batch [5]#011Speed: 199.28 samples/sec#011loss=1.258412\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Epoch[7] Batch[10] avg_epoch_loss=1.252342\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=1.24505694407\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Epoch[7] Batch [10]#011Speed: 176.77 samples/sec#011loss=1.245057\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16602.63991355896, \"sum\": 16602.63991355896, \"min\": 16602.63991355896}}, \"EndTime\": 1591458916.856831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458900.254127}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.556825845 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.25234160252\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:16 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_6ae0f32b-b046-45df-9a7d-f62beb1253fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 59.30900573730469, \"sum\": 59.30900573730469, \"min\": 59.30900573730469}}, \"EndTime\": 1591458916.916675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458916.856907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:22 INFO 140027608049472] Epoch[8] Batch[0] avg_epoch_loss=1.241968\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.24196840682\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] Epoch[8] Batch[5] avg_epoch_loss=1.250403\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.25040337425\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] Epoch[8] Batch [5]#011Speed: 198.76 samples/sec#011loss=1.250403\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:22 INFO 140027608049472] Epoch[8] Batch[0] avg_epoch_loss=1.241968\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.24196840682\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] Epoch[8] Batch[5] avg_epoch_loss=1.250403\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.25040337425\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:27 INFO 140027608049472] Epoch[8] Batch [5]#011Speed: 198.76 samples/sec#011loss=1.250403\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] processed a total of 2016 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15437.474012374878, \"sum\": 15437.474012374878, \"min\": 15437.474012374878}}, \"EndTime\": 1591458932.354268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458916.916728}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.590288394 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.24999972649\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3c0dfb58-f062-4a9a-b6f0-c34b7114dea8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.83404350280762, \"sum\": 42.83404350280762, \"min\": 42.83404350280762}}, \"EndTime\": 1591458932.397729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458932.35435}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:37 INFO 140027608049472] Epoch[9] Batch[0] avg_epoch_loss=1.242877\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:37 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.24287745638\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] processed a total of 2016 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15437.474012374878, \"sum\": 15437.474012374878, \"min\": 15437.474012374878}}, \"EndTime\": 1591458932.354268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458916.916728}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.590288394 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.24999972649\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:32 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3c0dfb58-f062-4a9a-b6f0-c34b7114dea8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.83404350280762, \"sum\": 42.83404350280762, \"min\": 42.83404350280762}}, \"EndTime\": 1591458932.397729, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458932.35435}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:37 INFO 140027608049472] Epoch[9] Batch[0] avg_epoch_loss=1.242877\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:37 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.24287745638\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] Epoch[9] Batch[5] avg_epoch_loss=1.238534\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.23853412364\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] Epoch[9] Batch [5]#011Speed: 200.69 samples/sec#011loss=1.238534\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Epoch[9] Batch[10] avg_epoch_loss=1.246415\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.25587189872\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Epoch[9] Batch [10]#011Speed: 177.18 samples/sec#011loss=1.255872\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] processed a total of 2135 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16529.609203338623, \"sum\": 16529.609203338623, \"min\": 16529.609203338623}}, \"EndTime\": 1591458948.927485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458932.397793}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.161338551 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.2464149305\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_f2b3d38e-cc10-4e05-8521-eb3bb3646795-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.07181930541992, \"sum\": 42.07181930541992, \"min\": 42.07181930541992}}, \"EndTime\": 1591458948.970108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458948.927555}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] Epoch[9] Batch[5] avg_epoch_loss=1.238534\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.23853412364\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:42 INFO 140027608049472] Epoch[9] Batch [5]#011Speed: 200.69 samples/sec#011loss=1.238534\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Epoch[9] Batch[10] avg_epoch_loss=1.246415\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=1.25587189872\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Epoch[9] Batch [10]#011Speed: 177.18 samples/sec#011loss=1.255872\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] processed a total of 2135 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16529.609203338623, \"sum\": 16529.609203338623, \"min\": 16529.609203338623}}, \"EndTime\": 1591458948.927485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458932.397793}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.161338551 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.2464149305\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:48 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_f2b3d38e-cc10-4e05-8521-eb3bb3646795-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.07181930541992, \"sum\": 42.07181930541992, \"min\": 42.07181930541992}}, \"EndTime\": 1591458948.970108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458948.927555}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:54 INFO 140027608049472] Epoch[10] Batch[0] avg_epoch_loss=1.254240\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.2542397841\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] Epoch[10] Batch[5] avg_epoch_loss=1.240330\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.24033029424\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] Epoch[10] Batch [5]#011Speed: 199.47 samples/sec#011loss=1.240330\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:54 INFO 140027608049472] Epoch[10] Batch[0] avg_epoch_loss=1.254240\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.2542397841\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] Epoch[10] Batch[5] avg_epoch_loss=1.240330\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.24033029424\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:55:59 INFO 140027608049472] Epoch[10] Batch [5]#011Speed: 199.47 samples/sec#011loss=1.240330\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] processed a total of 2078 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15446.55704498291, \"sum\": 15446.55704498291, \"min\": 15446.55704498291}}, \"EndTime\": 1591458964.416794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458948.970181}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.527324705 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.23489883711\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_7882fce1-cb6b-45ab-a0ac-7b3a40b67204-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.660879135131836, \"sum\": 43.660879135131836, \"min\": 43.660879135131836}}, \"EndTime\": 1591458964.461074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458964.416876}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] processed a total of 2078 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15446.55704498291, \"sum\": 15446.55704498291, \"min\": 15446.55704498291}}, \"EndTime\": 1591458964.416794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458948.970181}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.527324705 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.23489883711\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_7882fce1-cb6b-45ab-a0ac-7b3a40b67204-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.660879135131836, \"sum\": 43.660879135131836, \"min\": 43.660879135131836}}, \"EndTime\": 1591458964.461074, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458964.416876}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:09 INFO 140027608049472] Epoch[11] Batch[0] avg_epoch_loss=1.218137\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.21813705732\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] Epoch[11] Batch[5] avg_epoch_loss=1.213258\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.21325790357\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] Epoch[11] Batch [5]#011Speed: 199.44 samples/sec#011loss=1.213258\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] processed a total of 2075 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15541.123151779175, \"sum\": 15541.123151779175, \"min\": 15541.123151779175}}, \"EndTime\": 1591458980.002322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458964.461141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.515754539 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.21619459908\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_8cb3fbbe-c79d-4590-9a08-a17efc323638-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 56.849002838134766, \"sum\": 56.849002838134766, \"min\": 56.849002838134766}}, \"EndTime\": 1591458980.059723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458980.002401}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:09 INFO 140027608049472] Epoch[11] Batch[0] avg_epoch_loss=1.218137\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.21813705732\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] Epoch[11] Batch[5] avg_epoch_loss=1.213258\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.21325790357\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:15 INFO 140027608049472] Epoch[11] Batch [5]#011Speed: 199.44 samples/sec#011loss=1.213258\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] processed a total of 2075 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15541.123151779175, \"sum\": 15541.123151779175, \"min\": 15541.123151779175}}, \"EndTime\": 1591458980.002322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458964.461141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.515754539 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=11, train loss <loss>=1.21619459908\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:20 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_8cb3fbbe-c79d-4590-9a08-a17efc323638-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 56.849002838134766, \"sum\": 56.849002838134766, \"min\": 56.849002838134766}}, \"EndTime\": 1591458980.059723, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458980.002401}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:25 INFO 140027608049472] Epoch[12] Batch[0] avg_epoch_loss=1.212278\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:25 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.21227811417\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] Epoch[12] Batch[5] avg_epoch_loss=1.223230\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.22322984611\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] Epoch[12] Batch [5]#011Speed: 199.42 samples/sec#011loss=1.223230\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:25 INFO 140027608049472] Epoch[12] Batch[0] avg_epoch_loss=1.212278\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:25 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.21227811417\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] Epoch[12] Batch[5] avg_epoch_loss=1.223230\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.22322984611\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:30 INFO 140027608049472] Epoch[12] Batch [5]#011Speed: 199.42 samples/sec#011loss=1.223230\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Epoch[12] Batch[10] avg_epoch_loss=1.200827\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.17394446607\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Epoch[12] Batch [10]#011Speed: 176.68 samples/sec#011loss=1.173944\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] processed a total of 2194 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16643.083095550537, \"sum\": 16643.083095550537, \"min\": 16643.083095550537}}, \"EndTime\": 1591458996.702948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458980.059801}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.825633749 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.20082740064\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_94d8a620-11c8-403e-a75c-87eafa4df006-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 56.051015853881836, \"sum\": 56.051015853881836, \"min\": 56.051015853881836}}, \"EndTime\": 1591458996.75955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458996.703026}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Epoch[12] Batch[10] avg_epoch_loss=1.200827\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=1.17394446607\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Epoch[12] Batch [10]#011Speed: 176.68 samples/sec#011loss=1.173944\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] processed a total of 2194 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16643.083095550537, \"sum\": 16643.083095550537, \"min\": 16643.083095550537}}, \"EndTime\": 1591458996.702948, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458980.059801}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.825633749 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.20082740064\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:36 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_94d8a620-11c8-403e-a75c-87eafa4df006-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 56.051015853881836, \"sum\": 56.051015853881836, \"min\": 56.051015853881836}}, \"EndTime\": 1591458996.75955, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458996.703026}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:42 INFO 140027608049472] Epoch[13] Batch[0] avg_epoch_loss=1.208723\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.20872324818\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] Epoch[13] Batch[5] avg_epoch_loss=1.201401\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.20140076883\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] Epoch[13] Batch [5]#011Speed: 199.22 samples/sec#011loss=1.201401\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:42 INFO 140027608049472] Epoch[13] Batch[0] avg_epoch_loss=1.208723\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.20872324818\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] Epoch[13] Batch[5] avg_epoch_loss=1.201401\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.20140076883\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:47 INFO 140027608049472] Epoch[13] Batch [5]#011Speed: 199.22 samples/sec#011loss=1.201401\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] Epoch[13] Batch[10] avg_epoch_loss=1.203855\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=1.20679984902\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] Epoch[13] Batch [10]#011Speed: 177.07 samples/sec#011loss=1.206800\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] processed a total of 2178 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16585.65402030945, \"sum\": 16585.65402030945, \"min\": 16585.65402030945}}, \"EndTime\": 1591459013.345319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458996.759611}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.317443939 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.20385489619\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:58 INFO 140027608049472] Epoch[14] Batch[0] avg_epoch_loss=1.200222\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.20022230328\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] Epoch[13] Batch[10] avg_epoch_loss=1.203855\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=1.20679984902\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] Epoch[13] Batch [10]#011Speed: 177.07 samples/sec#011loss=1.206800\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] processed a total of 2178 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16585.65402030945, \"sum\": 16585.65402030945, \"min\": 16585.65402030945}}, \"EndTime\": 1591459013.345319, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591458996.759611}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.317443939 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.20385489619\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:53 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:58 INFO 140027608049472] Epoch[14] Batch[0] avg_epoch_loss=1.200222\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:56:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=1.20022230328\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] Epoch[14] Batch[5] avg_epoch_loss=1.206671\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.20667072512\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] Epoch[14] Batch [5]#011Speed: 200.29 samples/sec#011loss=1.206671\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] processed a total of 2102 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15421.639919281006, \"sum\": 15421.639919281006, \"min\": 15421.639919281006}}, \"EndTime\": 1591459028.767515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459013.345394}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.300933309 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.20026552452\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_0970af14-99d4-417a-9bc9-94a09323430e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.60906982421875, \"sum\": 44.60906982421875, \"min\": 44.60906982421875}}, \"EndTime\": 1591459028.812762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459028.767598}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] Epoch[14] Batch[5] avg_epoch_loss=1.206671\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.20667072512\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:03 INFO 140027608049472] Epoch[14] Batch [5]#011Speed: 200.29 samples/sec#011loss=1.206671\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] processed a total of 2102 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15421.639919281006, \"sum\": 15421.639919281006, \"min\": 15421.639919281006}}, \"EndTime\": 1591459028.767515, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459013.345394}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.300933309 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.20026552452\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:08 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_0970af14-99d4-417a-9bc9-94a09323430e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.60906982421875, \"sum\": 44.60906982421875, \"min\": 44.60906982421875}}, \"EndTime\": 1591459028.812762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459028.767598}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:14 INFO 140027608049472] Epoch[15] Batch[0] avg_epoch_loss=1.183127\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.18312720533\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] Epoch[15] Batch[5] avg_epoch_loss=1.188291\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.18829079394\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] Epoch[15] Batch [5]#011Speed: 199.99 samples/sec#011loss=1.188291\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:14 INFO 140027608049472] Epoch[15] Batch[0] avg_epoch_loss=1.183127\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.18312720533\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] Epoch[15] Batch[5] avg_epoch_loss=1.188291\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=1.18829079394\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:19 INFO 140027608049472] Epoch[15] Batch [5]#011Speed: 199.99 samples/sec#011loss=1.188291\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] processed a total of 2077 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15438.529014587402, \"sum\": 15438.529014587402, \"min\": 15438.529014587402}}, \"EndTime\": 1591459044.251401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459028.812815}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.532626323 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.19012242443\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3240a6b4-3c22-4811-87e9-28837793a577-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.14112663269043, \"sum\": 43.14112663269043, \"min\": 43.14112663269043}}, \"EndTime\": 1591459044.295152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459044.251476}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:29 INFO 140027608049472] Epoch[16] Batch[0] avg_epoch_loss=1.214624\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.21462408102\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] processed a total of 2077 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15438.529014587402, \"sum\": 15438.529014587402, \"min\": 15438.529014587402}}, \"EndTime\": 1591459044.251401, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459028.812815}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.532626323 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.19012242443\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:24 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3240a6b4-3c22-4811-87e9-28837793a577-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.14112663269043, \"sum\": 43.14112663269043, \"min\": 43.14112663269043}}, \"EndTime\": 1591459044.295152, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459044.251476}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:29 INFO 140027608049472] Epoch[16] Batch[0] avg_epoch_loss=1.214624\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=1.21462408102\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] Epoch[16] Batch[5] avg_epoch_loss=1.194713\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.19471275282\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] Epoch[16] Batch [5]#011Speed: 200.78 samples/sec#011loss=1.194713\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Epoch[16] Batch[10] avg_epoch_loss=1.188260\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.18051635454\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Epoch[16] Batch [10]#011Speed: 175.07 samples/sec#011loss=1.180516\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] processed a total of 2125 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16584.659099578857, \"sum\": 16584.659099578857, \"min\": 16584.659099578857}}, \"EndTime\": 1591459060.879944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459044.295219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.129572208 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.18825984451\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_6f2b3347-1105-40a5-b540-cd0cb9da95e2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.81998825073242, \"sum\": 55.81998825073242, \"min\": 55.81998825073242}}, \"EndTime\": 1591459060.936331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459060.880021}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] Epoch[16] Batch[5] avg_epoch_loss=1.194713\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=1.19471275282\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:34 INFO 140027608049472] Epoch[16] Batch [5]#011Speed: 200.78 samples/sec#011loss=1.194713\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Epoch[16] Batch[10] avg_epoch_loss=1.188260\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=1.18051635454\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Epoch[16] Batch [10]#011Speed: 175.07 samples/sec#011loss=1.180516\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] processed a total of 2125 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16584.659099578857, \"sum\": 16584.659099578857, \"min\": 16584.659099578857}}, \"EndTime\": 1591459060.879944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459044.295219}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.129572208 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=16, train loss <loss>=1.18825984451\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:40 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_6f2b3347-1105-40a5-b540-cd0cb9da95e2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.81998825073242, \"sum\": 55.81998825073242, \"min\": 55.81998825073242}}, \"EndTime\": 1591459060.936331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459060.880021}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:46 INFO 140027608049472] Epoch[17] Batch[0] avg_epoch_loss=1.152542\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.15254218623\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:46 INFO 140027608049472] Epoch[17] Batch[0] avg_epoch_loss=1.152542\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.15254218623\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] Epoch[17] Batch[5] avg_epoch_loss=1.175326\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.17532598148\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] Epoch[17] Batch [5]#011Speed: 200.24 samples/sec#011loss=1.175326\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] Epoch[17] Batch[10] avg_epoch_loss=1.195107\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=1.21884493558\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] Epoch[17] Batch [10]#011Speed: 176.78 samples/sec#011loss=1.218845\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] processed a total of 2121 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16555.859088897705, \"sum\": 16555.859088897705, \"min\": 16555.859088897705}}, \"EndTime\": 1591459077.492322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459060.936395}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.110841126 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.19510732425\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] Epoch[17] Batch[5] avg_epoch_loss=1.175326\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=1.17532598148\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:51 INFO 140027608049472] Epoch[17] Batch [5]#011Speed: 200.24 samples/sec#011loss=1.175326\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] Epoch[17] Batch[10] avg_epoch_loss=1.195107\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=1.21884493558\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] Epoch[17] Batch [10]#011Speed: 176.78 samples/sec#011loss=1.218845\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] processed a total of 2121 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16555.859088897705, \"sum\": 16555.859088897705, \"min\": 16555.859088897705}}, \"EndTime\": 1591459077.492322, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459060.936395}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.110841126 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=17, train loss <loss>=1.19510732425\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:57:57 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:02 INFO 140027608049472] Epoch[18] Batch[0] avg_epoch_loss=1.207270\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.20726992049\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] Epoch[18] Batch[5] avg_epoch_loss=1.177060\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.17705998931\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] Epoch[18] Batch [5]#011Speed: 199.94 samples/sec#011loss=1.177060\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:02 INFO 140027608049472] Epoch[18] Batch[0] avg_epoch_loss=1.207270\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=1.20726992049\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] Epoch[18] Batch[5] avg_epoch_loss=1.177060\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=1.17705998931\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:08 INFO 140027608049472] Epoch[18] Batch [5]#011Speed: 199.94 samples/sec#011loss=1.177060\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] Epoch[18] Batch[10] avg_epoch_loss=1.194762\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=1.2160036339\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] Epoch[18] Batch [10]#011Speed: 175.76 samples/sec#011loss=1.216004\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] processed a total of 2176 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16610.177993774414, \"sum\": 16610.177993774414, \"min\": 16610.177993774414}}, \"EndTime\": 1591459094.103048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459077.492401}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.003108229 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.19476164594\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:19 INFO 140027608049472] Epoch[19] Batch[0] avg_epoch_loss=1.186185\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.18618544093\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] Epoch[18] Batch[10] avg_epoch_loss=1.194762\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=1.2160036339\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] Epoch[18] Batch [10]#011Speed: 175.76 samples/sec#011loss=1.216004\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] processed a total of 2176 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16610.177993774414, \"sum\": 16610.177993774414, \"min\": 16610.177993774414}}, \"EndTime\": 1591459094.103048, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459077.492401}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.003108229 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] #quality_metric: host=algo-1, epoch=18, train loss <loss>=1.19476164594\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:14 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:19 INFO 140027608049472] Epoch[19] Batch[0] avg_epoch_loss=1.186185\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=1.18618544093\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] Epoch[19] Batch[5] avg_epoch_loss=1.170088\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.17008825818\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] Epoch[19] Batch [5]#011Speed: 199.71 samples/sec#011loss=1.170088\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Epoch[19] Batch[10] avg_epoch_loss=1.171600\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.17341445347\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Epoch[19] Batch [10]#011Speed: 175.11 samples/sec#011loss=1.173414\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16639.988899230957, \"sum\": 16639.988899230957, \"min\": 16639.988899230957}}, \"EndTime\": 1591459110.743576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459094.103128}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.003976062 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.17160016513\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_7f8653f6-320a-48a6-8836-bee2e67b9c20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.847084045410156, \"sum\": 43.847084045410156, \"min\": 43.847084045410156}}, \"EndTime\": 1591459110.788008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459110.743659}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] Epoch[19] Batch[5] avg_epoch_loss=1.170088\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=1.17008825818\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:24 INFO 140027608049472] Epoch[19] Batch [5]#011Speed: 199.71 samples/sec#011loss=1.170088\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Epoch[19] Batch[10] avg_epoch_loss=1.171600\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.17341445347\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Epoch[19] Batch [10]#011Speed: 175.11 samples/sec#011loss=1.173414\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16639.988899230957, \"sum\": 16639.988899230957, \"min\": 16639.988899230957}}, \"EndTime\": 1591459110.743576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459094.103128}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.003976062 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=19, train loss <loss>=1.17160016513\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:30 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_7f8653f6-320a-48a6-8836-bee2e67b9c20-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.847084045410156, \"sum\": 43.847084045410156, \"min\": 43.847084045410156}}, \"EndTime\": 1591459110.788008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459110.743659}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:36 INFO 140027608049472] Epoch[20] Batch[0] avg_epoch_loss=1.176608\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.1766081756\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] Epoch[20] Batch[5] avg_epoch_loss=1.170182\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.17018170627\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] Epoch[20] Batch [5]#011Speed: 199.93 samples/sec#011loss=1.170182\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:36 INFO 140027608049472] Epoch[20] Batch[0] avg_epoch_loss=1.176608\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=1.1766081756\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] Epoch[20] Batch[5] avg_epoch_loss=1.170182\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=1.17018170627\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:41 INFO 140027608049472] Epoch[20] Batch [5]#011Speed: 199.93 samples/sec#011loss=1.170182\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] processed a total of 2116 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15541.898012161255, \"sum\": 15541.898012161255, \"min\": 15541.898012161255}}, \"EndTime\": 1591459126.33003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459110.788069}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.147284526 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.16844367261\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_cc4ad1cf-4924-4444-878a-9facb9008bff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.81187057495117, \"sum\": 42.81187057495117, \"min\": 42.81187057495117}}, \"EndTime\": 1591459126.373396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459126.330091}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:51 INFO 140027608049472] Epoch[21] Batch[0] avg_epoch_loss=1.193545\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=1.19354507158\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] processed a total of 2116 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15541.898012161255, \"sum\": 15541.898012161255, \"min\": 15541.898012161255}}, \"EndTime\": 1591459126.33003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459110.788069}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=136.147284526 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=20, train loss <loss>=1.16844367261\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:46 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_cc4ad1cf-4924-4444-878a-9facb9008bff-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.81187057495117, \"sum\": 42.81187057495117, \"min\": 42.81187057495117}}, \"EndTime\": 1591459126.373396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459126.330091}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:51 INFO 140027608049472] Epoch[21] Batch[0] avg_epoch_loss=1.193545\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=1.19354507158\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] Epoch[21] Batch[5] avg_epoch_loss=1.158965\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.1589650568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] Epoch[21] Batch [5]#011Speed: 198.20 samples/sec#011loss=1.158965\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] processed a total of 2086 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15569.561958312988, \"sum\": 15569.561958312988, \"min\": 15569.561958312988}}, \"EndTime\": 1591459141.943084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459126.373465}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.978347871 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.14807861616\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:02 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_e6f22d1b-e5fa-4fde-a6d0-a02e666e1226-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.35590171813965, \"sum\": 60.35590171813965, \"min\": 60.35590171813965}}, \"EndTime\": 1591459142.004035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459141.943165}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] Epoch[21] Batch[5] avg_epoch_loss=1.158965\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=1.1589650568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:58:56 INFO 140027608049472] Epoch[21] Batch [5]#011Speed: 198.20 samples/sec#011loss=1.158965\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] processed a total of 2086 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15569.561958312988, \"sum\": 15569.561958312988, \"min\": 15569.561958312988}}, \"EndTime\": 1591459141.943084, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459126.373465}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.978347871 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=21, train loss <loss>=1.14807861616\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:02 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_e6f22d1b-e5fa-4fde-a6d0-a02e666e1226-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.35590171813965, \"sum\": 60.35590171813965, \"min\": 60.35590171813965}}, \"EndTime\": 1591459142.004035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459141.943165}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:07 INFO 140027608049472] Epoch[22] Batch[0] avg_epoch_loss=1.148855\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.14885495744\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:07 INFO 140027608049472] Epoch[22] Batch[0] avg_epoch_loss=1.148855\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=1.14885495744\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] Epoch[22] Batch[5] avg_epoch_loss=1.148568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.1485678115\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] Epoch[22] Batch [5]#011Speed: 199.84 samples/sec#011loss=1.148568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] Epoch[22] Batch[10] avg_epoch_loss=1.170101\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=1.19594011127\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] Epoch[22] Batch [10]#011Speed: 176.27 samples/sec#011loss=1.195940\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] processed a total of 2148 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16596.373081207275, \"sum\": 16596.373081207275, \"min\": 16596.373081207275}}, \"EndTime\": 1591459158.600549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459142.004107}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.425018809 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.17010067503\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] Epoch[22] Batch[5] avg_epoch_loss=1.148568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=1.1485678115\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:12 INFO 140027608049472] Epoch[22] Batch [5]#011Speed: 199.84 samples/sec#011loss=1.148568\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] Epoch[22] Batch[10] avg_epoch_loss=1.170101\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=1.19594011127\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] Epoch[22] Batch [10]#011Speed: 176.27 samples/sec#011loss=1.195940\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] processed a total of 2148 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16596.373081207275, \"sum\": 16596.373081207275, \"min\": 16596.373081207275}}, \"EndTime\": 1591459158.600549, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459142.004107}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.425018809 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=22, train loss <loss>=1.17010067503\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:18 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:23 INFO 140027608049472] Epoch[23] Batch[0] avg_epoch_loss=1.148513\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.14851329012\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] Epoch[23] Batch[5] avg_epoch_loss=1.137578\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.13757800456\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] Epoch[23] Batch [5]#011Speed: 198.02 samples/sec#011loss=1.137578\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:23 INFO 140027608049472] Epoch[23] Batch[0] avg_epoch_loss=1.148513\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=1.14851329012\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] Epoch[23] Batch[5] avg_epoch_loss=1.137578\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=1.13757800456\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:29 INFO 140027608049472] Epoch[23] Batch [5]#011Speed: 198.02 samples/sec#011loss=1.137578\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] processed a total of 2055 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15536.777973175049, \"sum\": 15536.777973175049, \"min\": 15536.777973175049}}, \"EndTime\": 1591459174.137918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459158.600619}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.265162096 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.14302105454\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_e2c85eed-3b9f-415f-a304-4d95abb7bfcf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.252044677734375, \"sum\": 62.252044677734375, \"min\": 62.252044677734375}}, \"EndTime\": 1591459174.201078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459174.138003}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:39 INFO 140027608049472] Epoch[24] Batch[0] avg_epoch_loss=1.150347\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.15034736777\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] processed a total of 2055 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15536.777973175049, \"sum\": 15536.777973175049, \"min\": 15536.777973175049}}, \"EndTime\": 1591459174.137918, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459158.600619}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.265162096 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=23, train loss <loss>=1.14302105454\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:34 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_e2c85eed-3b9f-415f-a304-4d95abb7bfcf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 62.252044677734375, \"sum\": 62.252044677734375, \"min\": 62.252044677734375}}, \"EndTime\": 1591459174.201078, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459174.138003}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:39 INFO 140027608049472] Epoch[24] Batch[0] avg_epoch_loss=1.150347\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=1.15034736777\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] Epoch[24] Batch[5] avg_epoch_loss=1.136862\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.13686219701\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] Epoch[24] Batch [5]#011Speed: 198.97 samples/sec#011loss=1.136862\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] Epoch[24] Batch[10] avg_epoch_loss=1.157146\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=1.18148639607\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] Epoch[24] Batch [10]#011Speed: 175.54 samples/sec#011loss=1.181486\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] processed a total of 2153 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16633.2790851593, \"sum\": 16633.2790851593, \"min\": 16633.2790851593}}, \"EndTime\": 1591459190.834492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459174.201149}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.438546638 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.15714592386\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] Epoch[24] Batch[5] avg_epoch_loss=1.136862\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=1.13686219701\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:44 INFO 140027608049472] Epoch[24] Batch [5]#011Speed: 198.97 samples/sec#011loss=1.136862\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] Epoch[24] Batch[10] avg_epoch_loss=1.157146\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=1.18148639607\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] Epoch[24] Batch [10]#011Speed: 175.54 samples/sec#011loss=1.181486\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] processed a total of 2153 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16633.2790851593, \"sum\": 16633.2790851593, \"min\": 16633.2790851593}}, \"EndTime\": 1591459190.834492, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459174.201149}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.438546638 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=24, train loss <loss>=1.15714592386\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:50 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:56 INFO 140027608049472] Epoch[25] Batch[0] avg_epoch_loss=1.115018\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.11501801689\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] Epoch[25] Batch[5] avg_epoch_loss=1.118882\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.11888183738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] Epoch[25] Batch [5]#011Speed: 198.68 samples/sec#011loss=1.118882\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:56 INFO 140027608049472] Epoch[25] Batch[0] avg_epoch_loss=1.115018\u001b[0m\n",
      "\u001b[34m[06/06/2020 15:59:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=1.11501801689\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] Epoch[25] Batch[5] avg_epoch_loss=1.118882\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=1.11888183738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:01 INFO 140027608049472] Epoch[25] Batch [5]#011Speed: 198.68 samples/sec#011loss=1.118882\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Epoch[25] Batch[10] avg_epoch_loss=1.133991\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=1.15212107245\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Epoch[25] Batch [10]#011Speed: 174.79 samples/sec#011loss=1.152121\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] processed a total of 2165 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16713.016033172607, \"sum\": 16713.016033172607, \"min\": 16713.016033172607}}, \"EndTime\": 1591459207.548012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459190.834555}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.538910361 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.13399058059\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c13e6ff3-4b83-4355-a73e-ed08e232d5ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 49.916982650756836, \"sum\": 49.916982650756836, \"min\": 49.916982650756836}}, \"EndTime\": 1591459207.598462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459207.548087}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:12 INFO 140027608049472] Epoch[26] Batch[0] avg_epoch_loss=1.119013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.1190126527\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Epoch[25] Batch[10] avg_epoch_loss=1.133991\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=1.15212107245\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Epoch[25] Batch [10]#011Speed: 174.79 samples/sec#011loss=1.152121\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] processed a total of 2165 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16713.016033172607, \"sum\": 16713.016033172607, \"min\": 16713.016033172607}}, \"EndTime\": 1591459207.548012, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459190.834555}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.538910361 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=25, train loss <loss>=1.13399058059\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:07 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c13e6ff3-4b83-4355-a73e-ed08e232d5ef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 49.916982650756836, \"sum\": 49.916982650756836, \"min\": 49.916982650756836}}, \"EndTime\": 1591459207.598462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459207.548087}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:12 INFO 140027608049472] Epoch[26] Batch[0] avg_epoch_loss=1.119013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=1.1190126527\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] Epoch[26] Batch[5] avg_epoch_loss=1.126184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.12618403165\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] Epoch[26] Batch [5]#011Speed: 199.49 samples/sec#011loss=1.126184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] Epoch[26] Batch[5] avg_epoch_loss=1.126184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=1.12618403165\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:18 INFO 140027608049472] Epoch[26] Batch [5]#011Speed: 199.49 samples/sec#011loss=1.126184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15536.264896392822, \"sum\": 15536.264896392822, \"min\": 15536.264896392822}}, \"EndTime\": 1591459223.134854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459207.598533}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.488438341 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.11749479186\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c4ab1def-b1e1-4eb5-8307-0a857df34a54-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 69.14615631103516, \"sum\": 69.14615631103516, \"min\": 69.14615631103516}}, \"EndTime\": 1591459223.204587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459223.134935}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:28 INFO 140027608049472] Epoch[27] Batch[0] avg_epoch_loss=1.056112\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.05611167764\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15536.264896392822, \"sum\": 15536.264896392822, \"min\": 15536.264896392822}}, \"EndTime\": 1591459223.134854, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459207.598533}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.488438341 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=26, train loss <loss>=1.11749479186\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c4ab1def-b1e1-4eb5-8307-0a857df34a54-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 69.14615631103516, \"sum\": 69.14615631103516, \"min\": 69.14615631103516}}, \"EndTime\": 1591459223.204587, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459223.134935}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:28 INFO 140027608049472] Epoch[27] Batch[0] avg_epoch_loss=1.056112\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=1.05611167764\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] Epoch[27] Batch[5] avg_epoch_loss=1.109092\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.10909201664\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] Epoch[27] Batch [5]#011Speed: 200.02 samples/sec#011loss=1.109092\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] processed a total of 2111 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15583.442211151123, \"sum\": 15583.442211151123, \"min\": 15583.442211151123}}, \"EndTime\": 1591459238.78818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459223.204663}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.463251702 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.10945923283\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_b691b64a-453a-48f0-9d88-16de2e736ad7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.413949966430664, \"sum\": 42.413949966430664, \"min\": 42.413949966430664}}, \"EndTime\": 1591459238.83123, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459238.788252}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] Epoch[27] Batch[5] avg_epoch_loss=1.109092\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=1.10909201664\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:33 INFO 140027608049472] Epoch[27] Batch [5]#011Speed: 200.02 samples/sec#011loss=1.109092\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] processed a total of 2111 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15583.442211151123, \"sum\": 15583.442211151123, \"min\": 15583.442211151123}}, \"EndTime\": 1591459238.78818, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459223.204663}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.463251702 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=27, train loss <loss>=1.10945923283\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:38 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_b691b64a-453a-48f0-9d88-16de2e736ad7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.413949966430664, \"sum\": 42.413949966430664, \"min\": 42.413949966430664}}, \"EndTime\": 1591459238.83123, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459238.788252}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:44 INFO 140027608049472] Epoch[28] Batch[0] avg_epoch_loss=1.130609\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.13060918844\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] Epoch[28] Batch[5] avg_epoch_loss=1.109097\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.10909707891\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] Epoch[28] Batch [5]#011Speed: 199.29 samples/sec#011loss=1.109097\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:44 INFO 140027608049472] Epoch[28] Batch[0] avg_epoch_loss=1.130609\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=1.13060918844\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] Epoch[28] Batch[5] avg_epoch_loss=1.109097\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=1.10909707891\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:49 INFO 140027608049472] Epoch[28] Batch [5]#011Speed: 199.29 samples/sec#011loss=1.109097\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] Epoch[28] Batch[10] avg_epoch_loss=1.122218\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=1.13796290272\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] Epoch[28] Batch [10]#011Speed: 175.07 samples/sec#011loss=1.137963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] processed a total of 2133 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16643.908977508545, \"sum\": 16643.908977508545, \"min\": 16643.908977508545}}, \"EndTime\": 1591459255.475278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459238.831305}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.154111355 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.12221790791\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] Epoch[28] Batch[10] avg_epoch_loss=1.122218\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=1.13796290272\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] Epoch[28] Batch [10]#011Speed: 175.07 samples/sec#011loss=1.137963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] processed a total of 2133 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16643.908977508545, \"sum\": 16643.908977508545, \"min\": 16643.908977508545}}, \"EndTime\": 1591459255.475278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459238.831305}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.154111355 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=28, train loss <loss>=1.12221790791\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:00:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:00 INFO 140027608049472] Epoch[29] Batch[0] avg_epoch_loss=1.113467\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.11346694658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] Epoch[29] Batch[5] avg_epoch_loss=1.108510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.10851014335\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] Epoch[29] Batch [5]#011Speed: 199.46 samples/sec#011loss=1.108510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:00 INFO 140027608049472] Epoch[29] Batch[0] avg_epoch_loss=1.113467\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=1.11346694658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] Epoch[29] Batch[5] avg_epoch_loss=1.108510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=1.10851014335\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:06 INFO 140027608049472] Epoch[29] Batch [5]#011Speed: 199.46 samples/sec#011loss=1.108510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Epoch[29] Batch[10] avg_epoch_loss=1.067975\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=1.01933218038\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Epoch[29] Batch [10]#011Speed: 174.57 samples/sec#011loss=1.019332\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] processed a total of 2180 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16722.84507751465, \"sum\": 16722.84507751465, \"min\": 16722.84507751465}}, \"EndTime\": 1591459272.198661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459255.475356}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.359814212 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.06797470564\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_5c1b9613-807b-4a76-ac79-c3ce311463ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.9379940032959, \"sum\": 42.9379940032959, \"min\": 42.9379940032959}}, \"EndTime\": 1591459272.242153, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459272.198724}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:17 INFO 140027608049472] Epoch[30] Batch[0] avg_epoch_loss=1.107401\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.1074010741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Epoch[29] Batch[10] avg_epoch_loss=1.067975\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=1.01933218038\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Epoch[29] Batch [10]#011Speed: 174.57 samples/sec#011loss=1.019332\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] processed a total of 2180 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16722.84507751465, \"sum\": 16722.84507751465, \"min\": 16722.84507751465}}, \"EndTime\": 1591459272.198661, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459255.475356}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.359814212 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=29, train loss <loss>=1.06797470564\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:12 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_5c1b9613-807b-4a76-ac79-c3ce311463ca-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.9379940032959, \"sum\": 42.9379940032959, \"min\": 42.9379940032959}}, \"EndTime\": 1591459272.242153, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459272.198724}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:17 INFO 140027608049472] Epoch[30] Batch[0] avg_epoch_loss=1.107401\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=1.1074010741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] Epoch[30] Batch[5] avg_epoch_loss=1.092397\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.09239737793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] Epoch[30] Batch [5]#011Speed: 199.76 samples/sec#011loss=1.092397\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] processed a total of 2103 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15562.577962875366, \"sum\": 15562.577962875366, \"min\": 15562.577962875366}}, \"EndTime\": 1591459287.804867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459272.24223}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.131062336 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.09167181051\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:33 INFO 140027608049472] Epoch[31] Batch[0] avg_epoch_loss=1.119210\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.11921022523\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] Epoch[31] Batch[5] avg_epoch_loss=1.090853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.09085300134\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] Epoch[31] Batch [5]#011Speed: 193.43 samples/sec#011loss=1.090853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] Epoch[30] Batch[5] avg_epoch_loss=1.092397\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=1.09239737793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:22 INFO 140027608049472] Epoch[30] Batch [5]#011Speed: 199.76 samples/sec#011loss=1.092397\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] processed a total of 2103 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15562.577962875366, \"sum\": 15562.577962875366, \"min\": 15562.577962875366}}, \"EndTime\": 1591459287.804867, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459272.24223}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.131062336 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=30, train loss <loss>=1.09167181051\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:27 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:33 INFO 140027608049472] Epoch[31] Batch[0] avg_epoch_loss=1.119210\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=1.11921022523\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] Epoch[31] Batch[5] avg_epoch_loss=1.090853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=1.09085300134\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:38 INFO 140027608049472] Epoch[31] Batch [5]#011Speed: 193.43 samples/sec#011loss=1.090853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] processed a total of 2002 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15738.46983909607, \"sum\": 15738.46983909607, \"min\": 15738.46983909607}}, \"EndTime\": 1591459303.543902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459287.804927}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.203253157 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.10633236147\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:48 INFO 140027608049472] Epoch[32] Batch[0] avg_epoch_loss=1.049327\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.04932720256\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] processed a total of 2002 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15738.46983909607, \"sum\": 15738.46983909607, \"min\": 15738.46983909607}}, \"EndTime\": 1591459303.543902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459287.804927}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.203253157 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=31, train loss <loss>=1.10633236147\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:43 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:48 INFO 140027608049472] Epoch[32] Batch[0] avg_epoch_loss=1.049327\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=1.04932720256\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] Epoch[32] Batch[5] avg_epoch_loss=1.068233\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.06823305814\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] Epoch[32] Batch [5]#011Speed: 200.02 samples/sec#011loss=1.068233\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] processed a total of 2119 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15631.537914276123, \"sum\": 15631.537914276123, \"min\": 15631.537914276123}}, \"EndTime\": 1591459319.175981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459303.543984}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.558236339 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.07705702152\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] Epoch[32] Batch[5] avg_epoch_loss=1.068233\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=1.06823305814\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:54 INFO 140027608049472] Epoch[32] Batch [5]#011Speed: 200.02 samples/sec#011loss=1.068233\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] processed a total of 2119 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15631.537914276123, \"sum\": 15631.537914276123, \"min\": 15631.537914276123}}, \"EndTime\": 1591459319.175981, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459303.543984}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.558236339 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=32, train loss <loss>=1.07705702152\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:01:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:04 INFO 140027608049472] Epoch[33] Batch[0] avg_epoch_loss=1.098220\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.09821974556\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] Epoch[33] Batch[5] avg_epoch_loss=1.075757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.07575696069\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] Epoch[33] Batch [5]#011Speed: 198.66 samples/sec#011loss=1.075757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:04 INFO 140027608049472] Epoch[33] Batch[0] avg_epoch_loss=1.098220\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=1.09821974556\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] Epoch[33] Batch[5] avg_epoch_loss=1.075757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=1.07575696069\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:09 INFO 140027608049472] Epoch[33] Batch [5]#011Speed: 198.66 samples/sec#011loss=1.075757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] Epoch[33] Batch[10] avg_epoch_loss=1.072805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=1.06926237862\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] Epoch[33] Batch [10]#011Speed: 174.95 samples/sec#011loss=1.069262\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] processed a total of 2205 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16723.48690032959, \"sum\": 16723.48690032959, \"min\": 16723.48690032959}}, \"EndTime\": 1591459335.900079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459319.176064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.849617502 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.07280487793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:21 INFO 140027608049472] Epoch[34] Batch[0] avg_epoch_loss=1.087104\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.08710364576\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] Epoch[33] Batch[10] avg_epoch_loss=1.072805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=1.06926237862\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] Epoch[33] Batch [10]#011Speed: 174.95 samples/sec#011loss=1.069262\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] processed a total of 2205 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16723.48690032959, \"sum\": 16723.48690032959, \"min\": 16723.48690032959}}, \"EndTime\": 1591459335.900079, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459319.176064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.849617502 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=33, train loss <loss>=1.07280487793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:15 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:21 INFO 140027608049472] Epoch[34] Batch[0] avg_epoch_loss=1.087104\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=1.08710364576\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] Epoch[34] Batch[5] avg_epoch_loss=1.082534\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.08253367442\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] Epoch[34] Batch [5]#011Speed: 200.14 samples/sec#011loss=1.082534\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] processed a total of 2112 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15571.735858917236, \"sum\": 15571.735858917236, \"min\": 15571.735858917236}}, \"EndTime\": 1591459351.472323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459335.900155}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.62953072 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.08211043736\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] Epoch[34] Batch[5] avg_epoch_loss=1.082534\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=1.08253367442\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:26 INFO 140027608049472] Epoch[34] Batch [5]#011Speed: 200.14 samples/sec#011loss=1.082534\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] processed a total of 2112 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15571.735858917236, \"sum\": 15571.735858917236, \"min\": 15571.735858917236}}, \"EndTime\": 1591459351.472323, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459335.900155}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.62953072 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=34, train loss <loss>=1.08211043736\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:31 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:36 INFO 140027608049472] Epoch[35] Batch[0] avg_epoch_loss=1.063703\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.06370299717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] Epoch[35] Batch[5] avg_epoch_loss=1.054805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.05480465199\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] Epoch[35] Batch [5]#011Speed: 200.25 samples/sec#011loss=1.054805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:36 INFO 140027608049472] Epoch[35] Batch[0] avg_epoch_loss=1.063703\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=1.06370299717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] Epoch[35] Batch[5] avg_epoch_loss=1.054805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=1.05480465199\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:42 INFO 140027608049472] Epoch[35] Batch [5]#011Speed: 200.25 samples/sec#011loss=1.054805\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] Epoch[35] Batch[10] avg_epoch_loss=1.081883\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=1.11437619767\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] Epoch[35] Batch [10]#011Speed: 176.89 samples/sec#011loss=1.114376\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] processed a total of 2183 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16592.880964279175, \"sum\": 16592.880964279175, \"min\": 16592.880964279175}}, \"EndTime\": 1591459368.065745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459351.472384}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.56138326 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.0818826273\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:53 INFO 140027608049472] Epoch[36] Batch[0] avg_epoch_loss=1.091679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.09167912321\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] Epoch[35] Batch[10] avg_epoch_loss=1.081883\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=1.11437619767\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] Epoch[35] Batch [10]#011Speed: 176.89 samples/sec#011loss=1.114376\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] processed a total of 2183 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16592.880964279175, \"sum\": 16592.880964279175, \"min\": 16592.880964279175}}, \"EndTime\": 1591459368.065745, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459351.472384}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.56138326 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=35, train loss <loss>=1.0818826273\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:48 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:53 INFO 140027608049472] Epoch[36] Batch[0] avg_epoch_loss=1.091679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=1.09167912321\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] Epoch[36] Batch[5] avg_epoch_loss=1.066118\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.06611843229\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] Epoch[36] Batch [5]#011Speed: 198.89 samples/sec#011loss=1.066118\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Epoch[36] Batch[10] avg_epoch_loss=1.045323\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.02036911587\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Epoch[36] Batch [10]#011Speed: 175.50 samples/sec#011loss=1.020369\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] processed a total of 2175 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16684.003114700317, \"sum\": 16684.003114700317, \"min\": 16684.003114700317}}, \"EndTime\": 1591459384.750328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459368.065807}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.363498734 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.04532328846\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ffe15e40-25e3-47b6-a47d-2ff5fde776b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.87600517272949, \"sum\": 42.87600517272949, \"min\": 42.87600517272949}}, \"EndTime\": 1591459384.793846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459384.750405}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] Epoch[36] Batch[5] avg_epoch_loss=1.066118\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=1.06611843229\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:02:58 INFO 140027608049472] Epoch[36] Batch [5]#011Speed: 198.89 samples/sec#011loss=1.066118\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Epoch[36] Batch[10] avg_epoch_loss=1.045323\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=1.02036911587\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Epoch[36] Batch [10]#011Speed: 175.50 samples/sec#011loss=1.020369\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] processed a total of 2175 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16684.003114700317, \"sum\": 16684.003114700317, \"min\": 16684.003114700317}}, \"EndTime\": 1591459384.750328, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459368.065807}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.363498734 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=36, train loss <loss>=1.04532328846\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ffe15e40-25e3-47b6-a47d-2ff5fde776b0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.87600517272949, \"sum\": 42.87600517272949, \"min\": 42.87600517272949}}, \"EndTime\": 1591459384.793846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459384.750405}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:10 INFO 140027608049472] Epoch[37] Batch[0] avg_epoch_loss=1.079526\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.07952578563\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:10 INFO 140027608049472] Epoch[37] Batch[0] avg_epoch_loss=1.079526\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=1.07952578563\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] Epoch[37] Batch[5] avg_epoch_loss=1.057169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.05716927246\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] Epoch[37] Batch [5]#011Speed: 198.25 samples/sec#011loss=1.057169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] processed a total of 2052 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15665.724039077759, \"sum\": 15665.724039077759, \"min\": 15665.724039077759}}, \"EndTime\": 1591459400.459716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459384.793926}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.985225081 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.06127842597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] Epoch[37] Batch[5] avg_epoch_loss=1.057169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=1.05716927246\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:15 INFO 140027608049472] Epoch[37] Batch [5]#011Speed: 198.25 samples/sec#011loss=1.057169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] processed a total of 2052 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15665.724039077759, \"sum\": 15665.724039077759, \"min\": 15665.724039077759}}, \"EndTime\": 1591459400.459716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459384.793926}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.985225081 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=37, train loss <loss>=1.06127842597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:20 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:25 INFO 140027608049472] Epoch[38] Batch[0] avg_epoch_loss=1.038355\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:25 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.03835519755\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] Epoch[38] Batch[5] avg_epoch_loss=1.045030\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.04502951424\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] Epoch[38] Batch [5]#011Speed: 197.41 samples/sec#011loss=1.045030\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:25 INFO 140027608049472] Epoch[38] Batch[0] avg_epoch_loss=1.038355\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:25 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=1.03835519755\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] Epoch[38] Batch[5] avg_epoch_loss=1.045030\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=1.04502951424\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:31 INFO 140027608049472] Epoch[38] Batch [5]#011Speed: 197.41 samples/sec#011loss=1.045030\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] processed a total of 2103 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15714.126110076904, \"sum\": 15714.126110076904, \"min\": 15714.126110076904}}, \"EndTime\": 1591459416.174479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459400.459841}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.827602529 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.04447497962\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_05a93ddf-4c8f-4154-b467-0c360fc9643f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.623924255371094, \"sum\": 43.623924255371094, \"min\": 43.623924255371094}}, \"EndTime\": 1591459416.218769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459416.174561}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:41 INFO 140027608049472] Epoch[39] Batch[0] avg_epoch_loss=1.040774\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.04077414747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] processed a total of 2103 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15714.126110076904, \"sum\": 15714.126110076904, \"min\": 15714.126110076904}}, \"EndTime\": 1591459416.174479, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459400.459841}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.827602529 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=38, train loss <loss>=1.04447497962\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:36 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_05a93ddf-4c8f-4154-b467-0c360fc9643f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.623924255371094, \"sum\": 43.623924255371094, \"min\": 43.623924255371094}}, \"EndTime\": 1591459416.218769, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459416.174561}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:41 INFO 140027608049472] Epoch[39] Batch[0] avg_epoch_loss=1.040774\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=1.04077414747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] Epoch[39] Batch[5] avg_epoch_loss=1.043346\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.043346489\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] Epoch[39] Batch [5]#011Speed: 198.23 samples/sec#011loss=1.043346\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] processed a total of 2079 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15604.280948638916, \"sum\": 15604.280948638916, \"min\": 15604.280948638916}}, \"EndTime\": 1591459431.823174, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459416.218831}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.231645155 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.0419069686\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_cff4ba38-4d20-4dba-bc0a-91ee313303a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.309926986694336, \"sum\": 43.309926986694336, \"min\": 43.309926986694336}}, \"EndTime\": 1591459431.867147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459431.823256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] Epoch[39] Batch[5] avg_epoch_loss=1.043346\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=1.043346489\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:46 INFO 140027608049472] Epoch[39] Batch [5]#011Speed: 198.23 samples/sec#011loss=1.043346\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] processed a total of 2079 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15604.280948638916, \"sum\": 15604.280948638916, \"min\": 15604.280948638916}}, \"EndTime\": 1591459431.823174, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459416.218831}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.231645155 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=39, train loss <loss>=1.0419069686\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:51 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_cff4ba38-4d20-4dba-bc0a-91ee313303a0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.309926986694336, \"sum\": 43.309926986694336, \"min\": 43.309926986694336}}, \"EndTime\": 1591459431.867147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459431.823256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:57 INFO 140027608049472] Epoch[40] Batch[0] avg_epoch_loss=1.057163\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.05716323853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] Epoch[40] Batch[5] avg_epoch_loss=1.035916\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.03591567465\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] Epoch[40] Batch [5]#011Speed: 198.65 samples/sec#011loss=1.035916\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:57 INFO 140027608049472] Epoch[40] Batch[0] avg_epoch_loss=1.057163\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:03:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=1.05716323853\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] Epoch[40] Batch[5] avg_epoch_loss=1.035916\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=1.03591567465\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:02 INFO 140027608049472] Epoch[40] Batch [5]#011Speed: 198.65 samples/sec#011loss=1.035916\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Epoch[40] Batch[10] avg_epoch_loss=1.031392\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=1.02596261366\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Epoch[40] Batch [10]#011Speed: 176.13 samples/sec#011loss=1.025963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] processed a total of 2179 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16637.43281364441, \"sum\": 16637.43281364441, \"min\": 16637.43281364441}}, \"EndTime\": 1591459448.504694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459431.867198}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.968841001 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.03139155602\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c289b794-6f98-4498-87d1-309afefa543f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.61081314086914, \"sum\": 66.61081314086914, \"min\": 66.61081314086914}}, \"EndTime\": 1591459448.57191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459448.50477}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:13 INFO 140027608049472] Epoch[41] Batch[0] avg_epoch_loss=1.020718\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:13 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.0207183406\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Epoch[40] Batch[10] avg_epoch_loss=1.031392\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=1.02596261366\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Epoch[40] Batch [10]#011Speed: 176.13 samples/sec#011loss=1.025963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] processed a total of 2179 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16637.43281364441, \"sum\": 16637.43281364441, \"min\": 16637.43281364441}}, \"EndTime\": 1591459448.504694, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459431.867198}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.968841001 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=40, train loss <loss>=1.03139155602\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:08 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c289b794-6f98-4498-87d1-309afefa543f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.61081314086914, \"sum\": 66.61081314086914, \"min\": 66.61081314086914}}, \"EndTime\": 1591459448.57191, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459448.50477}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:13 INFO 140027608049472] Epoch[41] Batch[0] avg_epoch_loss=1.020718\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:13 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=1.0207183406\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] Epoch[41] Batch[5] avg_epoch_loss=1.040741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.04074119472\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] Epoch[41] Batch [5]#011Speed: 199.34 samples/sec#011loss=1.040741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] processed a total of 2065 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15522.848129272461, \"sum\": 15522.848129272461, \"min\": 15522.848129272461}}, \"EndTime\": 1591459464.094895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459448.571984}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.02865871 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.00899540883\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ae11fdf6-fc70-4c91-9f20-b83cb787626a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 51.90300941467285, \"sum\": 51.90300941467285, \"min\": 51.90300941467285}}, \"EndTime\": 1591459464.147393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459464.09498}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] Epoch[41] Batch[5] avg_epoch_loss=1.040741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=1.04074119472\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:19 INFO 140027608049472] Epoch[41] Batch [5]#011Speed: 199.34 samples/sec#011loss=1.040741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] processed a total of 2065 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15522.848129272461, \"sum\": 15522.848129272461, \"min\": 15522.848129272461}}, \"EndTime\": 1591459464.094895, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459448.571984}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.02865871 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=41, train loss <loss>=1.00899540883\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:24 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ae11fdf6-fc70-4c91-9f20-b83cb787626a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 51.90300941467285, \"sum\": 51.90300941467285, \"min\": 51.90300941467285}}, \"EndTime\": 1591459464.147393, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459464.09498}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:29 INFO 140027608049472] Epoch[42] Batch[0] avg_epoch_loss=1.026775\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.02677543208\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] Epoch[42] Batch[5] avg_epoch_loss=1.028091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.02809141867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] Epoch[42] Batch [5]#011Speed: 199.43 samples/sec#011loss=1.028091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:29 INFO 140027608049472] Epoch[42] Batch[0] avg_epoch_loss=1.026775\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=1.02677543208\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] Epoch[42] Batch[5] avg_epoch_loss=1.028091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=1.02809141867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:34 INFO 140027608049472] Epoch[42] Batch [5]#011Speed: 199.43 samples/sec#011loss=1.028091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] Epoch[42] Batch[10] avg_epoch_loss=1.031026\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=1.03454697807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] Epoch[42] Batch [10]#011Speed: 175.13 samples/sec#011loss=1.034547\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] processed a total of 2134 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16726.534128189087, \"sum\": 16726.534128189087, \"min\": 16726.534128189087}}, \"EndTime\": 1591459480.874054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459464.147459}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.580882035 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.03102576385\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] Epoch[42] Batch[10] avg_epoch_loss=1.031026\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=1.03454697807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] Epoch[42] Batch [10]#011Speed: 175.13 samples/sec#011loss=1.034547\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] processed a total of 2134 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16726.534128189087, \"sum\": 16726.534128189087, \"min\": 16726.534128189087}}, \"EndTime\": 1591459480.874054, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459464.147459}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.580882035 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=42, train loss <loss>=1.03102576385\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:40 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:46 INFO 140027608049472] Epoch[43] Batch[0] avg_epoch_loss=1.023022\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.02302184195\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] Epoch[43] Batch[5] avg_epoch_loss=1.029510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.02950982628\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] Epoch[43] Batch [5]#011Speed: 198.99 samples/sec#011loss=1.029510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:46 INFO 140027608049472] Epoch[43] Batch[0] avg_epoch_loss=1.023022\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=1.02302184195\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] Epoch[43] Batch[5] avg_epoch_loss=1.029510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=1.02950982628\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:51 INFO 140027608049472] Epoch[43] Batch [5]#011Speed: 198.99 samples/sec#011loss=1.029510\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] Epoch[43] Batch[10] avg_epoch_loss=1.038888\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=1.05014249694\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] Epoch[43] Batch [10]#011Speed: 174.56 samples/sec#011loss=1.050142\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] processed a total of 2173 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16821.19393348694, \"sum\": 16821.19393348694, \"min\": 16821.19393348694}}, \"EndTime\": 1591459497.69575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459480.874129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.181485363 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.03888831294\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:03 INFO 140027608049472] Epoch[44] Batch[0] avg_epoch_loss=1.000271\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.00027077153\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] Epoch[43] Batch[10] avg_epoch_loss=1.038888\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=1.05014249694\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] Epoch[43] Batch [10]#011Speed: 174.56 samples/sec#011loss=1.050142\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] processed a total of 2173 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16821.19393348694, \"sum\": 16821.19393348694, \"min\": 16821.19393348694}}, \"EndTime\": 1591459497.69575, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459480.874129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.181485363 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=43, train loss <loss>=1.03888831294\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:04:57 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:03 INFO 140027608049472] Epoch[44] Batch[0] avg_epoch_loss=1.000271\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=1.00027077153\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] Epoch[44] Batch[5] avg_epoch_loss=1.017933\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.0179333237\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] Epoch[44] Batch [5]#011Speed: 198.45 samples/sec#011loss=1.017933\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] processed a total of 2087 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15643.866062164307, \"sum\": 15643.866062164307, \"min\": 15643.866062164307}}, \"EndTime\": 1591459513.340122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459497.695817}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.405932637 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.01848979446\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] Epoch[44] Batch[5] avg_epoch_loss=1.017933\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=1.0179333237\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:08 INFO 140027608049472] Epoch[44] Batch [5]#011Speed: 198.45 samples/sec#011loss=1.017933\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] processed a total of 2087 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15643.866062164307, \"sum\": 15643.866062164307, \"min\": 15643.866062164307}}, \"EndTime\": 1591459513.340122, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459497.695817}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.405932637 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] #quality_metric: host=algo-1, epoch=44, train loss <loss>=1.01848979446\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:13 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:18 INFO 140027608049472] Epoch[45] Batch[0] avg_epoch_loss=1.006650\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.00665009697\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] Epoch[45] Batch[5] avg_epoch_loss=1.007720\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.00771954374\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] Epoch[45] Batch [5]#011Speed: 197.81 samples/sec#011loss=1.007720\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:18 INFO 140027608049472] Epoch[45] Batch[0] avg_epoch_loss=1.006650\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=1.00665009697\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] Epoch[45] Batch[5] avg_epoch_loss=1.007720\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=1.00771954374\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:24 INFO 140027608049472] Epoch[45] Batch [5]#011Speed: 197.81 samples/sec#011loss=1.007720\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] Epoch[45] Batch[10] avg_epoch_loss=1.011343\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=1.01569097267\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] Epoch[45] Batch [10]#011Speed: 174.47 samples/sec#011loss=1.015691\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] processed a total of 2142 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16782.169818878174, \"sum\": 16782.169818878174, \"min\": 16782.169818878174}}, \"EndTime\": 1591459530.122802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459513.340203}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.634609875 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.01134292053\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:35 INFO 140027608049472] Epoch[46] Batch[0] avg_epoch_loss=1.000324\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:35 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.00032360149\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] Epoch[45] Batch[10] avg_epoch_loss=1.011343\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, batch=10 train loss <loss>=1.01569097267\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] Epoch[45] Batch [10]#011Speed: 174.47 samples/sec#011loss=1.015691\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] processed a total of 2142 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16782.169818878174, \"sum\": 16782.169818878174, \"min\": 16782.169818878174}}, \"EndTime\": 1591459530.122802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459513.340203}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.634609875 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=45, train loss <loss>=1.01134292053\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:30 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:35 INFO 140027608049472] Epoch[46] Batch[0] avg_epoch_loss=1.000324\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:35 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=1.00032360149\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] Epoch[46] Batch[5] avg_epoch_loss=1.003138\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.0031378764\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] Epoch[46] Batch [5]#011Speed: 198.45 samples/sec#011loss=1.003138\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] processed a total of 2051 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15600.446939468384, \"sum\": 15600.446939468384, \"min\": 15600.446939468384}}, \"EndTime\": 1591459545.723742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459530.122879}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.469740434 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.00875291644\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_bb33a8ab-a870-417c-984f-15c544601d48-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.9061393737793, \"sum\": 44.9061393737793, \"min\": 44.9061393737793}}, \"EndTime\": 1591459545.769261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459545.723812}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] Epoch[46] Batch[5] avg_epoch_loss=1.003138\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=1.0031378764\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:40 INFO 140027608049472] Epoch[46] Batch [5]#011Speed: 198.45 samples/sec#011loss=1.003138\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] processed a total of 2051 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15600.446939468384, \"sum\": 15600.446939468384, \"min\": 15600.446939468384}}, \"EndTime\": 1591459545.723742, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459530.122879}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.469740434 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=46, train loss <loss>=1.00875291644\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:45 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_bb33a8ab-a870-417c-984f-15c544601d48-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.9061393737793, \"sum\": 44.9061393737793, \"min\": 44.9061393737793}}, \"EndTime\": 1591459545.769261, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459545.723812}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:51 INFO 140027608049472] Epoch[47] Batch[0] avg_epoch_loss=0.979806\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.979805784405\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:51 INFO 140027608049472] Epoch[47] Batch[0] avg_epoch_loss=0.979806\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.979805784405\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] Epoch[47] Batch[5] avg_epoch_loss=0.998125\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.998124644441\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] Epoch[47] Batch [5]#011Speed: 198.78 samples/sec#011loss=0.998125\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] processed a total of 1996 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15589.71905708313, \"sum\": 15589.71905708313, \"min\": 15589.71905708313}}, \"EndTime\": 1591459561.359119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459545.769337}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.032291461 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.976096178451\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_346d5426-af1c-4a98-874c-884082ab52ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.94109344482422, \"sum\": 42.94109344482422, \"min\": 42.94109344482422}}, \"EndTime\": 1591459561.402643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459561.359181}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:06 INFO 140027608049472] Epoch[48] Batch[0] avg_epoch_loss=1.017528\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=1.01752817406\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] Epoch[47] Batch[5] avg_epoch_loss=0.998125\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.998124644441\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:05:56 INFO 140027608049472] Epoch[47] Batch [5]#011Speed: 198.78 samples/sec#011loss=0.998125\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] processed a total of 1996 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15589.71905708313, \"sum\": 15589.71905708313, \"min\": 15589.71905708313}}, \"EndTime\": 1591459561.359119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459545.769337}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.032291461 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.976096178451\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:01 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_346d5426-af1c-4a98-874c-884082ab52ee-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.94109344482422, \"sum\": 42.94109344482422, \"min\": 42.94109344482422}}, \"EndTime\": 1591459561.402643, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459561.359181}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:06 INFO 140027608049472] Epoch[48] Batch[0] avg_epoch_loss=1.017528\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=1.01752817406\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] Epoch[48] Batch[5] avg_epoch_loss=0.997671\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.997670647483\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] Epoch[48] Batch [5]#011Speed: 197.64 samples/sec#011loss=0.997671\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] Epoch[48] Batch[5] avg_epoch_loss=0.997671\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.997670647483\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:12 INFO 140027608049472] Epoch[48] Batch [5]#011Speed: 197.64 samples/sec#011loss=0.997671\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] Epoch[48] Batch[10] avg_epoch_loss=0.980781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.9605134928\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] Epoch[48] Batch [10]#011Speed: 175.65 samples/sec#011loss=0.960513\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16692.950963974, \"sum\": 16692.950963974, \"min\": 16692.950963974}}, \"EndTime\": 1591459578.095736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459561.40272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.5978381 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.980781031718\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:23 INFO 140027608049472] Epoch[49] Batch[0] avg_epoch_loss=0.987300\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.987299865147\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] Epoch[48] Batch[10] avg_epoch_loss=0.980781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.9605134928\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] Epoch[48] Batch [10]#011Speed: 175.65 samples/sec#011loss=0.960513\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16692.950963974, \"sum\": 16692.950963974, \"min\": 16692.950963974}}, \"EndTime\": 1591459578.095736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459561.40272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.5978381 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.980781031718\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:18 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:23 INFO 140027608049472] Epoch[49] Batch[0] avg_epoch_loss=0.987300\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.987299865147\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] Epoch[49] Batch[5] avg_epoch_loss=0.981530\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.981530111541\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] Epoch[49] Batch [5]#011Speed: 198.57 samples/sec#011loss=0.981530\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Epoch[49] Batch[10] avg_epoch_loss=0.945867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.903071680609\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Epoch[49] Batch [10]#011Speed: 175.86 samples/sec#011loss=0.903072\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16640.653133392334, \"sum\": 16640.653133392334, \"min\": 16640.653133392334}}, \"EndTime\": 1591459594.736964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459578.09582}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.200813537 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.94586718839\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c30404e2-9efa-47ba-a848-b1d20e8175e3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.08509826660156, \"sum\": 43.08509826660156, \"min\": 43.08509826660156}}, \"EndTime\": 1591459594.780648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459594.737038}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] Epoch[49] Batch[5] avg_epoch_loss=0.981530\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.981530111541\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:28 INFO 140027608049472] Epoch[49] Batch [5]#011Speed: 198.57 samples/sec#011loss=0.981530\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Epoch[49] Batch[10] avg_epoch_loss=0.945867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.903071680609\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Epoch[49] Batch [10]#011Speed: 175.86 samples/sec#011loss=0.903072\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16640.653133392334, \"sum\": 16640.653133392334, \"min\": 16640.653133392334}}, \"EndTime\": 1591459594.736964, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459578.09582}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.200813537 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.94586718839\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:34 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c30404e2-9efa-47ba-a848-b1d20e8175e3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 43.08509826660156, \"sum\": 43.08509826660156, \"min\": 43.08509826660156}}, \"EndTime\": 1591459594.780648, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459594.737038}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:40 INFO 140027608049472] Epoch[50] Batch[0] avg_epoch_loss=0.996681\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.9966805656\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] Epoch[50] Batch[5] avg_epoch_loss=0.982649\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.982649041422\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] Epoch[50] Batch [5]#011Speed: 197.77 samples/sec#011loss=0.982649\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:40 INFO 140027608049472] Epoch[50] Batch[0] avg_epoch_loss=0.996681\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:40 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.9966805656\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] Epoch[50] Batch[5] avg_epoch_loss=0.982649\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.982649041422\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:45 INFO 140027608049472] Epoch[50] Batch [5]#011Speed: 197.77 samples/sec#011loss=0.982649\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] Epoch[50] Batch[10] avg_epoch_loss=0.995717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=1.01139889123\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] Epoch[50] Batch [10]#011Speed: 176.16 samples/sec#011loss=1.011399\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] processed a total of 2186 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16698.445081710815, \"sum\": 16698.445081710815, \"min\": 16698.445081710815}}, \"EndTime\": 1591459611.479221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459594.780718}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.909491601 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.995717154972\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:56 INFO 140027608049472] Epoch[51] Batch[0] avg_epoch_loss=0.987270\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.987269563495\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] Epoch[50] Batch[10] avg_epoch_loss=0.995717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, batch=10 train loss <loss>=1.01139889123\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] Epoch[50] Batch [10]#011Speed: 176.16 samples/sec#011loss=1.011399\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] processed a total of 2186 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16698.445081710815, \"sum\": 16698.445081710815, \"min\": 16698.445081710815}}, \"EndTime\": 1591459611.479221, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459594.780718}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.909491601 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.995717154972\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:51 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:56 INFO 140027608049472] Epoch[51] Batch[0] avg_epoch_loss=0.987270\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:06:56 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.987269563495\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] Epoch[51] Batch[5] avg_epoch_loss=0.973990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.973989798588\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] Epoch[51] Batch [5]#011Speed: 197.85 samples/sec#011loss=0.973990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] processed a total of 2097 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15632.778882980347, \"sum\": 15632.778882980347, \"min\": 15632.778882980347}}, \"EndTime\": 1591459627.112509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459611.479302}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.140178729 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.973359313101\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] Epoch[51] Batch[5] avg_epoch_loss=0.973990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.973989798588\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:02 INFO 140027608049472] Epoch[51] Batch [5]#011Speed: 197.85 samples/sec#011loss=0.973990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] processed a total of 2097 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15632.778882980347, \"sum\": 15632.778882980347, \"min\": 15632.778882980347}}, \"EndTime\": 1591459627.112509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459611.479302}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.140178729 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.973359313101\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:07 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:12 INFO 140027608049472] Epoch[52] Batch[0] avg_epoch_loss=0.944771\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.944770812988\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] Epoch[52] Batch[5] avg_epoch_loss=0.959099\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.95909936773\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] Epoch[52] Batch [5]#011Speed: 200.39 samples/sec#011loss=0.959099\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:12 INFO 140027608049472] Epoch[52] Batch[0] avg_epoch_loss=0.944771\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.944770812988\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] Epoch[52] Batch[5] avg_epoch_loss=0.959099\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.95909936773\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:17 INFO 140027608049472] Epoch[52] Batch [5]#011Speed: 200.39 samples/sec#011loss=0.959099\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Epoch[52] Batch[10] avg_epoch_loss=0.908085\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.846867320223\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Epoch[52] Batch [10]#011Speed: 175.22 samples/sec#011loss=0.846867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16617.307901382446, \"sum\": 16617.307901382446, \"min\": 16617.307901382446}}, \"EndTime\": 1591459643.730379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459627.112593}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.382299143 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.908084800681\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3509b709-5805-429e-b4c1-13cd5319d953-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.61295890808105, \"sum\": 66.61295890808105, \"min\": 66.61295890808105}}, \"EndTime\": 1591459643.797563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459643.730456}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Epoch[52] Batch[10] avg_epoch_loss=0.908085\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.846867320223\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Epoch[52] Batch [10]#011Speed: 175.22 samples/sec#011loss=0.846867\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] processed a total of 2150 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16617.307901382446, \"sum\": 16617.307901382446, \"min\": 16617.307901382446}}, \"EndTime\": 1591459643.730379, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459627.112593}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.382299143 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.908084800681\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:23 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3509b709-5805-429e-b4c1-13cd5319d953-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 66.61295890808105, \"sum\": 66.61295890808105, \"min\": 66.61295890808105}}, \"EndTime\": 1591459643.797563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459643.730456}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:29 INFO 140027608049472] Epoch[53] Batch[0] avg_epoch_loss=0.973726\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.973725948694\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] Epoch[53] Batch[5] avg_epoch_loss=0.963960\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.963960047788\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] Epoch[53] Batch [5]#011Speed: 199.65 samples/sec#011loss=0.963960\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:29 INFO 140027608049472] Epoch[53] Batch[0] avg_epoch_loss=0.973726\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:29 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.973725948694\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] Epoch[53] Batch[5] avg_epoch_loss=0.963960\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.963960047788\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:34 INFO 140027608049472] Epoch[53] Batch [5]#011Speed: 199.65 samples/sec#011loss=0.963960\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] processed a total of 2115 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15677.62804031372, \"sum\": 15677.62804031372, \"min\": 15677.62804031372}}, \"EndTime\": 1591459659.475331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459643.797635}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.904563298 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.961571128413\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:44 INFO 140027608049472] Epoch[54] Batch[0] avg_epoch_loss=0.957770\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.957769789786\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] processed a total of 2115 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15677.62804031372, \"sum\": 15677.62804031372, \"min\": 15677.62804031372}}, \"EndTime\": 1591459659.475331, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459643.797635}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.904563298 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.961571128413\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:39 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:44 INFO 140027608049472] Epoch[54] Batch[0] avg_epoch_loss=0.957770\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.957769789786\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] Epoch[54] Batch[5] avg_epoch_loss=0.965723\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.965722857781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] Epoch[54] Batch [5]#011Speed: 198.70 samples/sec#011loss=0.965723\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] processed a total of 2087 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15550.931930541992, \"sum\": 15550.931930541992, \"min\": 15550.931930541992}}, \"EndTime\": 1591459675.026851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459659.475415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.203113943 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.97238945871\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] Epoch[54] Batch[5] avg_epoch_loss=0.965723\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.965722857781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:50 INFO 140027608049472] Epoch[54] Batch [5]#011Speed: 198.70 samples/sec#011loss=0.965723\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] processed a total of 2087 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15550.931930541992, \"sum\": 15550.931930541992, \"min\": 15550.931930541992}}, \"EndTime\": 1591459675.026851, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459659.475415}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.203113943 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.97238945871\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:07:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:00 INFO 140027608049472] Epoch[55] Batch[0] avg_epoch_loss=0.935461\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.93546122425\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] Epoch[55] Batch[5] avg_epoch_loss=0.951597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.951596913848\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] Epoch[55] Batch [5]#011Speed: 198.79 samples/sec#011loss=0.951597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:00 INFO 140027608049472] Epoch[55] Batch[0] avg_epoch_loss=0.935461\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.93546122425\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] Epoch[55] Batch[5] avg_epoch_loss=0.951597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.951596913848\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:05 INFO 140027608049472] Epoch[55] Batch [5]#011Speed: 198.79 samples/sec#011loss=0.951597\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] Epoch[55] Batch[10] avg_epoch_loss=0.948855\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.945565191305\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] Epoch[55] Batch [10]#011Speed: 174.48 samples/sec#011loss=0.945565\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] processed a total of 2161 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16755.187034606934, \"sum\": 16755.187034606934, \"min\": 16755.187034606934}}, \"EndTime\": 1591459691.782606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459675.026936}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.974155682 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.948855221783\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:17 INFO 140027608049472] Epoch[56] Batch[0] avg_epoch_loss=0.912382\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.912381873941\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] Epoch[55] Batch[10] avg_epoch_loss=0.948855\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=0.945565191305\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] Epoch[55] Batch [10]#011Speed: 174.48 samples/sec#011loss=0.945565\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] processed a total of 2161 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16755.187034606934, \"sum\": 16755.187034606934, \"min\": 16755.187034606934}}, \"EndTime\": 1591459691.782606, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459675.026936}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.974155682 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.948855221783\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:11 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:17 INFO 140027608049472] Epoch[56] Batch[0] avg_epoch_loss=0.912382\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.912381873941\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] Epoch[56] Batch[5] avg_epoch_loss=0.959345\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.959345079818\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] Epoch[56] Batch [5]#011Speed: 199.28 samples/sec#011loss=0.959345\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] Epoch[56] Batch[5] avg_epoch_loss=0.959345\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.959345079818\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:22 INFO 140027608049472] Epoch[56] Batch [5]#011Speed: 199.28 samples/sec#011loss=0.959345\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] Epoch[56] Batch[10] avg_epoch_loss=0.957641\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=0.955595685851\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] Epoch[56] Batch [10]#011Speed: 175.11 samples/sec#011loss=0.955596\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] processed a total of 2158 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16725.62313079834, \"sum\": 16725.62313079834, \"min\": 16725.62313079834}}, \"EndTime\": 1591459708.508799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459691.782677}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.022699667 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.957640809833\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:33 INFO 140027608049472] Epoch[57] Batch[0] avg_epoch_loss=0.921165\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.921164746554\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] Epoch[57] Batch[5] avg_epoch_loss=0.945758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.945758231781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] Epoch[57] Batch [5]#011Speed: 197.91 samples/sec#011loss=0.945758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] Epoch[56] Batch[10] avg_epoch_loss=0.957641\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=0.955595685851\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] Epoch[56] Batch [10]#011Speed: 175.11 samples/sec#011loss=0.955596\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] processed a total of 2158 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16725.62313079834, \"sum\": 16725.62313079834, \"min\": 16725.62313079834}}, \"EndTime\": 1591459708.508799, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459691.782677}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.022699667 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.957640809833\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:28 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:33 INFO 140027608049472] Epoch[57] Batch[0] avg_epoch_loss=0.921165\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.921164746554\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] Epoch[57] Batch[5] avg_epoch_loss=0.945758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.945758231781\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:39 INFO 140027608049472] Epoch[57] Batch [5]#011Speed: 197.91 samples/sec#011loss=0.945758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] processed a total of 2058 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15654.707908630371, \"sum\": 15654.707908630371, \"min\": 15654.707908630371}}, \"EndTime\": 1591459724.164014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459708.508877}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.461051332 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.955608641427\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:49 INFO 140027608049472] Epoch[58] Batch[0] avg_epoch_loss=0.913322\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.913322232804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] processed a total of 2058 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15654.707908630371, \"sum\": 15654.707908630371, \"min\": 15654.707908630371}}, \"EndTime\": 1591459724.164014, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459708.508877}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=131.461051332 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.955608641427\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:49 INFO 140027608049472] Epoch[58] Batch[0] avg_epoch_loss=0.913322\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.913322232804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] Epoch[58] Batch[5] avg_epoch_loss=0.930557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.930557143013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] Epoch[58] Batch [5]#011Speed: 199.00 samples/sec#011loss=0.930557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15688.28296661377, \"sum\": 15688.28296661377, \"min\": 15688.28296661377}}, \"EndTime\": 1591459739.852859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459724.164097}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.17555264 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.937861734066\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] Epoch[58] Batch[5] avg_epoch_loss=0.930557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.930557143013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:54 INFO 140027608049472] Epoch[58] Batch [5]#011Speed: 199.00 samples/sec#011loss=0.930557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] processed a total of 2105 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15688.28296661377, \"sum\": 15688.28296661377, \"min\": 15688.28296661377}}, \"EndTime\": 1591459739.852859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459724.164097}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.17555264 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.937861734066\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:08:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:05 INFO 140027608049472] Epoch[59] Batch[0] avg_epoch_loss=0.956011\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.956010566567\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:05 INFO 140027608049472] Epoch[59] Batch[0] avg_epoch_loss=0.956011\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.956010566567\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] Epoch[59] Batch[5] avg_epoch_loss=0.931927\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.931926871246\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] Epoch[59] Batch [5]#011Speed: 197.66 samples/sec#011loss=0.931927\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] processed a total of 2097 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15619.915008544922, \"sum\": 15619.915008544922, \"min\": 15619.915008544922}}, \"EndTime\": 1591459755.47341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459739.852942}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.250666212 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.928227219492\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] Epoch[59] Batch[5] avg_epoch_loss=0.931927\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.931926871246\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:10 INFO 140027608049472] Epoch[59] Batch [5]#011Speed: 197.66 samples/sec#011loss=0.931927\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] processed a total of 2097 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15619.915008544922, \"sum\": 15619.915008544922, \"min\": 15619.915008544922}}, \"EndTime\": 1591459755.47341, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459739.852942}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.250666212 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.928227219492\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:15 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:20 INFO 140027608049472] Epoch[60] Batch[0] avg_epoch_loss=0.927094\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.927093577835\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] Epoch[60] Batch[5] avg_epoch_loss=0.927297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.927297076339\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] Epoch[60] Batch [5]#011Speed: 199.53 samples/sec#011loss=0.927297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:20 INFO 140027608049472] Epoch[60] Batch[0] avg_epoch_loss=0.927094\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.927093577835\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] Epoch[60] Batch[5] avg_epoch_loss=0.927297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.927297076339\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:26 INFO 140027608049472] Epoch[60] Batch [5]#011Speed: 199.53 samples/sec#011loss=0.927297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] processed a total of 2110 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15660.393953323364, \"sum\": 15660.393953323364, \"min\": 15660.393953323364}}, \"EndTime\": 1591459771.134371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459755.473492}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.733765217 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.9309223319\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:36 INFO 140027608049472] Epoch[61] Batch[0] avg_epoch_loss=0.932161\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.93216064741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] processed a total of 2110 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15660.393953323364, \"sum\": 15660.393953323364, \"min\": 15660.393953323364}}, \"EndTime\": 1591459771.134371, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459755.473492}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.733765217 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.9309223319\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:31 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:36 INFO 140027608049472] Epoch[61] Batch[0] avg_epoch_loss=0.932161\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.93216064741\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] Epoch[61] Batch[5] avg_epoch_loss=0.918435\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.918435438624\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] Epoch[61] Batch [5]#011Speed: 199.01 samples/sec#011loss=0.918435\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] Epoch[61] Batch[10] avg_epoch_loss=0.930450\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.944868483633\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] Epoch[61] Batch [10]#011Speed: 175.63 samples/sec#011loss=0.944868\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] processed a total of 2124 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16685.692071914673, \"sum\": 16685.692071914673, \"min\": 16685.692071914673}}, \"EndTime\": 1591459787.820596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459771.134454}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.293804223 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.930450459083\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] Epoch[61] Batch[5] avg_epoch_loss=0.918435\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.918435438624\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:41 INFO 140027608049472] Epoch[61] Batch [5]#011Speed: 199.01 samples/sec#011loss=0.918435\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] Epoch[61] Batch[10] avg_epoch_loss=0.930450\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.944868483633\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] Epoch[61] Batch [10]#011Speed: 175.63 samples/sec#011loss=0.944868\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] processed a total of 2124 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16685.692071914673, \"sum\": 16685.692071914673, \"min\": 16685.692071914673}}, \"EndTime\": 1591459787.820596, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459771.134454}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.293804223 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.930450459083\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:47 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:53 INFO 140027608049472] Epoch[62] Batch[0] avg_epoch_loss=0.928207\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.928207469436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] Epoch[62] Batch[5] avg_epoch_loss=0.925540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.925540252302\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] Epoch[62] Batch [5]#011Speed: 198.54 samples/sec#011loss=0.925540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:53 INFO 140027608049472] Epoch[62] Batch[0] avg_epoch_loss=0.928207\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.928207469436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] Epoch[62] Batch[5] avg_epoch_loss=0.925540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.925540252302\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:09:58 INFO 140027608049472] Epoch[62] Batch [5]#011Speed: 198.54 samples/sec#011loss=0.925540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Epoch[62] Batch[10] avg_epoch_loss=0.905587\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.881642510756\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Epoch[62] Batch [10]#011Speed: 174.28 samples/sec#011loss=0.881643\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] processed a total of 2159 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16729.612112045288, \"sum\": 16729.612112045288, \"min\": 16729.612112045288}}, \"EndTime\": 1591459804.550763, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459787.820675}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.051717111 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.905586733417\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2401de9d-87c6-4b87-851c-a77e2a975826-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.91496276855469, \"sum\": 67.91496276855469, \"min\": 67.91496276855469}}, \"EndTime\": 1591459804.619247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459804.550843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Epoch[62] Batch[10] avg_epoch_loss=0.905587\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, batch=10 train loss <loss>=0.881642510756\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Epoch[62] Batch [10]#011Speed: 174.28 samples/sec#011loss=0.881643\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] processed a total of 2159 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16729.612112045288, \"sum\": 16729.612112045288, \"min\": 16729.612112045288}}, \"EndTime\": 1591459804.550763, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459787.820675}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.051717111 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.905586733417\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2401de9d-87c6-4b87-851c-a77e2a975826-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 67.91496276855469, \"sum\": 67.91496276855469, \"min\": 67.91496276855469}}, \"EndTime\": 1591459804.619247, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459804.550843}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:09 INFO 140027608049472] Epoch[63] Batch[0] avg_epoch_loss=0.883795\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.883795252386\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] Epoch[63] Batch[5] avg_epoch_loss=0.912696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.912696058645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] Epoch[63] Batch [5]#011Speed: 199.27 samples/sec#011loss=0.912696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:09 INFO 140027608049472] Epoch[63] Batch[0] avg_epoch_loss=0.883795\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.883795252386\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] Epoch[63] Batch[5] avg_epoch_loss=0.912696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.912696058645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:15 INFO 140027608049472] Epoch[63] Batch [5]#011Speed: 199.27 samples/sec#011loss=0.912696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Epoch[63] Batch[10] avg_epoch_loss=0.897457\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.879169752013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Epoch[63] Batch [10]#011Speed: 174.59 samples/sec#011loss=0.879170\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] processed a total of 2124 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16706.249952316284, \"sum\": 16706.249952316284, \"min\": 16706.249952316284}}, \"EndTime\": 1591459821.32563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459804.619323}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.137270936 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.897456828358\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2d3efdd8-bbe2-49f9-82d7-0d2a75198c13-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.33894157409668, \"sum\": 44.33894157409668, \"min\": 44.33894157409668}}, \"EndTime\": 1591459821.370559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459821.325701}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:26 INFO 140027608049472] Epoch[64] Batch[0] avg_epoch_loss=0.915045\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.91504503646\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Epoch[63] Batch[10] avg_epoch_loss=0.897457\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.879169752013\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Epoch[63] Batch [10]#011Speed: 174.59 samples/sec#011loss=0.879170\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] processed a total of 2124 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16706.249952316284, \"sum\": 16706.249952316284, \"min\": 16706.249952316284}}, \"EndTime\": 1591459821.32563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459804.619323}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.137270936 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.897456828358\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:21 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_2d3efdd8-bbe2-49f9-82d7-0d2a75198c13-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 44.33894157409668, \"sum\": 44.33894157409668, \"min\": 44.33894157409668}}, \"EndTime\": 1591459821.370559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459821.325701}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:26 INFO 140027608049472] Epoch[64] Batch[0] avg_epoch_loss=0.915045\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.91504503646\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] Epoch[64] Batch[5] avg_epoch_loss=0.915540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.915539915457\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] Epoch[64] Batch [5]#011Speed: 197.68 samples/sec#011loss=0.915540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] Epoch[64] Batch[10] avg_epoch_loss=0.945492\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.981434314656\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] Epoch[64] Batch [10]#011Speed: 176.33 samples/sec#011loss=0.981434\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] processed a total of 2149 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16673.004150390625, \"sum\": 16673.004150390625, \"min\": 16673.004150390625}}, \"EndTime\": 1591459838.043706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459821.370638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.890117547 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.945491915093\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] Epoch[64] Batch[5] avg_epoch_loss=0.915540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.915539915457\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:32 INFO 140027608049472] Epoch[64] Batch [5]#011Speed: 197.68 samples/sec#011loss=0.915540\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] Epoch[64] Batch[10] avg_epoch_loss=0.945492\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.981434314656\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] Epoch[64] Batch [10]#011Speed: 176.33 samples/sec#011loss=0.981434\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] processed a total of 2149 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16673.004150390625, \"sum\": 16673.004150390625, \"min\": 16673.004150390625}}, \"EndTime\": 1591459838.043706, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459821.370638}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.890117547 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.945491915093\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:38 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:43 INFO 140027608049472] Epoch[65] Batch[0] avg_epoch_loss=0.927468\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.927467778044\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] Epoch[65] Batch[5] avg_epoch_loss=0.922873\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.922872999179\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] Epoch[65] Batch [5]#011Speed: 197.37 samples/sec#011loss=0.922873\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:43 INFO 140027608049472] Epoch[65] Batch[0] avg_epoch_loss=0.927468\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.927467778044\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] Epoch[65] Batch[5] avg_epoch_loss=0.922873\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.922872999179\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:48 INFO 140027608049472] Epoch[65] Batch [5]#011Speed: 197.37 samples/sec#011loss=0.922873\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] Epoch[65] Batch[10] avg_epoch_loss=0.924842\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.927205528403\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] Epoch[65] Batch [10]#011Speed: 174.12 samples/sec#011loss=0.927206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] processed a total of 2138 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16802.83212661743, \"sum\": 16802.83212661743, \"min\": 16802.83212661743}}, \"EndTime\": 1591459854.847017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459838.043784}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.239622396 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.924842330645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:00 INFO 140027608049472] Epoch[66] Batch[0] avg_epoch_loss=0.900691\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.900691338305\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] Epoch[65] Batch[10] avg_epoch_loss=0.924842\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=0.927205528403\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] Epoch[65] Batch [10]#011Speed: 174.12 samples/sec#011loss=0.927206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] processed a total of 2138 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16802.83212661743, \"sum\": 16802.83212661743, \"min\": 16802.83212661743}}, \"EndTime\": 1591459854.847017, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459838.043784}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.239622396 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.924842330645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:10:54 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:00 INFO 140027608049472] Epoch[66] Batch[0] avg_epoch_loss=0.900691\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.900691338305\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] Epoch[66] Batch[5] avg_epoch_loss=0.909292\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.909291717241\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] Epoch[66] Batch [5]#011Speed: 198.39 samples/sec#011loss=0.909292\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] Epoch[66] Batch[5] avg_epoch_loss=0.909292\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.909291717241\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:05 INFO 140027608049472] Epoch[66] Batch [5]#011Speed: 198.39 samples/sec#011loss=0.909292\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] Epoch[66] Batch[10] avg_epoch_loss=0.901870\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.892963135917\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] Epoch[66] Batch [10]#011Speed: 175.56 samples/sec#011loss=0.892963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] processed a total of 2197 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16791.664838790894, \"sum\": 16791.664838790894, \"min\": 16791.664838790894}}, \"EndTime\": 1591459871.639175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459854.847094}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.837943678 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.901869634821\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:16 INFO 140027608049472] Epoch[67] Batch[0] avg_epoch_loss=0.921549\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.921549023322\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] Epoch[66] Batch[10] avg_epoch_loss=0.901870\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.892963135917\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] Epoch[66] Batch [10]#011Speed: 175.56 samples/sec#011loss=0.892963\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] processed a total of 2197 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16791.664838790894, \"sum\": 16791.664838790894, \"min\": 16791.664838790894}}, \"EndTime\": 1591459871.639175, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459854.847094}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.837943678 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.901869634821\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:11 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:16 INFO 140027608049472] Epoch[67] Batch[0] avg_epoch_loss=0.921549\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:16 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.921549023322\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] Epoch[67] Batch[5] avg_epoch_loss=0.910499\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.910499416807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] Epoch[67] Batch [5]#011Speed: 199.29 samples/sec#011loss=0.910499\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] Epoch[67] Batch[10] avg_epoch_loss=0.930071\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=0.953556593409\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] Epoch[67] Batch [10]#011Speed: 174.53 samples/sec#011loss=0.953557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] processed a total of 2126 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16721.81487083435, \"sum\": 16721.81487083435, \"min\": 16721.81487083435}}, \"EndTime\": 1591459888.361512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459871.639245}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.138455927 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.930070860717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] Epoch[67] Batch[5] avg_epoch_loss=0.910499\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.910499416807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:22 INFO 140027608049472] Epoch[67] Batch [5]#011Speed: 199.29 samples/sec#011loss=0.910499\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] Epoch[67] Batch[10] avg_epoch_loss=0.930071\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=0.953556593409\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] Epoch[67] Batch [10]#011Speed: 174.53 samples/sec#011loss=0.953557\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] processed a total of 2126 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16721.81487083435, \"sum\": 16721.81487083435, \"min\": 16721.81487083435}}, \"EndTime\": 1591459888.361512, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459871.639245}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.138455927 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.930070860717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:28 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:33 INFO 140027608049472] Epoch[68] Batch[0] avg_epoch_loss=0.905776\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.905776257785\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] Epoch[68] Batch[5] avg_epoch_loss=0.903436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.903436096959\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] Epoch[68] Batch [5]#011Speed: 196.18 samples/sec#011loss=0.903436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:33 INFO 140027608049472] Epoch[68] Batch[0] avg_epoch_loss=0.905776\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.905776257785\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] Epoch[68] Batch[5] avg_epoch_loss=0.903436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.903436096959\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:39 INFO 140027608049472] Epoch[68] Batch [5]#011Speed: 196.18 samples/sec#011loss=0.903436\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] processed a total of 2017 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15677.422046661377, \"sum\": 15677.422046661377, \"min\": 15677.422046661377}}, \"EndTime\": 1591459904.039494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459888.361586}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.655215837 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.897710505072\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:49 INFO 140027608049472] Epoch[69] Batch[0] avg_epoch_loss=0.891871\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.891871110448\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] processed a total of 2017 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15677.422046661377, \"sum\": 15677.422046661377, \"min\": 15677.422046661377}}, \"EndTime\": 1591459904.039494, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459888.361586}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.655215837 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.897710505072\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:49 INFO 140027608049472] Epoch[69] Batch[0] avg_epoch_loss=0.891871\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.891871110448\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] Epoch[69] Batch[5] avg_epoch_loss=0.890831\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.8908305618\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] Epoch[69] Batch [5]#011Speed: 199.24 samples/sec#011loss=0.890831\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Epoch[69] Batch[10] avg_epoch_loss=0.892588\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=0.894696044922\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Epoch[69] Batch [10]#011Speed: 173.76 samples/sec#011loss=0.894696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16855.350017547607, \"sum\": 16855.350017547607, \"min\": 16855.350017547607}}, \"EndTime\": 1591459920.895414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459904.039595}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.614456932 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.892587599583\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c6873cd5-6ed2-40e2-bb50-89d7f7df098e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.14405059814453, \"sum\": 52.14405059814453, \"min\": 52.14405059814453}}, \"EndTime\": 1591459920.948119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459920.895488}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] Epoch[69] Batch[5] avg_epoch_loss=0.890831\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.8908305618\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:11:54 INFO 140027608049472] Epoch[69] Batch [5]#011Speed: 199.24 samples/sec#011loss=0.890831\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Epoch[69] Batch[10] avg_epoch_loss=0.892588\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, batch=10 train loss <loss>=0.894696044922\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Epoch[69] Batch [10]#011Speed: 173.76 samples/sec#011loss=0.894696\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16855.350017547607, \"sum\": 16855.350017547607, \"min\": 16855.350017547607}}, \"EndTime\": 1591459920.895414, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459904.039595}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.614456932 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.892587599583\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:00 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c6873cd5-6ed2-40e2-bb50-89d7f7df098e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 52.14405059814453, \"sum\": 52.14405059814453, \"min\": 52.14405059814453}}, \"EndTime\": 1591459920.948119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459920.895488}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:06 INFO 140027608049472] Epoch[70] Batch[0] avg_epoch_loss=0.884970\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.884970323095\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:06 INFO 140027608049472] Epoch[70] Batch[0] avg_epoch_loss=0.884970\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.884970323095\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] Epoch[70] Batch[5] avg_epoch_loss=0.893102\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.893101974103\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] Epoch[70] Batch [5]#011Speed: 199.89 samples/sec#011loss=0.893102\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Epoch[70] Batch[10] avg_epoch_loss=0.877897\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.859650953761\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Epoch[70] Batch [10]#011Speed: 176.63 samples/sec#011loss=0.859651\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] processed a total of 2139 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16666.35298728943, \"sum\": 16666.35298728943, \"min\": 16666.35298728943}}, \"EndTime\": 1591459937.614589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459920.948185}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.341637048 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.877896964857\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_611401fe-eb9c-4245-846f-d93489e272e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.3121452331543, \"sum\": 42.3121452331543, \"min\": 42.3121452331543}}, \"EndTime\": 1591459937.657485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459937.61466}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] Epoch[70] Batch[5] avg_epoch_loss=0.893102\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.893101974103\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:11 INFO 140027608049472] Epoch[70] Batch [5]#011Speed: 199.89 samples/sec#011loss=0.893102\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Epoch[70] Batch[10] avg_epoch_loss=0.877897\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=0.859650953761\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Epoch[70] Batch [10]#011Speed: 176.63 samples/sec#011loss=0.859651\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] processed a total of 2139 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16666.35298728943, \"sum\": 16666.35298728943, \"min\": 16666.35298728943}}, \"EndTime\": 1591459937.614589, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459920.948185}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.341637048 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.877896964857\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:17 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_611401fe-eb9c-4245-846f-d93489e272e9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.3121452331543, \"sum\": 42.3121452331543, \"min\": 42.3121452331543}}, \"EndTime\": 1591459937.657485, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459937.61466}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:22 INFO 140027608049472] Epoch[71] Batch[0] avg_epoch_loss=0.898717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.898716620679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] Epoch[71] Batch[5] avg_epoch_loss=0.889383\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.889383304044\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] Epoch[71] Batch [5]#011Speed: 200.29 samples/sec#011loss=0.889383\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:22 INFO 140027608049472] Epoch[71] Batch[0] avg_epoch_loss=0.898717\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.898716620679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] Epoch[71] Batch[5] avg_epoch_loss=0.889383\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.889383304044\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:28 INFO 140027608049472] Epoch[71] Batch [5]#011Speed: 200.29 samples/sec#011loss=0.889383\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] Epoch[71] Batch[10] avg_epoch_loss=0.884458\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.878547524506\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] Epoch[71] Batch [10]#011Speed: 174.51 samples/sec#011loss=0.878548\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16705.490112304688, \"sum\": 16705.490112304688, \"min\": 16705.490112304688}}, \"EndTime\": 1591459954.363107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459937.657548}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.759210936 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.884457949709\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:39 INFO 140027608049472] Epoch[72] Batch[0] avg_epoch_loss=0.868464\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.8684639121\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] Epoch[71] Batch[10] avg_epoch_loss=0.884458\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.878547524506\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] Epoch[71] Batch [10]#011Speed: 174.51 samples/sec#011loss=0.878548\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16705.490112304688, \"sum\": 16705.490112304688, \"min\": 16705.490112304688}}, \"EndTime\": 1591459954.363107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459937.657548}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.759210936 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.884457949709\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:34 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:39 INFO 140027608049472] Epoch[72] Batch[0] avg_epoch_loss=0.868464\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.8684639121\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] Epoch[72] Batch[5] avg_epoch_loss=0.885331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.885330943941\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] Epoch[72] Batch [5]#011Speed: 198.99 samples/sec#011loss=0.885331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] processed a total of 2045 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15651.516914367676, \"sum\": 15651.516914367676, \"min\": 15651.516914367676}}, \"EndTime\": 1591459970.015146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459954.363178}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.657318748 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.870554618116\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_15c98bd2-d5a6-4aa2-8403-963f8310a226-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.15216636657715, \"sum\": 42.15216636657715, \"min\": 42.15216636657715}}, \"EndTime\": 1591459970.05794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459970.015228}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] Epoch[72] Batch[5] avg_epoch_loss=0.885331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.885330943941\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:45 INFO 140027608049472] Epoch[72] Batch [5]#011Speed: 198.99 samples/sec#011loss=0.885331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] processed a total of 2045 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15651.516914367676, \"sum\": 15651.516914367676, \"min\": 15651.516914367676}}, \"EndTime\": 1591459970.015146, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459954.363178}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.657318748 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.870554618116\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:50 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_15c98bd2-d5a6-4aa2-8403-963f8310a226-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.15216636657715, \"sum\": 42.15216636657715, \"min\": 42.15216636657715}}, \"EndTime\": 1591459970.05794, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459970.015228}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:55 INFO 140027608049472] Epoch[73] Batch[0] avg_epoch_loss=0.884251\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.88425078482\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] Epoch[73] Batch[5] avg_epoch_loss=0.880572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.880571833197\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] Epoch[73] Batch [5]#011Speed: 198.22 samples/sec#011loss=0.880572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:55 INFO 140027608049472] Epoch[73] Batch[0] avg_epoch_loss=0.884251\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:12:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.88425078482\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] Epoch[73] Batch[5] avg_epoch_loss=0.880572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.880571833197\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:00 INFO 140027608049472] Epoch[73] Batch [5]#011Speed: 198.22 samples/sec#011loss=0.880572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] Epoch[73] Batch[10] avg_epoch_loss=0.882675\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.885198571547\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] Epoch[73] Batch [10]#011Speed: 175.97 samples/sec#011loss=0.885199\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] processed a total of 2169 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16708.79888534546, \"sum\": 16708.79888534546, \"min\": 16708.79888534546}}, \"EndTime\": 1591459986.766875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459970.058012}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.810964507 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.882674896083\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:12 INFO 140027608049472] Epoch[74] Batch[0] avg_epoch_loss=0.857160\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.857160028422\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] Epoch[73] Batch[10] avg_epoch_loss=0.882675\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.885198571547\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] Epoch[73] Batch [10]#011Speed: 175.97 samples/sec#011loss=0.885199\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] processed a total of 2169 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16708.79888534546, \"sum\": 16708.79888534546, \"min\": 16708.79888534546}}, \"EndTime\": 1591459986.766875, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459970.058012}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.810964507 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.882674896083\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:06 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:12 INFO 140027608049472] Epoch[74] Batch[0] avg_epoch_loss=0.857160\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.857160028422\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] Epoch[74] Batch[5] avg_epoch_loss=0.877747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.877746905921\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] Epoch[74] Batch [5]#011Speed: 199.24 samples/sec#011loss=0.877747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] Epoch[74] Batch[5] avg_epoch_loss=0.877747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.877746905921\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:17 INFO 140027608049472] Epoch[74] Batch [5]#011Speed: 199.24 samples/sec#011loss=0.877747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] processed a total of 2091 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15631.53100013733, \"sum\": 15631.53100013733, \"min\": 15631.53100013733}}, \"EndTime\": 1591460002.398928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459986.766951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.767066943 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.881959123432\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:27 INFO 140027608049472] Epoch[75] Batch[0] avg_epoch_loss=0.927299\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.927298635807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] processed a total of 2091 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15631.53100013733, \"sum\": 15631.53100013733, \"min\": 15631.53100013733}}, \"EndTime\": 1591460002.398928, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591459986.766951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.767066943 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.881959123432\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:22 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:27 INFO 140027608049472] Epoch[75] Batch[0] avg_epoch_loss=0.927299\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.927298635807\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] Epoch[75] Batch[5] avg_epoch_loss=0.890012\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.890011553494\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] Epoch[75] Batch [5]#011Speed: 198.53 samples/sec#011loss=0.890012\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] Epoch[75] Batch[10] avg_epoch_loss=0.902248\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.916930734886\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] Epoch[75] Batch [10]#011Speed: 175.45 samples/sec#011loss=0.916931\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] processed a total of 2126 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16725.594997406006, \"sum\": 16725.594997406006, \"min\": 16725.594997406006}}, \"EndTime\": 1591460019.125071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460002.399011}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.109678263 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.902247545036\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] Epoch[75] Batch[5] avg_epoch_loss=0.890012\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.890011553494\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:33 INFO 140027608049472] Epoch[75] Batch [5]#011Speed: 198.53 samples/sec#011loss=0.890012\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] Epoch[75] Batch[10] avg_epoch_loss=0.902248\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.916930734886\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] Epoch[75] Batch [10]#011Speed: 175.45 samples/sec#011loss=0.916931\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] processed a total of 2126 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16725.594997406006, \"sum\": 16725.594997406006, \"min\": 16725.594997406006}}, \"EndTime\": 1591460019.125071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460002.399011}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.109678263 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.902247545036\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:39 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:44 INFO 140027608049472] Epoch[76] Batch[0] avg_epoch_loss=0.873952\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.873951821957\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] Epoch[76] Batch[5] avg_epoch_loss=0.872942\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.872942474653\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] Epoch[76] Batch [5]#011Speed: 198.92 samples/sec#011loss=0.872942\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:44 INFO 140027608049472] Epoch[76] Batch[0] avg_epoch_loss=0.873952\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.873951821957\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] Epoch[76] Batch[5] avg_epoch_loss=0.872942\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.872942474653\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:49 INFO 140027608049472] Epoch[76] Batch [5]#011Speed: 198.92 samples/sec#011loss=0.872942\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] Epoch[76] Batch[10] avg_epoch_loss=0.895550\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=0.922679627616\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] Epoch[76] Batch [10]#011Speed: 173.42 samples/sec#011loss=0.922680\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16733.367919921875, \"sum\": 16733.367919921875, \"min\": 16733.367919921875}}, \"EndTime\": 1591460035.859007, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460019.125153}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.544781801 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.895550271455\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:01 INFO 140027608049472] Epoch[77] Batch[0] avg_epoch_loss=0.895391\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.89539135627\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] Epoch[76] Batch[10] avg_epoch_loss=0.895550\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, batch=10 train loss <loss>=0.922679627616\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] Epoch[76] Batch [10]#011Speed: 173.42 samples/sec#011loss=0.922680\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] processed a total of 2151 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16733.367919921875, \"sum\": 16733.367919921875, \"min\": 16733.367919921875}}, \"EndTime\": 1591460035.859007, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460019.125153}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.544781801 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.895550271455\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:13:55 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:01 INFO 140027608049472] Epoch[77] Batch[0] avg_epoch_loss=0.895391\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.89539135627\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] Epoch[77] Batch[5] avg_epoch_loss=0.877992\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.877992342103\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] Epoch[77] Batch [5]#011Speed: 196.77 samples/sec#011loss=0.877992\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] Epoch[77] Batch[5] avg_epoch_loss=0.877992\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.877992342103\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:06 INFO 140027608049472] Epoch[77] Batch [5]#011Speed: 196.77 samples/sec#011loss=0.877992\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] Epoch[77] Batch[10] avg_epoch_loss=0.881735\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.886225790348\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] Epoch[77] Batch [10]#011Speed: 175.85 samples/sec#011loss=0.886226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] processed a total of 2152 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16748.46315383911, \"sum\": 16748.46315383911, \"min\": 16748.46315383911}}, \"EndTime\": 1591460052.607994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460035.859068}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.488526807 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.881734818578\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:17 INFO 140027608049472] Epoch[78] Batch[0] avg_epoch_loss=0.881294\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.881293890611\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] Epoch[78] Batch[5] avg_epoch_loss=0.866553\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.866552784758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] Epoch[78] Batch [5]#011Speed: 199.48 samples/sec#011loss=0.866553\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] Epoch[77] Batch[10] avg_epoch_loss=0.881735\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=0.886225790348\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] Epoch[77] Batch [10]#011Speed: 175.85 samples/sec#011loss=0.886226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] processed a total of 2152 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16748.46315383911, \"sum\": 16748.46315383911, \"min\": 16748.46315383911}}, \"EndTime\": 1591460052.607994, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460035.859068}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=128.488526807 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.881734818578\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:12 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:17 INFO 140027608049472] Epoch[78] Batch[0] avg_epoch_loss=0.881294\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.881293890611\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] Epoch[78] Batch[5] avg_epoch_loss=0.866553\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.866552784758\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:23 INFO 140027608049472] Epoch[78] Batch [5]#011Speed: 199.48 samples/sec#011loss=0.866553\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] processed a total of 2104 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15630.34701347351, \"sum\": 15630.34701347351, \"min\": 15630.34701347351}}, \"EndTime\": 1591460068.238891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460052.608071}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.609169417 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.863491842882\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3b5565b9-c4bb-460e-b9a9-b1b3b473fe0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.73200035095215, \"sum\": 42.73200035095215, \"min\": 42.73200035095215}}, \"EndTime\": 1591460068.282235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460068.238951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] processed a total of 2104 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15630.34701347351, \"sum\": 15630.34701347351, \"min\": 15630.34701347351}}, \"EndTime\": 1591460068.238891, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460052.608071}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.609169417 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.863491842882\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:28 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_3b5565b9-c4bb-460e-b9a9-b1b3b473fe0b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.73200035095215, \"sum\": 42.73200035095215, \"min\": 42.73200035095215}}, \"EndTime\": 1591460068.282235, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460068.238951}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:33 INFO 140027608049472] Epoch[79] Batch[0] avg_epoch_loss=0.881407\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.881406820045\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] Epoch[79] Batch[5] avg_epoch_loss=0.871763\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.871763337333\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] Epoch[79] Batch [5]#011Speed: 199.41 samples/sec#011loss=0.871763\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:33 INFO 140027608049472] Epoch[79] Batch[0] avg_epoch_loss=0.881407\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.881406820045\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] Epoch[79] Batch[5] avg_epoch_loss=0.871763\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.871763337333\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:38 INFO 140027608049472] Epoch[79] Batch [5]#011Speed: 199.41 samples/sec#011loss=0.871763\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] Epoch[79] Batch[10] avg_epoch_loss=0.891412\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=0.914991400377\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] Epoch[79] Batch [10]#011Speed: 176.05 samples/sec#011loss=0.914991\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16683.459997177124, \"sum\": 16683.459997177124, \"min\": 16683.459997177124}}, \"EndTime\": 1591460084.965843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460068.282303}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.670311098 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.891412456899\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:50 INFO 140027608049472] Epoch[80] Batch[0] avg_epoch_loss=0.868196\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.868195731685\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] Epoch[79] Batch[10] avg_epoch_loss=0.891412\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=0.914991400377\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] Epoch[79] Batch [10]#011Speed: 176.05 samples/sec#011loss=0.914991\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] processed a total of 2130 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16683.459997177124, \"sum\": 16683.459997177124, \"min\": 16683.459997177124}}, \"EndTime\": 1591460084.965843, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460068.282303}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.670311098 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.891412456899\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:44 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:50 INFO 140027608049472] Epoch[80] Batch[0] avg_epoch_loss=0.868196\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:50 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.868195731685\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] Epoch[80] Batch[5] avg_epoch_loss=0.867228\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.867228418026\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] Epoch[80] Batch [5]#011Speed: 198.77 samples/sec#011loss=0.867228\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Epoch[80] Batch[10] avg_epoch_loss=0.845316\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.819020238912\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Epoch[80] Batch [10]#011Speed: 173.51 samples/sec#011loss=0.819020\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] processed a total of 2140 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16770.159006118774, \"sum\": 16770.159006118774, \"min\": 16770.159006118774}}, \"EndTime\": 1591460101.736501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460084.96592}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.60671735 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.845315609338\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_90029849-1e61-434b-998f-d838ea369e4b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.00079154968262, \"sum\": 45.00079154968262, \"min\": 45.00079154968262}}, \"EndTime\": 1591460101.782114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460101.736581}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] Epoch[80] Batch[5] avg_epoch_loss=0.867228\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.867228418026\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:14:55 INFO 140027608049472] Epoch[80] Batch [5]#011Speed: 198.77 samples/sec#011loss=0.867228\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Epoch[80] Batch[10] avg_epoch_loss=0.845316\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.819020238912\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Epoch[80] Batch [10]#011Speed: 173.51 samples/sec#011loss=0.819020\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] processed a total of 2140 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16770.159006118774, \"sum\": 16770.159006118774, \"min\": 16770.159006118774}}, \"EndTime\": 1591460101.736501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460084.96592}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.60671735 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.845315609338\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:01 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_90029849-1e61-434b-998f-d838ea369e4b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.00079154968262, \"sum\": 45.00079154968262, \"min\": 45.00079154968262}}, \"EndTime\": 1591460101.782114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460101.736581}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:07 INFO 140027608049472] Epoch[81] Batch[0] avg_epoch_loss=0.855695\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.855694608868\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] Epoch[81] Batch[5] avg_epoch_loss=0.863661\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.863661424169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] Epoch[81] Batch [5]#011Speed: 198.24 samples/sec#011loss=0.863661\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:07 INFO 140027608049472] Epoch[81] Batch[0] avg_epoch_loss=0.855695\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:07 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.855694608868\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] Epoch[81] Batch[5] avg_epoch_loss=0.863661\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.863661424169\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:12 INFO 140027608049472] Epoch[81] Batch [5]#011Speed: 198.24 samples/sec#011loss=0.863661\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] processed a total of 2050 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15670.994997024536, \"sum\": 15670.994997024536, \"min\": 15670.994997024536}}, \"EndTime\": 1591460117.453244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460101.78218}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.813887848 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.863509390489\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] processed a total of 2050 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15670.994997024536, \"sum\": 15670.994997024536, \"min\": 15670.994997024536}}, \"EndTime\": 1591460117.453244, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460101.78218}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.813887848 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.863509390489\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:17 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:22 INFO 140027608049472] Epoch[82] Batch[0] avg_epoch_loss=0.874957\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.874956742773\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] Epoch[82] Batch[5] avg_epoch_loss=0.862429\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.862428761128\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] Epoch[82] Batch [5]#011Speed: 198.75 samples/sec#011loss=0.862429\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] processed a total of 2111 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15625.95796585083, \"sum\": 15625.95796585083, \"min\": 15625.95796585083}}, \"EndTime\": 1591460133.079817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460117.453329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.094653699 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.854550091725\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:22 INFO 140027608049472] Epoch[82] Batch[0] avg_epoch_loss=0.874957\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:22 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.874956742773\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] Epoch[82] Batch[5] avg_epoch_loss=0.862429\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.862428761128\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:28 INFO 140027608049472] Epoch[82] Batch [5]#011Speed: 198.75 samples/sec#011loss=0.862429\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] processed a total of 2111 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15625.95796585083, \"sum\": 15625.95796585083, \"min\": 15625.95796585083}}, \"EndTime\": 1591460133.079817, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460117.453329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=135.094653699 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.854550091725\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:33 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:38 INFO 140027608049472] Epoch[83] Batch[0] avg_epoch_loss=0.856902\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.856902428393\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] Epoch[83] Batch[5] avg_epoch_loss=0.856732\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.856732338479\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] Epoch[83] Batch [5]#011Speed: 198.83 samples/sec#011loss=0.856732\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:38 INFO 140027608049472] Epoch[83] Batch[0] avg_epoch_loss=0.856902\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.856902428393\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] Epoch[83] Batch[5] avg_epoch_loss=0.856732\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.856732338479\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:43 INFO 140027608049472] Epoch[83] Batch [5]#011Speed: 198.83 samples/sec#011loss=0.856732\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] processed a total of 2106 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15680.85503578186, \"sum\": 15680.85503578186, \"min\": 15680.85503578186}}, \"EndTime\": 1591460148.761279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460133.079884}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.302863516 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.85745092428\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:54 INFO 140027608049472] Epoch[84] Batch[0] avg_epoch_loss=0.859543\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.859543350508\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] processed a total of 2106 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15680.85503578186, \"sum\": 15680.85503578186, \"min\": 15680.85503578186}}, \"EndTime\": 1591460148.761279, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460133.079884}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.302863516 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.85745092428\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:48 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:54 INFO 140027608049472] Epoch[84] Batch[0] avg_epoch_loss=0.859543\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:54 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.859543350508\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] Epoch[84] Batch[5] avg_epoch_loss=0.856396\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.856396465182\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] Epoch[84] Batch [5]#011Speed: 197.27 samples/sec#011loss=0.856396\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] Epoch[84] Batch[5] avg_epoch_loss=0.856396\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.856396465182\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:15:59 INFO 140027608049472] Epoch[84] Batch [5]#011Speed: 197.27 samples/sec#011loss=0.856396\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] processed a total of 2066 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15784.823179244995, \"sum\": 15784.823179244995, \"min\": 15784.823179244995}}, \"EndTime\": 1591460164.546639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460148.761361}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.884330891 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.853460549409\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:09 INFO 140027608049472] Epoch[85] Batch[0] avg_epoch_loss=0.856370\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.856370242137\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] processed a total of 2066 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15784.823179244995, \"sum\": 15784.823179244995, \"min\": 15784.823179244995}}, \"EndTime\": 1591460164.546639, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460148.761361}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.884330891 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.853460549409\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:04 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:09 INFO 140027608049472] Epoch[85] Batch[0] avg_epoch_loss=0.856370\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:09 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.856370242137\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] Epoch[85] Batch[5] avg_epoch_loss=0.845990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.845989707131\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] Epoch[85] Batch [5]#011Speed: 197.09 samples/sec#011loss=0.845990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] Epoch[85] Batch[10] avg_epoch_loss=0.851297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.857666202761\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] Epoch[85] Batch [10]#011Speed: 174.79 samples/sec#011loss=0.857666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] processed a total of 2136 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16780.993938446045, \"sum\": 16780.993938446045, \"min\": 16780.993938446045}}, \"EndTime\": 1591460181.328267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460164.546716}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.285994012 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.851297205145\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] Epoch[85] Batch[5] avg_epoch_loss=0.845990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.845989707131\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:15 INFO 140027608049472] Epoch[85] Batch [5]#011Speed: 197.09 samples/sec#011loss=0.845990\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] Epoch[85] Batch[10] avg_epoch_loss=0.851297\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=0.857666202761\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] Epoch[85] Batch [10]#011Speed: 174.79 samples/sec#011loss=0.857666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] processed a total of 2136 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16780.993938446045, \"sum\": 16780.993938446045, \"min\": 16780.993938446045}}, \"EndTime\": 1591460181.328267, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460164.546716}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.285994012 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.851297205145\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:21 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:26 INFO 140027608049472] Epoch[86] Batch[0] avg_epoch_loss=0.821747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.821747258024\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] Epoch[86] Batch[5] avg_epoch_loss=0.841206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.841206376658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] Epoch[86] Batch [5]#011Speed: 197.75 samples/sec#011loss=0.841206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:26 INFO 140027608049472] Epoch[86] Batch[0] avg_epoch_loss=0.821747\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.821747258024\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] Epoch[86] Batch[5] avg_epoch_loss=0.841206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.841206376658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:32 INFO 140027608049472] Epoch[86] Batch [5]#011Speed: 197.75 samples/sec#011loss=0.841206\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] Epoch[86] Batch[10] avg_epoch_loss=0.869120\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.902616292126\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] Epoch[86] Batch [10]#011Speed: 175.55 samples/sec#011loss=0.902616\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] processed a total of 2163 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16741.270065307617, \"sum\": 16741.270065307617, \"min\": 16741.270065307617}}, \"EndTime\": 1591460198.070119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460181.328345}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.200717555 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.869119974598\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:43 INFO 140027608049472] Epoch[87] Batch[0] avg_epoch_loss=0.844016\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.844015805226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] Epoch[86] Batch[10] avg_epoch_loss=0.869120\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=0.902616292126\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] Epoch[86] Batch [10]#011Speed: 175.55 samples/sec#011loss=0.902616\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] processed a total of 2163 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16741.270065307617, \"sum\": 16741.270065307617, \"min\": 16741.270065307617}}, \"EndTime\": 1591460198.070119, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460181.328345}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=129.200717555 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.869119974598\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:38 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:43 INFO 140027608049472] Epoch[87] Batch[0] avg_epoch_loss=0.844016\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:43 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.844015805226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] Epoch[87] Batch[5] avg_epoch_loss=0.847213\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.847212545527\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] Epoch[87] Batch [5]#011Speed: 199.08 samples/sec#011loss=0.847213\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] processed a total of 2101 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15619.917154312134, \"sum\": 15619.917154312134, \"min\": 15619.917154312134}}, \"EndTime\": 1591460213.690576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460198.070202}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.506722961 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.854894587679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] Epoch[87] Batch[5] avg_epoch_loss=0.847213\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.847212545527\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:48 INFO 140027608049472] Epoch[87] Batch [5]#011Speed: 199.08 samples/sec#011loss=0.847213\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] processed a total of 2101 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15619.917154312134, \"sum\": 15619.917154312134, \"min\": 15619.917154312134}}, \"EndTime\": 1591460213.690576, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460198.070202}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.506722961 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.854894587679\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:53 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:59 INFO 140027608049472] Epoch[88] Batch[0] avg_epoch_loss=0.834887\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.834887306645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:59 INFO 140027608049472] Epoch[88] Batch[0] avg_epoch_loss=0.834887\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:16:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.834887306645\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] Epoch[88] Batch[5] avg_epoch_loss=0.827742\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.827742450642\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] Epoch[88] Batch [5]#011Speed: 196.30 samples/sec#011loss=0.827742\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Epoch[88] Batch[10] avg_epoch_loss=0.844937\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.865571536658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Epoch[88] Batch [10]#011Speed: 175.83 samples/sec#011loss=0.865572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] processed a total of 2138 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16772.61781692505, \"sum\": 16772.61781692505, \"min\": 16772.61781692505}}, \"EndTime\": 1591460230.463736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460213.690659}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.468829417 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.84493748974\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c1b5bfcd-a723-4599-b69c-8e2488144df4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.79710006713867, \"sum\": 55.79710006713867, \"min\": 55.79710006713867}}, \"EndTime\": 1591460230.520072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460230.463811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] Epoch[88] Batch[5] avg_epoch_loss=0.827742\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.827742450642\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:04 INFO 140027608049472] Epoch[88] Batch [5]#011Speed: 196.30 samples/sec#011loss=0.827742\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Epoch[88] Batch[10] avg_epoch_loss=0.844937\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.865571536658\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Epoch[88] Batch [10]#011Speed: 175.83 samples/sec#011loss=0.865572\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] processed a total of 2138 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16772.61781692505, \"sum\": 16772.61781692505, \"min\": 16772.61781692505}}, \"EndTime\": 1591460230.463736, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460213.690659}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.468829417 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.84493748974\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:10 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_c1b5bfcd-a723-4599-b69c-8e2488144df4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 55.79710006713867, \"sum\": 55.79710006713867, \"min\": 55.79710006713867}}, \"EndTime\": 1591460230.520072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460230.463811}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:15 INFO 140027608049472] Epoch[89] Batch[0] avg_epoch_loss=0.836911\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.836910751631\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] Epoch[89] Batch[5] avg_epoch_loss=0.837440\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.837440046874\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] Epoch[89] Batch [5]#011Speed: 198.70 samples/sec#011loss=0.837440\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:15 INFO 140027608049472] Epoch[89] Batch[0] avg_epoch_loss=0.836911\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.836910751631\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] Epoch[89] Batch[5] avg_epoch_loss=0.837440\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.837440046874\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:21 INFO 140027608049472] Epoch[89] Batch [5]#011Speed: 198.70 samples/sec#011loss=0.837440\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] Epoch[89] Batch[10] avg_epoch_loss=0.857205\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.880923015666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] Epoch[89] Batch [10]#011Speed: 174.59 samples/sec#011loss=0.880923\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] processed a total of 2136 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16760.838985443115, \"sum\": 16760.838985443115, \"min\": 16760.838985443115}}, \"EndTime\": 1591460247.281035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460230.520133}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.439061966 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.857205032689\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:32 INFO 140027608049472] Epoch[90] Batch[0] avg_epoch_loss=0.821630\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.821630441918\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] Epoch[89] Batch[10] avg_epoch_loss=0.857205\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.880923015666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] Epoch[89] Batch [10]#011Speed: 174.59 samples/sec#011loss=0.880923\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] processed a total of 2136 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16760.838985443115, \"sum\": 16760.838985443115, \"min\": 16760.838985443115}}, \"EndTime\": 1591460247.281035, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460230.520133}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=127.439061966 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.857205032689\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:27 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:32 INFO 140027608049472] Epoch[90] Batch[0] avg_epoch_loss=0.821630\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:32 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.821630441918\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] Epoch[90] Batch[5] avg_epoch_loss=0.836804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.836803988091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] Epoch[90] Batch [5]#011Speed: 199.38 samples/sec#011loss=0.836804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] processed a total of 2099 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15691.796064376831, \"sum\": 15691.796064376831, \"min\": 15691.796064376831}}, \"EndTime\": 1591460262.973352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460247.281113}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.763246499 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.839702534226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:43 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_aaeb9b94-d8fc-43e6-b076-aff5bda921b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.15504837036133, \"sum\": 45.15504837036133, \"min\": 45.15504837036133}}, \"EndTime\": 1591460263.019147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460262.973423}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] Epoch[90] Batch[5] avg_epoch_loss=0.836804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.836803988091\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:38 INFO 140027608049472] Epoch[90] Batch [5]#011Speed: 199.38 samples/sec#011loss=0.836804\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] processed a total of 2099 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15691.796064376831, \"sum\": 15691.796064376831, \"min\": 15691.796064376831}}, \"EndTime\": 1591460262.973352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460247.281113}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=133.763246499 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.839702534226\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:42 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:43 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_aaeb9b94-d8fc-43e6-b076-aff5bda921b2-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.15504837036133, \"sum\": 45.15504837036133, \"min\": 45.15504837036133}}, \"EndTime\": 1591460263.019147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460262.973423}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:48 INFO 140027608049472] Epoch[91] Batch[0] avg_epoch_loss=0.847184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.847183587416\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] Epoch[91] Batch[5] avg_epoch_loss=0.840521\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.840520642838\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] Epoch[91] Batch [5]#011Speed: 199.57 samples/sec#011loss=0.840521\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:48 INFO 140027608049472] Epoch[91] Batch[0] avg_epoch_loss=0.847184\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:48 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.847183587416\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] Epoch[91] Batch[5] avg_epoch_loss=0.840521\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.840520642838\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:53 INFO 140027608049472] Epoch[91] Batch [5]#011Speed: 199.57 samples/sec#011loss=0.840521\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] Epoch[91] Batch[10] avg_epoch_loss=0.864027\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.892233967331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] Epoch[91] Batch [10]#011Speed: 176.94 samples/sec#011loss=0.892234\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] processed a total of 2204 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16666.910886764526, \"sum\": 16666.910886764526, \"min\": 16666.910886764526}}, \"EndTime\": 1591460279.686179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460263.019216}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.23714106 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.864026699426\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:05 INFO 140027608049472] Epoch[92] Batch[0] avg_epoch_loss=0.839224\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.839224185584\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] Epoch[91] Batch[10] avg_epoch_loss=0.864027\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.892233967331\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] Epoch[91] Batch [10]#011Speed: 176.94 samples/sec#011loss=0.892234\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] processed a total of 2204 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16666.910886764526, \"sum\": 16666.910886764526, \"min\": 16666.910886764526}}, \"EndTime\": 1591460279.686179, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460263.019216}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.23714106 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.864026699426\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:17:59 INFO 140027608049472] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:05 INFO 140027608049472] Epoch[92] Batch[0] avg_epoch_loss=0.839224\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:05 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.839224185584\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] Epoch[92] Batch[5] avg_epoch_loss=0.842757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.842756523276\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] Epoch[92] Batch [5]#011Speed: 199.03 samples/sec#011loss=0.842757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] processed a total of 2080 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15638.978004455566, \"sum\": 15638.978004455566, \"min\": 15638.978004455566}}, \"EndTime\": 1591460295.325708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460279.686264}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.999972071 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.839529519711\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ef7d1117-111e-45c8-b851-ebdd1edf2f10-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.678117752075195, \"sum\": 42.678117752075195, \"min\": 42.678117752075195}}, \"EndTime\": 1591460295.369069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460295.325792}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] Epoch[92] Batch[5] avg_epoch_loss=0.842757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.842756523276\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:10 INFO 140027608049472] Epoch[92] Batch [5]#011Speed: 199.03 samples/sec#011loss=0.842757\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] processed a total of 2080 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15638.978004455566, \"sum\": 15638.978004455566, \"min\": 15638.978004455566}}, \"EndTime\": 1591460295.325708, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460279.686264}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.999972071 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.839529519711\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:15 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_ef7d1117-111e-45c8-b851-ebdd1edf2f10-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.678117752075195, \"sum\": 42.678117752075195, \"min\": 42.678117752075195}}, \"EndTime\": 1591460295.369069, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460295.325792}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:20 INFO 140027608049472] Epoch[93] Batch[0] avg_epoch_loss=0.840402\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.840402135309\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:20 INFO 140027608049472] Epoch[93] Batch[0] avg_epoch_loss=0.840402\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:20 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.840402135309\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] Epoch[93] Batch[5] avg_epoch_loss=0.839086\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.839085812839\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] Epoch[93] Batch [5]#011Speed: 200.34 samples/sec#011loss=0.839086\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] processed a total of 2106 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15614.656925201416, \"sum\": 15614.656925201416, \"min\": 15614.656925201416}}, \"EndTime\": 1591460310.983846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460295.369137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.872485373 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.836260676834\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:31 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_404b4e00-6242-4d32-a426-0d336a5b8dc7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.58610916137695, \"sum\": 45.58610916137695, \"min\": 45.58610916137695}}, \"EndTime\": 1591460311.029984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460310.983907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] Epoch[93] Batch[5] avg_epoch_loss=0.839086\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.839085812839\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:26 INFO 140027608049472] Epoch[93] Batch [5]#011Speed: 200.34 samples/sec#011loss=0.839086\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] processed a total of 2106 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15614.656925201416, \"sum\": 15614.656925201416, \"min\": 15614.656925201416}}, \"EndTime\": 1591460310.983846, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460295.369137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=134.872485373 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.836260676834\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:30 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:31 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_404b4e00-6242-4d32-a426-0d336a5b8dc7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 45.58610916137695, \"sum\": 45.58610916137695, \"min\": 45.58610916137695}}, \"EndTime\": 1591460311.029984, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460310.983907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:36 INFO 140027608049472] Epoch[94] Batch[0] avg_epoch_loss=0.851899\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.851898769163\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] Epoch[94] Batch[5] avg_epoch_loss=0.834832\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.834832353412\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] Epoch[94] Batch [5]#011Speed: 198.40 samples/sec#011loss=0.834832\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:36 INFO 140027608049472] Epoch[94] Batch[0] avg_epoch_loss=0.851899\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:36 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.851898769163\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] Epoch[94] Batch[5] avg_epoch_loss=0.834832\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.834832353412\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:41 INFO 140027608049472] Epoch[94] Batch [5]#011Speed: 198.40 samples/sec#011loss=0.834832\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] processed a total of 2085 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15680.377006530762, \"sum\": 15680.377006530762, \"min\": 15680.377006530762}}, \"EndTime\": 1591460326.710473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460311.030035}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.967690388 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.832741769755\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_4419e8dd-37dd-44e9-9690-42b853d90ee9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.94896125793457, \"sum\": 42.94896125793457, \"min\": 42.94896125793457}}, \"EndTime\": 1591460326.754105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460326.710557}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:52 INFO 140027608049472] Epoch[95] Batch[0] avg_epoch_loss=0.850817\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:52 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.850817338476\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] processed a total of 2085 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 15680.377006530762, \"sum\": 15680.377006530762, \"min\": 15680.377006530762}}, \"EndTime\": 1591460326.710473, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460311.030035}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=132.967690388 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.832741769755\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:46 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_4419e8dd-37dd-44e9-9690-42b853d90ee9-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 42.94896125793457, \"sum\": 42.94896125793457, \"min\": 42.94896125793457}}, \"EndTime\": 1591460326.754105, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460326.710557}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:52 INFO 140027608049472] Epoch[95] Batch[0] avg_epoch_loss=0.850817\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:52 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.850817338476\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] Epoch[95] Batch[5] avg_epoch_loss=0.832738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.832737556793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] Epoch[95] Batch [5]#011Speed: 199.40 samples/sec#011loss=0.832738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Epoch[95] Batch[10] avg_epoch_loss=0.831949\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=0.831003369025\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Epoch[95] Batch [10]#011Speed: 174.68 samples/sec#011loss=0.831003\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] processed a total of 2186 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16723.823070526123, \"sum\": 16723.823070526123, \"min\": 16723.823070526123}}, \"EndTime\": 1591460343.478062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460326.754173}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.710841043 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.831949289626\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_47022ec9-3123-4eb2-9024-51387162a1ea-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.1348876953125, \"sum\": 60.1348876953125, \"min\": 60.1348876953125}}, \"EndTime\": 1591460343.538746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460343.47814}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Final loss: 0.831949289626 (occurred at epoch 95)\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, train final_loss <loss>=0.831949289626\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 WARNING 140027608049472] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 673.975944519043, \"sum\": 673.975944519043, \"min\": 673.975944519043}}, \"EndTime\": 1591460344.213294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460343.538819}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 817.6989555358887, \"sum\": 817.6989555358887, \"min\": 817.6989555358887}}, \"EndTime\": 1591460344.356971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.213376}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 26.082992553710938, \"sum\": 26.082992553710938, \"min\": 26.082992553710938}}, \"EndTime\": 1591460344.383169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.357034}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.030040740966796875, \"sum\": 0.030040740966796875, \"min\": 0.030040740966796875}}, \"EndTime\": 1591460344.383902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.383223}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] Epoch[95] Batch[5] avg_epoch_loss=0.832738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.832737556793\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:18:57 INFO 140027608049472] Epoch[95] Batch [5]#011Speed: 199.40 samples/sec#011loss=0.832738\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Epoch[95] Batch[10] avg_epoch_loss=0.831949\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=0.831003369025\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Epoch[95] Batch [10]#011Speed: 174.68 samples/sec#011loss=0.831003\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] processed a total of 2186 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 16723.823070526123, \"sum\": 16723.823070526123, \"min\": 16723.823070526123}}, \"EndTime\": 1591460343.478062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460326.754173}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #throughput_metric: host=algo-1, train throughput=130.710841043 records/second\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.831949289626\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/state_47022ec9-3123-4eb2-9024-51387162a1ea-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 60.1348876953125, \"sum\": 60.1348876953125, \"min\": 60.1348876953125}}, \"EndTime\": 1591460343.538746, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460343.47814}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Final loss: 0.831949289626 (occurred at epoch 95)\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] #quality_metric: host=algo-1, train final_loss <loss>=0.831949289626\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 WARNING 140027608049472] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:03 INFO 140027608049472] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 673.975944519043, \"sum\": 673.975944519043, \"min\": 673.975944519043}}, \"EndTime\": 1591460344.213294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460343.538819}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 817.6989555358887, \"sum\": 817.6989555358887, \"min\": 817.6989555358887}}, \"EndTime\": 1591460344.356971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.213376}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 26.082992553710938, \"sum\": 26.082992553710938, \"min\": 26.082992553710938}}, \"EndTime\": 1591460344.383169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.357034}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:04 INFO 140027608049472] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.030040740966796875, \"sum\": 0.030040740966796875, \"min\": 0.030040740966796875}}, \"EndTime\": 1591460344.383902, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.383223}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-06 16:19:15 Uploading - Uploading generated training model\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 5775.040864944458, \"sum\": 5775.040864944458, \"min\": 5775.040864944458}}, \"EndTime\": 1591460350.158911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.383953}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, RMSE): 0.65366594624\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, mean_absolute_QuantileLoss): 190.9419305540621\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, mean_wQuantileLoss): 0.4150911533783959\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.1]): 0.30953743229741637\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.2]): 0.4464391830174819\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.3]): 0.5113558554584565\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.4]): 0.5276410217647967\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.5]): 0.5071769634585666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.6]): 0.46706262878101806\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.7]): 0.408618979262917\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.8]): 0.3298138740658759\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.9]): 0.22817444229903425\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.415091153378\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #quality_metric: host=algo-1, test RMSE <loss>=0.65366594624\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1566838.4191989899, \"sum\": 1566838.4191989899, \"min\": 1566838.4191989899}, \"setuptime\": {\"count\": 1, \"max\": 8.57400894165039, \"sum\": 8.57400894165039, \"min\": 8.57400894165039}}, \"EndTime\": 1591460350.201676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460350.158978}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-06 16:19:15 Uploading - Uploading generated training model\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 5775.040864944458, \"sum\": 5775.040864944458, \"min\": 5775.040864944458}}, \"EndTime\": 1591460350.158911, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460344.383953}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, RMSE): 0.65366594624\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, mean_absolute_QuantileLoss): 190.9419305540621\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, mean_wQuantileLoss): 0.4150911533783959\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.1]): 0.30953743229741637\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.2]): 0.4464391830174819\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.3]): 0.5113558554584565\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.4]): 0.5276410217647967\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.5]): 0.5071769634585666\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.6]): 0.46706262878101806\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.7]): 0.408618979262917\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.8]): 0.3298138740658759\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #test_score (algo-1, wQuantileLoss[0.9]): 0.22817444229903425\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.415091153378\u001b[0m\n",
      "\u001b[34m[06/06/2020 16:19:10 INFO 140027608049472] #quality_metric: host=algo-1, test RMSE <loss>=0.65366594624\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 1566838.4191989899, \"sum\": 1566838.4191989899, \"min\": 1566838.4191989899}, \"setuptime\": {\"count\": 1, \"max\": 8.57400894165039, \"sum\": 8.57400894165039, \"min\": 8.57400894165039}}, \"EndTime\": 1591460350.201676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591460350.158978}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-06 16:19:32 Completed - Training job completed\n",
      "\n",
      "2020-06-06 16:19:32 Completed - Training job completed\n",
      "Training seconds: 1636\n",
      "Billable seconds: 1636\n",
      "CPU times: user 4.07 s, sys: 206 ms, total: 4.27 s\n",
      "Wall time: 29min 29s\n",
      "Training seconds: 1636\n",
      "Billable seconds: 1636\n",
      "CPU times: user 4.07 s, sys: 206 ms, total: 4.27 s\n",
      "Wall time: 29min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator_hyper_param.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------!!"
     ]
    }
   ],
   "source": [
    "predictor_hyper_param = estimator_hyper_param.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['prediction'] = pred\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['prediction'] = pred\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>A</th>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>-0.626587</td>\n",
       "      <td>0.988258</td>\n",
       "      <td>-0.481819</td>\n",
       "      <td>-0.618605</td>\n",
       "      <td>-0.39746</td>\n",
       "      <td>0.378273</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1       PC2       PC3  \\\n",
       "Date       Ticker                                                      \n",
       "2019-01-02 A        0.016688  0.002458 -0.626587  0.988258 -0.481819   \n",
       "\n",
       "                        PC4      PC5       PC6  target  prediction  \n",
       "Date       Ticker                                                   \n",
       "2019-01-02 A      -0.618605 -0.39746  0.378273      -1          -1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <th>A</th>\n",
       "      <td>0.016688</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>-0.626587</td>\n",
       "      <td>0.988258</td>\n",
       "      <td>-0.481819</td>\n",
       "      <td>-0.618605</td>\n",
       "      <td>-0.39746</td>\n",
       "      <td>0.378273</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume       PC1       PC2       PC3  \\\n",
       "Date       Ticker                                                      \n",
       "2019-01-02 A        0.016688  0.002458 -0.626587  0.988258 -0.481819   \n",
       "\n",
       "                        PC4      PC5       PC6  target  prediction  \n",
       "Date       Ticker                                                   \n",
       "2019-01-02 A      -0.618605 -0.39746  0.378273      -1          -1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('A', '2019-01-02',stock_hyper_param_data,predictor_hyper_param, dynamic_feat, cat['A'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['prediction'] = pred\n",
      "/home/ec2-user/SageMaker/MLEND-Capstone-Project/helper.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_df['prediction'] = pred\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3373015873015873,\n",
       "             target  prediction\n",
       " 2019-01-02      -1           1\n",
       " 2019-01-03       1           0\n",
       " 2019-01-04      -1           1\n",
       " 2019-01-07       0           1\n",
       " 2019-01-08      -1           0\n",
       " ...            ...         ...\n",
       " 2019-12-24       0           0\n",
       " 2019-12-26       0           0\n",
       " 2019-12-27       0           0\n",
       " 2019-12-30       0           0\n",
       " 2019-12-31       0           0\n",
       " \n",
       " [252 rows x 2 columns])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.3373015873015873,\n",
       "             target  prediction\n",
       " 2019-01-02      -1           1\n",
       " 2019-01-03       1           0\n",
       " 2019-01-04      -1           1\n",
       " 2019-01-07       0           1\n",
       " 2019-01-08      -1           0\n",
       " ...            ...         ...\n",
       " 2019-12-24       0           0\n",
       " 2019-12-26       0           0\n",
       " 2019-12-27       0           0\n",
       " 2019-12-30       0           0\n",
       " 2019-12-31       0           0\n",
       " \n",
       " [252 rows x 2 columns])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_index = pd.read_csv('test_date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "get_prediction_accuracy('GOOGL', date_index, stock_hyper_param_data,predictor_hyper_param, dynamic_feat, cat['GOOGL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
