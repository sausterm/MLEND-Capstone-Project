{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4 as bs\n",
    "import pickle\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()             # IAM role to use by SageMaker\n",
    "\n",
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = 'MLEND-Capstone-Project'    \n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix)\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix)\n",
    "\n",
    "image_name = sagemaker.amazon.amazon_estimator.get_image_uri(region, \"forecasting-deepar\", \"latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data_preprocessed = pd.read_csv('stock_data_preprocessed.csv',parse_dates=True, index_col=[0,1])\n",
    "print(\"-1: \", stock_data_preprocessed[stock_data_preprocessed['target']==-1].size)\n",
    "print(\" 0: \", stock_data_preprocessed[stock_data_preprocessed['target']==0].size)\n",
    "print(\" 1: \", stock_data_preprocessed[stock_data_preprocessed['target']==1].size)\n",
    "#stock_data_preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use 2 hour frequency for the time series\n",
    "freq = 'D'\n",
    "\n",
    "# we predict for 7 days\n",
    "prediction_length = 1\n",
    "\n",
    "# we also use 7 days as context length, this is the number of state updates accomplished before making predictions\n",
    "context_length = 50\n",
    "\n",
    "end_training = pd.Timestamp(\"2018-12-31 00:00:00\", freq=freq)\n",
    "\n",
    "timeseries = []\n",
    "for ID,ticker in list(enumerate(tickers)):\n",
    "    ticker = stock_data_preprocessed.loc[(slice(None), ticker), :]\n",
    "    if ticker.index[0][0]<end_training:\n",
    "        timeseries.append(ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n"
     ]
    }
   ],
   "source": [
    "training_data = [\n",
    "    {\n",
    "            \"start\": str(ts.index[0][0]),\n",
    "            \"target\": ts['target'][ts.index[0][0]:end_training].tolist(), # We use -1, because pandas indexing includes the upper bound \n",
    "            \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training].values.T.tolist()\n",
    "    }\n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4910\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 10\n",
    "\n",
    "test_data = [\n",
    "    {\n",
    "        \"start\": str(ts.index[0][0]),\n",
    "        \"target\": ts['target'][ts.index[0][0]:end_training + (2*k * prediction_length)].tolist(),\n",
    "        \"dynamic_feat\": ts[['Adj Close','Volume']][ts.index[0][0]:end_training + (2*k * prediction_length)].values.T.tolist()\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1) \n",
    "    for ts in timeseries\n",
    "]\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_dataset(filename, data): \n",
    "    with open(filename, 'wb') as f:\n",
    "        # for each of our times series, there is one JSON line\n",
    "        for d in data:\n",
    "            json_line = json.dumps(d) + '\\n'\n",
    "            json_line = json_line.encode('utf-8')\n",
    "            f.write(json_line)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_json_dataset(\"train.json\", training_data)\n",
    "write_json_dataset(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith('s3://')\n",
    "    split = s3_path.split('/')\n",
    "    bucket = split[2]\n",
    "    path = '/'.join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "    \n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print('File s3://{}/{} already exists.\\nSet override to upload anyway.\\n'.format(s3_bucket, s3_path))\n",
    "            return\n",
    "        else:\n",
    "            print('Overwriting existing file')\n",
    "    with open(local_file, 'rb') as data:\n",
    "        print('Uploading file to {}'.format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/train/train.json\n",
      "Uploading file to s3://sagemaker-us-east-2-017500148529/MLEND-Capstone-Project/data/test/test.json\n",
      "CPU times: user 2.82 s, sys: 311 ms, total: 3.13 s\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"start\": \"2010-01-04 00:00:00\", \"target\": [-1, 0, 0, 0, 0, 0, -1, -1, -1, -1, -1, -1, -1, -1, -1, -...\n"
     ]
    }
   ],
   "source": [
    "s3filesystem = s3fs.S3FileSystem()\n",
    "with s3filesystem.open(s3_data_path + \"/train/train.json\", 'rb') as fp:\n",
    "    print(fp.readline().decode(\"utf-8\")[:100] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_name=image_name,\n",
    "    role=role,\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.c4.2xlarge',\n",
    "    base_job_name='MLEND-Capstone-Project',\n",
    "    output_path=s3_output_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"100\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_dynamic_feat\": 'auto',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-01 21:26:26 Starting - Starting the training job...\n",
      "2020-06-01 21:26:28 Starting - Launching requested ML instances......\n",
      "2020-06-01 21:27:31 Starting - Preparing the instances for training...\n",
      "2020-06-01 21:28:19 Downloading - Downloading input data...\n",
      "2020-06-01 21:28:45 Training - Downloading the training image..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'num_dynamic_feat': u'auto', u'learning_rate': u'5E-4', u'prediction_length': u'1', u'epochs': u'100', u'time_freq': u'D', u'context_length': u'50', u'mini_batch_size': u'64', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'5E-4', u'num_layers': u'2', u'epochs': u'100', u'embedding_dimension': u'10', u'num_cells': u'40', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'1', u'time_freq': u'D', u'context_length': u'50', u'_kvstore': u'auto', u'early_stopping_patience': u'40'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:07 INFO 140176366790464] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=2 from dataset.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Training set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] number of time series: 491\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] number of observations: 1074589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] mean target length: 2188\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] min/mean/max target: -1.0/0.0764683055568/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] mean abs(target): 0.698935127756\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:08 INFO 140176366790464] Small number of time series. Doing 2 passes over dataset with prob 0.651731160896 per epoch.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Test set statistics:\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Real time series\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] number of time series: 4910\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] number of observations: 10781660\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] mean target length: 2195\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] min/mean/max target: -1.0/0.078016464997/1.0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] mean abs(target): 0.699434317165\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] contains missing values: no\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] nvidia-smi took: 0.0251898765564 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:11 INFO 140176366790464] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 237.9310131072998, \"sum\": 237.9310131072998, \"min\": 237.9310131072998}}, \"EndTime\": 1591046952.135294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046951.896421}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:12 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 454.9570083618164, \"sum\": 454.9570083618164, \"min\": 454.9570083618164}}, \"EndTime\": 1591046952.35154, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046952.135377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch[0] avg_epoch_loss=1.434263\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=1.43426251411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch[5] avg_epoch_loss=1.347553\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=1.34755325317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Epoch[0] Batch [5]#011Speed: 1057.48 samples/sec#011loss=1.347553\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 100, \"sum\": 100.0, \"min\": 100}, \"update.time\": {\"count\": 1, \"max\": 1520.5039978027344, \"sum\": 1520.5039978027344, \"min\": 1520.5039978027344}}, \"EndTime\": 1591046953.872186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046952.351601}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=406.412889916 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #progress_metric: host=algo-1, completed 1 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=0, train loss <loss>=1.33785898685\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:13 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_5ca4bba6-b581-4608-99b1-c634cbc6fd74-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.88897132873535, \"sum\": 21.88897132873535, \"min\": 21.88897132873535}}, \"EndTime\": 1591046953.894778, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046953.872269}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch[0] avg_epoch_loss=1.307173\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=1.30717325211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch[5] avg_epoch_loss=1.274586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=1.27458610137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:14 INFO 140176366790464] Epoch[1] Batch [5]#011Speed: 955.40 samples/sec#011loss=1.274586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.981035232544, \"sum\": 1435.981035232544, \"min\": 1435.981035232544}}, \"EndTime\": 1591046955.330912, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046953.894858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.989672728 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=1, train loss <loss>=1.28925626278\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:15 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_4784194a-534a-4cc3-9991-313d0e9e070d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.56996726989746, \"sum\": 21.56996726989746, \"min\": 21.56996726989746}}, \"EndTime\": 1591046955.353148, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046955.330997}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-01 21:29:05 Training - Training image download completed. Training in progress.\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch[0] avg_epoch_loss=1.323897\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=1.32389700413\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch[5] avg_epoch_loss=1.301920\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=1.30192011595\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:16 INFO 140176366790464] Epoch[2] Batch [5]#011Speed: 856.68 samples/sec#011loss=1.301920\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[2] Batch[10] avg_epoch_loss=1.275736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=1.24431614876\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[2] Batch [10]#011Speed: 509.51 samples/sec#011loss=1.244316\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1665.7500267028809, \"sum\": 1665.7500267028809, \"min\": 1665.7500267028809}}, \"EndTime\": 1591046957.019038, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046955.353216}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=401.592202613 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 3 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=2, train loss <loss>=1.2757364945\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_cbb51694-2816-4cbc-a53b-e9332d5c27b1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.693220138549805, \"sum\": 31.693220138549805, \"min\": 31.693220138549805}}, \"EndTime\": 1591046957.051301, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046957.019118}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch[0] avg_epoch_loss=1.203594\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=1.20359432697\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch[5] avg_epoch_loss=1.217710\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=1.21770979961\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:17 INFO 140176366790464] Epoch[3] Batch [5]#011Speed: 1025.84 samples/sec#011loss=1.217710\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Epoch[3] Batch[10] avg_epoch_loss=1.210092\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=1.20095090866\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Epoch[3] Batch [10]#011Speed: 551.13 samples/sec#011loss=1.200951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.8831119537354, \"sum\": 1490.8831119537354, \"min\": 1490.8831119537354}}, \"EndTime\": 1591046958.542325, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046957.051375}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.679527929 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=3, train loss <loss>=1.2100921219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:18 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b9b64dda-4a21-4d76-ad11-68c0fb8e4068-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.612001419067383, \"sum\": 20.612001419067383, \"min\": 20.612001419067383}}, \"EndTime\": 1591046958.563548, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046958.542405}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch[0] avg_epoch_loss=1.175823\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=1.17582261562\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch[5] avg_epoch_loss=1.188703\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=1.18870335817\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] Epoch[4] Batch [5]#011Speed: 1051.15 samples/sec#011loss=1.188703\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1418.821096420288, \"sum\": 1418.821096420288, \"min\": 1418.821096420288}}, \"EndTime\": 1591046959.982502, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046958.563615}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.220008062 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=4, train loss <loss>=1.18140784502\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:19 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_0db4e8a0-1d16-4b68-9c06-b5ddbc638c73-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.331829071044922, \"sum\": 27.331829071044922, \"min\": 27.331829071044922}}, \"EndTime\": 1591046960.010443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046959.982587}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch[0] avg_epoch_loss=1.216788\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=1.21678829193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch[5] avg_epoch_loss=1.202264\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=1.20226403077\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:20 INFO 140176366790464] Epoch[5] Batch [5]#011Speed: 1033.04 samples/sec#011loss=1.202264\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1455.3709030151367, \"sum\": 1455.3709030151367, \"min\": 1455.3709030151367}}, \"EndTime\": 1591046961.465959, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046960.010515}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.902983998 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=5, train loss <loss>=1.17771522999\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:21 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_6c540192-de02-4353-ac66-05a0c230a269-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.270980834960938, \"sum\": 31.270980834960938, \"min\": 31.270980834960938}}, \"EndTime\": 1591046961.497824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046961.466043}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch[0] avg_epoch_loss=1.206661\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=1.20666122437\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch[5] avg_epoch_loss=1.149882\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=1.14988209804\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:22 INFO 140176366790464] Epoch[6] Batch [5]#011Speed: 1040.98 samples/sec#011loss=1.149882\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[6] Batch[10] avg_epoch_loss=1.148364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=1.14654290676\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[6] Batch [10]#011Speed: 526.90 samples/sec#011loss=1.146543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.862102508545, \"sum\": 1520.862102508545, \"min\": 1520.862102508545}}, \"EndTime\": 1591046963.018824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046961.497897}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.732063104 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 7 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=6, train loss <loss>=1.14836428382\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_180e14ef-68ac-4cda-95d9-1babde6633e1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 33.22196006774902, \"sum\": 33.22196006774902, \"min\": 33.22196006774902}}, \"EndTime\": 1591046963.052633, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046963.018922}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch[0] avg_epoch_loss=1.096255\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=1.09625530243\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch[5] avg_epoch_loss=1.125320\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=1.125319918\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:23 INFO 140176366790464] Epoch[7] Batch [5]#011Speed: 989.42 samples/sec#011loss=1.125320\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1430.624008178711, \"sum\": 1430.624008178711, \"min\": 1430.624008178711}}, \"EndTime\": 1591046964.483405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046963.052712}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.338901291 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=7, train loss <loss>=1.12926626205\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:24 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_7a62479f-6d6c-4272-962e-fdd7d7aab96e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.73502540588379, \"sum\": 20.73502540588379, \"min\": 20.73502540588379}}, \"EndTime\": 1591046964.504816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046964.48349}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[0] avg_epoch_loss=1.076855\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=1.07685542107\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[5] avg_epoch_loss=1.104612\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=1.10461242994\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch [5]#011Speed: 1059.97 samples/sec#011loss=1.104612\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch[10] avg_epoch_loss=1.091296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=1.0753166914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] Epoch[8] Batch [10]#011Speed: 547.63 samples/sec#011loss=1.075317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1473.8872051239014, \"sum\": 1473.8872051239014, \"min\": 1473.8872051239014}}, \"EndTime\": 1591046965.978834, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046964.504888}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=465.402461675 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #progress_metric: host=algo-1, completed 9 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=8, train loss <loss>=1.09129618515\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:25 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_eeb65e78-7f8f-4418-9ee7-d47ece5cef28-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.09305763244629, \"sum\": 22.09305763244629, \"min\": 22.09305763244629}}, \"EndTime\": 1591046966.001504, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046965.978903}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch[0] avg_epoch_loss=1.095278\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=1.09527790546\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch[5] avg_epoch_loss=1.059614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=1.05961447954\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:26 INFO 140176366790464] Epoch[9] Batch [5]#011Speed: 1032.40 samples/sec#011loss=1.059614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1490.45991897583, \"sum\": 1490.45991897583, \"min\": 1490.45991897583}}, \"EndTime\": 1591046967.49208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046966.001562}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=422.654313178 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=9, train loss <loss>=1.07141076326\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:27 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_f62ae336-7240-4652-a154-d834ca5f4fa6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.196998596191406, \"sum\": 32.196998596191406, \"min\": 32.196998596191406}}, \"EndTime\": 1591046967.52489, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046967.492162}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch[0] avg_epoch_loss=1.059746\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=1.05974590778\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch[5] avg_epoch_loss=1.055233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=1.05523314079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Epoch[10] Batch [5]#011Speed: 1047.59 samples/sec#011loss=1.055233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.3737716674805, \"sum\": 1422.3737716674805, \"min\": 1422.3737716674805}}, \"EndTime\": 1591046968.947409, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046967.524967}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=432.347176113 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #progress_metric: host=algo-1, completed 11 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=10, train loss <loss>=1.05586122274\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:28 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e57c85bc-d58c-4101-b5a8-9c3bc9ababaa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.725011825561523, \"sum\": 20.725011825561523, \"min\": 20.725011825561523}}, \"EndTime\": 1591046968.968726, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046968.947471}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch[0] avg_epoch_loss=1.057597\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=1.05759680271\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch[5] avg_epoch_loss=1.058986\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=1.05898612738\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:29 INFO 140176366790464] Epoch[11] Batch [5]#011Speed: 1028.71 samples/sec#011loss=1.058986\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Epoch[11] Batch[10] avg_epoch_loss=0.981045\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=0.887515249848\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Epoch[11] Batch [10]#011Speed: 519.53 samples/sec#011loss=0.887515\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.25394821167, \"sum\": 1516.25394821167, \"min\": 1516.25394821167}}, \"EndTime\": 1591046970.485108, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046968.968794}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=427.997471735 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=11, train loss <loss>=0.981044819409\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:30 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c389346c-b50d-4d84-8a21-6d5ddbf90e1a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.47800064086914, \"sum\": 30.47800064086914, \"min\": 30.47800064086914}}, \"EndTime\": 1591046970.516098, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046970.485183}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch[0] avg_epoch_loss=1.046480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=1.04647958279\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch[5] avg_epoch_loss=1.043967\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=1.04396746556\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:31 INFO 140176366790464] Epoch[12] Batch [5]#011Speed: 879.94 samples/sec#011loss=1.043967\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1527.3561477661133, \"sum\": 1527.3561477661133, \"min\": 1527.3561477661133}}, \"EndTime\": 1591046972.043584, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046970.516167}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=400.005766457 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 13 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=12, train loss <loss>=1.03745931983\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] Epoch[13] Batch[0] avg_epoch_loss=1.040768\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=1.0407679081\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] Epoch[13] Batch[5] avg_epoch_loss=1.000455\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=1.00045485298\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] Epoch[13] Batch [5]#011Speed: 520.63 samples/sec#011loss=1.000455\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1444.3769454956055, \"sum\": 1444.3769454956055, \"min\": 1444.3769454956055}}, \"EndTime\": 1591046973.488524, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046972.043669}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.745486259 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=13, train loss <loss>=1.00560665131\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:33 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch[0] avg_epoch_loss=0.973218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=0.973218262196\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch[5] avg_epoch_loss=1.018670\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=1.01867032051\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:34 INFO 140176366790464] Epoch[14] Batch [5]#011Speed: 999.10 samples/sec#011loss=1.018670\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[14] Batch[10] avg_epoch_loss=1.036511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=1.05791945457\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[14] Batch [10]#011Speed: 520.46 samples/sec#011loss=1.057919\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.0660648345947, \"sum\": 1526.0660648345947, \"min\": 1526.0660648345947}}, \"EndTime\": 1591046975.01518, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046973.488634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.933729411 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=14, train loss <loss>=1.03651083599\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch[0] avg_epoch_loss=1.023366\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=1.0233656168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch[5] avg_epoch_loss=0.998777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=0.99877657493\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:35 INFO 140176366790464] Epoch[15] Batch [5]#011Speed: 1029.44 samples/sec#011loss=0.998777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] Epoch[15] Batch[10] avg_epoch_loss=1.005851\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=1.01434094906\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] Epoch[15] Batch [10]#011Speed: 512.18 samples/sec#011loss=1.014341\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] processed a total of 654 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1542.576789855957, \"sum\": 1542.576789855957, \"min\": 1542.576789855957}}, \"EndTime\": 1591046976.558272, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046975.01526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.936180413 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=15, train loss <loss>=1.00585129044\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:36 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch[0] avg_epoch_loss=0.974487\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=0.974487304688\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch[5] avg_epoch_loss=0.974869\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=0.97486932079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:37 INFO 140176366790464] Epoch[16] Batch [5]#011Speed: 993.04 samples/sec#011loss=0.974869\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.9577884674072, \"sum\": 1486.9577884674072, \"min\": 1486.9577884674072}}, \"EndTime\": 1591046978.045735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046976.558343}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.665832862 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 17 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=16, train loss <loss>=0.974238604307\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_5d588f61-d101-4ffa-97c6-ad60c7344677-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.818058013916016, \"sum\": 29.818058013916016, \"min\": 29.818058013916016}}, \"EndTime\": 1591046978.076143, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046978.045821}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch[0] avg_epoch_loss=1.008034\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=1.00803422928\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch[5] avg_epoch_loss=0.996452\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=0.996452351411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:38 INFO 140176366790464] Epoch[17] Batch [5]#011Speed: 1044.86 samples/sec#011loss=0.996452\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] Epoch[17] Batch[10] avg_epoch_loss=0.993349\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=0.989624071121\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] Epoch[17] Batch [10]#011Speed: 530.68 samples/sec#011loss=0.989624\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1505.0280094146729, \"sum\": 1505.0280094146729, \"min\": 1505.0280094146729}}, \"EndTime\": 1591046979.581294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046978.076207}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=449.792952386 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=17, train loss <loss>=0.993348587643\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:39 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch[0] avg_epoch_loss=0.944226\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=0.94422596693\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch[5] avg_epoch_loss=0.968781\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=0.968780736128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:40 INFO 140176366790464] Epoch[18] Batch [5]#011Speed: 1046.89 samples/sec#011loss=0.968781\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[18] Batch[10] avg_epoch_loss=0.969385\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=0.970109069347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[18] Batch [10]#011Speed: 537.83 samples/sec#011loss=0.970109\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1493.8538074493408, \"sum\": 1493.8538074493408, \"min\": 1493.8538074493408}}, \"EndTime\": 1591046981.07561, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046979.581366}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=456.503286232 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #progress_metric: host=algo-1, completed 19 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=18, train loss <loss>=0.969384523955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_d227e5ad-530b-40a2-8438-efa9d09c6f71-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.90423011779785, \"sum\": 21.90423011779785, \"min\": 21.90423011779785}}, \"EndTime\": 1591046981.098114, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046981.075683}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] Epoch[19] Batch[0] avg_epoch_loss=0.960261\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=0.960261046886\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch[5] avg_epoch_loss=0.944997\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=0.944997131824\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch [5]#011Speed: 1023.09 samples/sec#011loss=0.944997\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch[10] avg_epoch_loss=0.984822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=1.03261078596\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] Epoch[19] Batch [10]#011Speed: 496.46 samples/sec#011loss=1.032611\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1567.7821636199951, \"sum\": 1567.7821636199951, \"min\": 1567.7821636199951}}, \"EndTime\": 1591046982.666057, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046981.098201}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=414.567522956 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=19, train loss <loss>=0.984821520068\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:42 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch[0] avg_epoch_loss=0.900181\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=0.90018093586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch[5] avg_epoch_loss=0.950742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=0.95074219505\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:43 INFO 140176366790464] Epoch[20] Batch [5]#011Speed: 1005.57 samples/sec#011loss=0.950742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] processed a total of 609 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1469.6581363677979, \"sum\": 1469.6581363677979, \"min\": 1469.6581363677979}}, \"EndTime\": 1591046984.136249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046982.666135}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=414.347129542 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #progress_metric: host=algo-1, completed 21 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=20, train loss <loss>=0.951052796841\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_db3f2e13-5fef-4282-9274-7847fec9bc1e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.32009506225586, \"sum\": 31.32009506225586, \"min\": 31.32009506225586}}, \"EndTime\": 1591046984.168199, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046984.136335}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] Epoch[21] Batch[0] avg_epoch_loss=0.859429\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=0.859429001808\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Epoch[21] Batch[5] avg_epoch_loss=0.910801\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=0.910800645749\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Epoch[21] Batch [5]#011Speed: 1013.13 samples/sec#011loss=0.910801\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1478.4798622131348, \"sum\": 1478.4798622131348, \"min\": 1478.4798622131348}}, \"EndTime\": 1591046985.646816, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046984.168272}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.137787481 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=21, train loss <loss>=0.937435770035\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:45 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_88e11870-345d-40a1-aa8c-4ce4df868a22-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 29.582977294921875, \"sum\": 29.582977294921875, \"min\": 29.582977294921875}}, \"EndTime\": 1591046985.676963, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046985.646897}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch[0] avg_epoch_loss=0.900517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=0.90051728487\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch[5] avg_epoch_loss=0.904150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=0.90415028731\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:46 INFO 140176366790464] Epoch[22] Batch [5]#011Speed: 979.91 samples/sec#011loss=0.904150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1517.3249244689941, \"sum\": 1517.3249244689941, \"min\": 1517.3249244689941}}, \"EndTime\": 1591046987.194438, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046985.677041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=415.828115958 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #progress_metric: host=algo-1, completed 23 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=22, train loss <loss>=0.915119498968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_93c67a9b-49d9-44c7-8154-e864f7b3b793-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 19.981861114501953, \"sum\": 19.981861114501953, \"min\": 19.981861114501953}}, \"EndTime\": 1591046987.21509, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046987.194525}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] Epoch[23] Batch[0] avg_epoch_loss=0.934589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=0.934589147568\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch[5] avg_epoch_loss=0.893955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=0.893955220779\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch [5]#011Speed: 1019.85 samples/sec#011loss=0.893955\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch[10] avg_epoch_loss=0.895388\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, batch=10 train loss <loss>=0.897106790543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Epoch[23] Batch [10]#011Speed: 536.78 samples/sec#011loss=0.897107\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] processed a total of 680 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.322093963623, \"sum\": 1510.322093963623, \"min\": 1510.322093963623}}, \"EndTime\": 1591046988.725555, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046987.21517}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=450.201400929 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=23, train loss <loss>=0.89538775249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:48 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e742f2d4-2c49-4bed-b448-b72961016676-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.193981170654297, \"sum\": 21.193981170654297, \"min\": 21.193981170654297}}, \"EndTime\": 1591046988.747294, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046988.725628}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch[0] avg_epoch_loss=0.989229\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=0.989228904247\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch[5] avg_epoch_loss=0.916364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=0.916364053885\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:49 INFO 140176366790464] Epoch[24] Batch [5]#011Speed: 1013.31 samples/sec#011loss=0.916364\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[24] Batch[10] avg_epoch_loss=0.932889\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=0.952719438076\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[24] Batch [10]#011Speed: 546.80 samples/sec#011loss=0.952719\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1500.5691051483154, \"sum\": 1500.5691051483154, \"min\": 1500.5691051483154}}, \"EndTime\": 1591046990.247988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046988.747359}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=436.466174564 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=24, train loss <loss>=0.932889228517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] Epoch[25] Batch[0] avg_epoch_loss=0.871741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=0.871741056442\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch[5] avg_epoch_loss=0.896741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=0.896740913391\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch [5]#011Speed: 1015.28 samples/sec#011loss=0.896741\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch[10] avg_epoch_loss=0.882382\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=0.865150702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Epoch[25] Batch [10]#011Speed: 524.81 samples/sec#011loss=0.865151\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.7550525665283, \"sum\": 1520.7550525665283, \"min\": 1520.7550525665283}}, \"EndTime\": 1591046991.769274, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046990.248069}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=424.098242235 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=25, train loss <loss>=0.882381726395\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:51 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_2eeff84a-8846-4632-9d4b-7dfb985b7aa8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 27.378082275390625, \"sum\": 27.378082275390625, \"min\": 27.378082275390625}}, \"EndTime\": 1591046991.797239, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046991.769355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:52 INFO 140176366790464] Epoch[26] Batch[0] avg_epoch_loss=0.900884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=0.90088391304\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[26] Batch[5] avg_epoch_loss=0.930150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=0.930150111516\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[26] Batch [5]#011Speed: 513.46 samples/sec#011loss=0.930150\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.180856704712, \"sum\": 1508.180856704712, \"min\": 1508.180856704712}}, \"EndTime\": 1591046993.305568, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046991.797317}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=397.798325935 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #progress_metric: host=algo-1, completed 27 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=26, train loss <loss>=0.919858849049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] Epoch[27] Batch[0] avg_epoch_loss=0.872662\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=0.872662246227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Epoch[27] Batch[5] avg_epoch_loss=0.880638\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=0.880638460318\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Epoch[27] Batch [5]#011Speed: 1033.57 samples/sec#011loss=0.880638\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1451.3449668884277, \"sum\": 1451.3449668884277, \"min\": 1451.3449668884277}}, \"EndTime\": 1591046994.757464, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046993.305651}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=423.019173411 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=27, train loss <loss>=0.857738488913\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:54 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b407efdd-c237-4b12-ac97-97d70583c0ad-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.90289306640625, \"sum\": 23.90289306640625, \"min\": 23.90289306640625}}, \"EndTime\": 1591046994.781988, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046994.75755}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch[0] avg_epoch_loss=0.805616\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=0.805615723133\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch[5] avg_epoch_loss=0.833884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=0.83388414979\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:55 INFO 140176366790464] Epoch[28] Batch [5]#011Speed: 1053.10 samples/sec#011loss=0.833884\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] processed a total of 602 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1402.8701782226562, \"sum\": 1402.8701782226562, \"min\": 1402.8701782226562}}, \"EndTime\": 1591046996.184978, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046994.782041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.086631121 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #progress_metric: host=algo-1, completed 29 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=28, train loss <loss>=0.824995923042\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_af48909c-b47f-41c5-aa56-8357005b75d4-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.855924606323242, \"sum\": 23.855924606323242, \"min\": 23.855924606323242}}, \"EndTime\": 1591046996.209454, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046996.185046}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] Epoch[29] Batch[0] avg_epoch_loss=0.852328\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=0.852328002453\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch[5] avg_epoch_loss=0.813227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=0.813226789236\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch [5]#011Speed: 1041.36 samples/sec#011loss=0.813227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch[10] avg_epoch_loss=0.829030\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=0.847994363308\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] Epoch[29] Batch [10]#011Speed: 546.26 samples/sec#011loss=0.847994\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1489.7451400756836, \"sum\": 1489.7451400756836, \"min\": 1489.7451400756836}}, \"EndTime\": 1591046997.69934, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046996.209526}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=452.393943758 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=29, train loss <loss>=0.829030231996\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:57 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch[0] avg_epoch_loss=0.797326\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=0.797326207161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch[5] avg_epoch_loss=0.826752\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=0.826751907667\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:58 INFO 140176366790464] Epoch[30] Batch [5]#011Speed: 1062.37 samples/sec#011loss=0.826752\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[30] Batch[10] avg_epoch_loss=0.772874\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, batch=10 train loss <loss>=0.70821980834\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[30] Batch [10]#011Speed: 548.04 samples/sec#011loss=0.708220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1472.1829891204834, \"sum\": 1472.1829891204834, \"min\": 1472.1829891204834}}, \"EndTime\": 1591046999.172072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046997.69941}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.962559449 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #progress_metric: host=algo-1, completed 31 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=30, train loss <loss>=0.7728736807\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_40fd83f6-240e-429b-bf24-f4cadd3079e8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.91193199157715, \"sum\": 20.91193199157715, \"min\": 20.91193199157715}}, \"EndTime\": 1591046999.193551, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046999.172136}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] Epoch[31] Batch[0] avg_epoch_loss=0.690408\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:29:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=0.690408170223\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch[5] avg_epoch_loss=0.771321\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=0.771320551634\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch [5]#011Speed: 1057.79 samples/sec#011loss=0.771321\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch[10] avg_epoch_loss=0.776506\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=0.782727503777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] Epoch[31] Batch [10]#011Speed: 506.31 samples/sec#011loss=0.782728\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1525.5892276763916, \"sum\": 1525.5892276763916, \"min\": 1525.5892276763916}}, \"EndTime\": 1591047000.719277, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591046999.193627}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.310304845 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=31, train loss <loss>=0.776505529881\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:00 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch[0] avg_epoch_loss=0.747332\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=0.747332274914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch[5] avg_epoch_loss=0.777958\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=0.777958105008\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:01 INFO 140176366790464] Epoch[32] Batch [5]#011Speed: 904.56 samples/sec#011loss=0.777958\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1547.8250980377197, \"sum\": 1547.8250980377197, \"min\": 1547.8250980377197}}, \"EndTime\": 1591047002.26761, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047000.719355}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=392.781348354 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #progress_metric: host=algo-1, completed 33 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=32, train loss <loss>=0.760727983713\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_0bd85a49-98a3-4a76-bde0-48584aa7952b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.591995239257812, \"sum\": 23.591995239257812, \"min\": 23.591995239257812}}, \"EndTime\": 1591047002.29186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047002.267688}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] Epoch[33] Batch[0] avg_epoch_loss=0.679262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=0.679261684418\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Epoch[33] Batch[5] avg_epoch_loss=0.704604\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=0.704604069392\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Epoch[33] Batch [5]#011Speed: 972.71 samples/sec#011loss=0.704604\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1465.4648303985596, \"sum\": 1465.4648303985596, \"min\": 1465.4648303985596}}, \"EndTime\": 1591047003.757483, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047002.291946}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=422.361273366 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=33, train loss <loss>=0.758519339561\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:03 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_3f172a59-3fdd-4b04-8a33-587e9660c683-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.65699005126953, \"sum\": 21.65699005126953, \"min\": 21.65699005126953}}, \"EndTime\": 1591047003.779776, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047003.757547}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch[0] avg_epoch_loss=0.685365\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=0.68536490202\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch[5] avg_epoch_loss=0.711852\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=0.711852282286\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:04 INFO 140176366790464] Epoch[34] Batch [5]#011Speed: 1047.69 samples/sec#011loss=0.711852\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[34] Batch[10] avg_epoch_loss=0.718220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=0.72586145401\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[34] Batch [10]#011Speed: 535.75 samples/sec#011loss=0.725861\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1519.247055053711, \"sum\": 1519.247055053711, \"min\": 1519.247055053711}}, \"EndTime\": 1591047005.29915, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047003.779839}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=458.74699171 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #progress_metric: host=algo-1, completed 35 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=34, train loss <loss>=0.718220087615\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_703bd155-b559-472a-8eea-88f581a3d661-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 30.399084091186523, \"sum\": 30.399084091186523, \"min\": 30.399084091186523}}, \"EndTime\": 1591047005.33008, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047005.299225}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] Epoch[35] Batch[0] avg_epoch_loss=0.663839\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=0.66383934021\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch[5] avg_epoch_loss=0.678925\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=0.678924550613\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch [5]#011Speed: 1026.90 samples/sec#011loss=0.678925\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch[10] avg_epoch_loss=0.638208\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=0.589347708225\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Epoch[35] Batch [10]#011Speed: 494.47 samples/sec#011loss=0.589348\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1580.7859897613525, \"sum\": 1580.7859897613525, \"min\": 1580.7859897613525}}, \"EndTime\": 1591047006.911003, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047005.330153}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.86854421 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=35, train loss <loss>=0.638207804073\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:06 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_1f88cf85-aa6a-4a59-bb88-f0712d0bcf36-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.71007537841797, \"sum\": 32.71007537841797, \"min\": 32.71007537841797}}, \"EndTime\": 1591047006.944249, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047006.911082}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch[0] avg_epoch_loss=0.784701\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=0.7847006917\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch[5] avg_epoch_loss=0.709399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=0.70939947168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:07 INFO 140176366790464] Epoch[36] Batch [5]#011Speed: 922.08 samples/sec#011loss=0.709399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] Epoch[36] Batch[10] avg_epoch_loss=0.642862\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, batch=10 train loss <loss>=0.563017517328\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] Epoch[36] Batch [10]#011Speed: 498.17 samples/sec#011loss=0.563018\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1616.5931224822998, \"sum\": 1616.5931224822998, \"min\": 1616.5931224822998}}, \"EndTime\": 1591047008.560985, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047006.944326}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.648318169 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #progress_metric: host=algo-1, completed 37 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=36, train loss <loss>=0.642862219702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:08 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch[0] avg_epoch_loss=0.639017\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=0.639016926289\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch[5] avg_epoch_loss=0.659143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=0.659143437942\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:09 INFO 140176366790464] Epoch[37] Batch [5]#011Speed: 1011.03 samples/sec#011loss=0.659143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[37] Batch[10] avg_epoch_loss=0.640500\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=0.618128192425\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[37] Batch [10]#011Speed: 529.71 samples/sec#011loss=0.618128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1532.477855682373, \"sum\": 1532.477855682373, \"min\": 1532.477855682373}}, \"EndTime\": 1591047010.093961, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047008.561064}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=418.245414824 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=37, train loss <loss>=0.640500144525\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch[0] avg_epoch_loss=0.569648\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=0.569647967815\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch[5] avg_epoch_loss=0.622329\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=0.622328837713\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:10 INFO 140176366790464] Epoch[38] Batch [5]#011Speed: 1050.82 samples/sec#011loss=0.622329\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1462.6328945159912, \"sum\": 1462.6328945159912, \"min\": 1462.6328945159912}}, \"EndTime\": 1591047011.557092, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047010.094039}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.694496738 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #progress_metric: host=algo-1, completed 39 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=38, train loss <loss>=0.608988711238\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:11 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_1130916f-95dc-4ed1-b09c-44ef7fd39402-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.601055145263672, \"sum\": 23.601055145263672, \"min\": 23.601055145263672}}, \"EndTime\": 1591047011.58131, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047011.557175}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch[0] avg_epoch_loss=0.636542\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=0.63654178381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch[5] avg_epoch_loss=0.626477\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=0.626477410396\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:12 INFO 140176366790464] Epoch[39] Batch [5]#011Speed: 1027.83 samples/sec#011loss=0.626477\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1454.9999237060547, \"sum\": 1454.9999237060547, \"min\": 1454.9999237060547}}, \"EndTime\": 1591047013.036443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047011.581377}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=424.700465469 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=39, train loss <loss>=0.644965153933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch[0] avg_epoch_loss=0.650846\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=0.650845944881\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch[5] avg_epoch_loss=0.589878\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=0.589877973\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:13 INFO 140176366790464] Epoch[40] Batch [5]#011Speed: 1053.75 samples/sec#011loss=0.589878\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] Epoch[40] Batch[10] avg_epoch_loss=0.628673\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, batch=10 train loss <loss>=0.675227642059\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] Epoch[40] Batch [10]#011Speed: 525.85 samples/sec#011loss=0.675228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1520.4601287841797, \"sum\": 1520.4601287841797, \"min\": 1520.4601287841797}}, \"EndTime\": 1591047014.55748, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047013.036527}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.785957162 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #progress_metric: host=algo-1, completed 41 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=40, train loss <loss>=0.628673277118\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:14 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch[0] avg_epoch_loss=0.540145\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=0.540144562721\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch[5] avg_epoch_loss=0.614805\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=0.61480542024\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:15 INFO 140176366790464] Epoch[41] Batch [5]#011Speed: 1033.56 samples/sec#011loss=0.614805\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1445.2619552612305, \"sum\": 1445.2619552612305, \"min\": 1445.2619552612305}}, \"EndTime\": 1591047016.003262, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047014.557554}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.495966072 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=41, train loss <loss>=0.632378613949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch[0] avg_epoch_loss=0.599517\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=0.599517285824\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch[5] avg_epoch_loss=0.578702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=0.578702489535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:16 INFO 140176366790464] Epoch[42] Batch [5]#011Speed: 1044.48 samples/sec#011loss=0.578702\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Epoch[42] Batch[10] avg_epoch_loss=0.608220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=0.643640971184\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Epoch[42] Batch [10]#011Speed: 515.94 samples/sec#011loss=0.643641\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.4539813995361, \"sum\": 1516.4539813995361, \"min\": 1516.4539813995361}}, \"EndTime\": 1591047017.520317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047016.003334}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.144838229 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 43 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=42, train loss <loss>=0.608219981194\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:17 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_6b355eb6-5353-42d8-960f-ab8c3271abf7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.00419044494629, \"sum\": 31.00419044494629, \"min\": 31.00419044494629}}, \"EndTime\": 1591047017.551926, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047017.52041}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch[0] avg_epoch_loss=0.652531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=0.652530789375\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch[5] avg_epoch_loss=0.583306\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=0.583305845658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:18 INFO 140176366790464] Epoch[43] Batch [5]#011Speed: 1051.99 samples/sec#011loss=0.583306\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[43] Batch[10] avg_epoch_loss=0.660428\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=0.752974402905\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[43] Batch [10]#011Speed: 526.94 samples/sec#011loss=0.752974\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1512.1538639068604, \"sum\": 1512.1538639068604, \"min\": 1512.1538639068604}}, \"EndTime\": 1591047019.064214, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047017.551996}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=435.768134248 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=43, train loss <loss>=0.660427917134\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch[0] avg_epoch_loss=0.417933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=0.417933017015\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch[5] avg_epoch_loss=0.573763\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=0.573763405283\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:19 INFO 140176366790464] Epoch[44] Batch [5]#011Speed: 1026.28 samples/sec#011loss=0.573763\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Epoch[44] Batch[10] avg_epoch_loss=0.598684\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, batch=10 train loss <loss>=0.628589510918\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Epoch[44] Batch [10]#011Speed: 540.49 samples/sec#011loss=0.628590\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1512.887954711914, \"sum\": 1512.887954711914, \"min\": 1512.887954711914}}, \"EndTime\": 1591047020.577655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047019.064294}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=450.764550887 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #progress_metric: host=algo-1, completed 45 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=44, train loss <loss>=0.59868436239\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e33839ee-ad1c-4816-8189-c82dd7dfa918-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.82316780090332, \"sum\": 21.82316780090332, \"min\": 21.82316780090332}}, \"EndTime\": 1591047020.600062, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047020.577721}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch[0] avg_epoch_loss=0.526484\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=0.526483535767\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch[5] avg_epoch_loss=0.554193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=0.554193456968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:21 INFO 140176366790464] Epoch[45] Batch [5]#011Speed: 1038.26 samples/sec#011loss=0.554193\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.3970851898193, \"sum\": 1426.3970851898193, \"min\": 1426.3970851898193}}, \"EndTime\": 1591047022.026577, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047020.600122}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.244747105 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=45, train loss <loss>=0.534400874376\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_31c5899b-2c77-40cb-9d36-88d22e292c8e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.10004425048828, \"sum\": 21.10004425048828, \"min\": 21.10004425048828}}, \"EndTime\": 1591047022.04832, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047022.026652}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch[0] avg_epoch_loss=0.542186\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=0.542185842991\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch[5] avg_epoch_loss=0.497872\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=0.497871950269\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:22 INFO 140176366790464] Epoch[46] Batch [5]#011Speed: 1056.82 samples/sec#011loss=0.497872\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] Epoch[46] Batch[10] avg_epoch_loss=0.563041\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, batch=10 train loss <loss>=0.641244953871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] Epoch[46] Batch [10]#011Speed: 545.60 samples/sec#011loss=0.641245\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] processed a total of 682 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1475.7030010223389, \"sum\": 1475.7030010223389, \"min\": 1475.7030010223389}}, \"EndTime\": 1591047023.524171, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047022.048396}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=462.115955993 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 47 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=46, train loss <loss>=0.563041497361\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:23 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch[0] avg_epoch_loss=0.617388\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=0.617388248444\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch[5] avg_epoch_loss=0.500635\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=0.500635142128\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:24 INFO 140176366790464] Epoch[47] Batch [5]#011Speed: 1039.63 samples/sec#011loss=0.500635\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[47] Batch[10] avg_epoch_loss=0.549917\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, batch=10 train loss <loss>=0.609054267406\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[47] Batch [10]#011Speed: 528.87 samples/sec#011loss=0.609054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] processed a total of 657 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1524.4121551513672, \"sum\": 1524.4121551513672, \"min\": 1524.4121551513672}}, \"EndTime\": 1591047025.049136, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047023.52425}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.953317705 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=47, train loss <loss>=0.549916562709\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] Epoch[48] Batch[0] avg_epoch_loss=0.512581\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=0.512580811977\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch[5] avg_epoch_loss=0.496037\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=0.496037264665\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch [5]#011Speed: 1010.11 samples/sec#011loss=0.496037\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch[10] avg_epoch_loss=0.458899\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=0.414332565665\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Epoch[48] Batch [10]#011Speed: 522.70 samples/sec#011loss=0.414333\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1564.054012298584, \"sum\": 1564.054012298584, \"min\": 1564.054012298584}}, \"EndTime\": 1591047026.61372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047025.049214}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=417.476189482 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #progress_metric: host=algo-1, completed 49 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=48, train loss <loss>=0.45889876512\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:26 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_f72693f2-b942-4b5e-97fa-d4867b0d40f6-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 23.675918579101562, \"sum\": 23.675918579101562, \"min\": 23.675918579101562}}, \"EndTime\": 1591047026.638051, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047026.613789}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch[0] avg_epoch_loss=0.516531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=0.516531229019\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch[5] avg_epoch_loss=0.484970\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=0.484969869256\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:27 INFO 140176366790464] Epoch[49] Batch [5]#011Speed: 1029.26 samples/sec#011loss=0.484970\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[49] Batch[10] avg_epoch_loss=0.448160\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=0.403988751769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[49] Batch [10]#011Speed: 539.28 samples/sec#011loss=0.403989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] processed a total of 675 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1504.9200057983398, \"sum\": 1504.9200057983398, \"min\": 1504.9200057983398}}, \"EndTime\": 1591047028.14313, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047026.638139}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.493368254 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=49, train loss <loss>=0.448160270398\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_7b3377e1-061d-4122-8fd8-2112da2b3f53-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.9808349609375, \"sum\": 20.9808349609375, \"min\": 20.9808349609375}}, \"EndTime\": 1591047028.164716, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047028.14321}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] Epoch[50] Batch[0] avg_epoch_loss=0.587347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=0.587346613407\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Epoch[50] Batch[5] avg_epoch_loss=0.542049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=0.54204852879\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Epoch[50] Batch [5]#011Speed: 1036.25 samples/sec#011loss=0.542049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1423.9718914031982, \"sum\": 1423.9718914031982, \"min\": 1423.9718914031982}}, \"EndTime\": 1591047029.588825, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047028.164786}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=412.194249202 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #progress_metric: host=algo-1, completed 51 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=50, train loss <loss>=0.439182299376\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:29 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_a91c38a4-3cee-482f-b660-a18741e75db7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.697044372558594, \"sum\": 21.697044372558594, \"min\": 21.697044372558594}}, \"EndTime\": 1591047029.611176, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047029.588898}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch[0] avg_epoch_loss=0.562171\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=0.562170863152\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch[5] avg_epoch_loss=0.504951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=0.504951417446\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:30 INFO 140176366790464] Epoch[51] Batch [5]#011Speed: 1015.42 samples/sec#011loss=0.504951\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[51] Batch[10] avg_epoch_loss=0.506237\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=0.507779818773\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[51] Batch [10]#011Speed: 522.18 samples/sec#011loss=0.507780\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] processed a total of 683 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1524.31321144104, \"sum\": 1524.31321144104, \"min\": 1524.31321144104}}, \"EndTime\": 1591047031.135638, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047029.611256}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.035673823 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=51, train loss <loss>=0.506237054413\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] Epoch[52] Batch[0] avg_epoch_loss=0.312347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=0.312346637249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch[5] avg_epoch_loss=0.424499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=0.424499183893\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch [5]#011Speed: 1025.74 samples/sec#011loss=0.424499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch[10] avg_epoch_loss=0.420736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=0.416219693422\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Epoch[52] Batch [10]#011Speed: 524.62 samples/sec#011loss=0.416220\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] processed a total of 670 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1554.2430877685547, \"sum\": 1554.2430877685547, \"min\": 1554.2430877685547}}, \"EndTime\": 1591047032.690423, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047031.135716}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.049433997 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 53 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=52, train loss <loss>=0.420735779134\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:32 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c4eb0e6f-c735-47a6-94ff-d6624037f5fa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 22.22895622253418, \"sum\": 22.22895622253418, \"min\": 22.22895622253418}}, \"EndTime\": 1591047032.713197, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047032.690487}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch[0] avg_epoch_loss=0.636280\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=0.63628000021\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch[5] avg_epoch_loss=0.493469\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=0.493469024698\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:33 INFO 140176366790464] Epoch[53] Batch [5]#011Speed: 1002.17 samples/sec#011loss=0.493469\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[53] Batch[10] avg_epoch_loss=0.446379\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, batch=10 train loss <loss>=0.389871093631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[53] Batch [10]#011Speed: 530.24 samples/sec#011loss=0.389871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1515.8979892730713, \"sum\": 1515.8979892730713, \"min\": 1515.8979892730713}}, \"EndTime\": 1591047034.229281, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047032.713275}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.956515148 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=53, train loss <loss>=0.446379056031\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] Epoch[54] Batch[0] avg_epoch_loss=0.585544\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=0.585544168949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] Epoch[54] Batch[5] avg_epoch_loss=0.506531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=0.506531119347\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] Epoch[54] Batch [5]#011Speed: 1047.04 samples/sec#011loss=0.506531\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1434.4329833984375, \"sum\": 1434.4329833984375, \"min\": 1434.4329833984375}}, \"EndTime\": 1591047035.664246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047034.229362}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=420.339113214 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 55 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=54, train loss <loss>=0.531550344825\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch[0] avg_epoch_loss=0.521139\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=0.52113866806\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch[5] avg_epoch_loss=0.443499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=0.443498805165\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:36 INFO 140176366790464] Epoch[55] Batch [5]#011Speed: 1017.70 samples/sec#011loss=0.443499\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1437.1540546417236, \"sum\": 1437.1540546417236, \"min\": 1437.1540546417236}}, \"EndTime\": 1591047037.102044, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047035.664329}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=420.938053999 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=55, train loss <loss>=0.485042539239\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch[0] avg_epoch_loss=0.519255\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=0.519254803658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch[5] avg_epoch_loss=0.441065\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=0.441064625978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:37 INFO 140176366790464] Epoch[56] Batch [5]#011Speed: 1039.01 samples/sec#011loss=0.441065\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] Epoch[56] Batch[10] avg_epoch_loss=0.500867\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, batch=10 train loss <loss>=0.572630298138\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] Epoch[56] Batch [10]#011Speed: 550.95 samples/sec#011loss=0.572630\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1470.2210426330566, \"sum\": 1470.2210426330566, \"min\": 1470.2210426330566}}, \"EndTime\": 1591047038.572881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047037.102116}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=440.034273745 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 57 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=56, train loss <loss>=0.500867204233\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:38 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch[0] avg_epoch_loss=0.522297\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=0.522297024727\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch[5] avg_epoch_loss=0.448813\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=0.448813100656\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:39 INFO 140176366790464] Epoch[57] Batch [5]#011Speed: 1052.69 samples/sec#011loss=0.448813\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[57] Batch[10] avg_epoch_loss=0.612590\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, batch=10 train loss <loss>=0.809122800827\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[57] Batch [10]#011Speed: 542.01 samples/sec#011loss=0.809123\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1477.4610996246338, \"sum\": 1477.4610996246338, \"min\": 1477.4610996246338}}, \"EndTime\": 1591047040.050907, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047038.57296}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.81596759 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=57, train loss <loss>=0.612590237097\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch[0] avg_epoch_loss=0.539978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=0.539977669716\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch[5] avg_epoch_loss=0.517978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=0.517978390058\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:40 INFO 140176366790464] Epoch[58] Batch [5]#011Speed: 1011.95 samples/sec#011loss=0.517978\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] processed a total of 601 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.361083984375, \"sum\": 1426.361083984375, \"min\": 1426.361083984375}}, \"EndTime\": 1591047041.477856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047040.050991}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=421.319185828 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #progress_metric: host=algo-1, completed 59 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=58, train loss <loss>=0.464243760705\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:41 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch[0] avg_epoch_loss=0.443170\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=0.443169862032\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch[5] avg_epoch_loss=0.476053\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=0.476052607099\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] Epoch[59] Batch [5]#011Speed: 1056.49 samples/sec#011loss=0.476053\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] processed a total of 604 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1407.660961151123, \"sum\": 1407.660961151123, \"min\": 1407.660961151123}}, \"EndTime\": 1591047042.886101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047041.477925}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.037864796 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=59, train loss <loss>=0.529165032506\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:42 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch[0] avg_epoch_loss=0.403933\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=0.403933376074\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch[5] avg_epoch_loss=0.428443\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=0.428443282843\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:43 INFO 140176366790464] Epoch[60] Batch [5]#011Speed: 1052.21 samples/sec#011loss=0.428443\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1420.5610752105713, \"sum\": 1420.5610752105713, \"min\": 1420.5610752105713}}, \"EndTime\": 1591047044.307232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047042.886201}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=419.523596181 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #progress_metric: host=algo-1, completed 61 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=60, train loss <loss>=0.461417669058\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] Epoch[61] Batch[0] avg_epoch_loss=0.348168\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:44 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=0.348167806864\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch[5] avg_epoch_loss=0.394218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=0.394217674931\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch [5]#011Speed: 1047.05 samples/sec#011loss=0.394218\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch[10] avg_epoch_loss=0.361949\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, batch=10 train loss <loss>=0.323226645589\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Epoch[61] Batch [10]#011Speed: 528.64 samples/sec#011loss=0.323227\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1496.3538646697998, \"sum\": 1496.3538646697998, \"min\": 1496.3538646697998}}, \"EndTime\": 1591047045.804147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047044.307295}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.014327707 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] #quality_metric: host=algo-1, epoch=61, train loss <loss>=0.36194902523\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:45 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e017a7af-40bf-4007-80b7-cf44632c9840-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.041860580444336, \"sum\": 31.041860580444336, \"min\": 31.041860580444336}}, \"EndTime\": 1591047045.835785, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047045.804226}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch[0] avg_epoch_loss=0.405998\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=0.405998051167\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch[5] avg_epoch_loss=0.416228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=0.416227628787\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:46 INFO 140176366790464] Epoch[62] Batch [5]#011Speed: 953.08 samples/sec#011loss=0.416228\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1516.9329643249512, \"sum\": 1516.9329643249512, \"min\": 1516.9329643249512}}, \"EndTime\": 1591047047.352857, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047045.835858}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=410.004385983 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #progress_metric: host=algo-1, completed 63 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=62, train loss <loss>=0.453421983123\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] Epoch[63] Batch[0] avg_epoch_loss=0.408436\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:47 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=0.408436208963\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch[5] avg_epoch_loss=0.402242\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=0.402241925399\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch [5]#011Speed: 1045.92 samples/sec#011loss=0.402242\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch[10] avg_epoch_loss=0.415737\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=0.431930136681\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] Epoch[63] Batch [10]#011Speed: 532.80 samples/sec#011loss=0.431930\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1521.9471454620361, \"sum\": 1521.9471454620361, \"min\": 1521.9471454620361}}, \"EndTime\": 1591047048.875357, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047047.352942}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.104771633 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] #quality_metric: host=algo-1, epoch=63, train loss <loss>=0.41573656689\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:48 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch[0] avg_epoch_loss=0.516642\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=0.516642093658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch[5] avg_epoch_loss=0.459528\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=0.459528103471\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:49 INFO 140176366790464] Epoch[64] Batch [5]#011Speed: 1046.48 samples/sec#011loss=0.459528\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] Epoch[64] Batch[10] avg_epoch_loss=0.398084\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, batch=10 train loss <loss>=0.324351853132\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] Epoch[64] Batch [10]#011Speed: 540.92 samples/sec#011loss=0.324352\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1527.1978378295898, \"sum\": 1527.1978378295898, \"min\": 1527.1978378295898}}, \"EndTime\": 1591047050.403065, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047048.875437}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.823050706 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #progress_metric: host=algo-1, completed 65 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] #quality_metric: host=algo-1, epoch=64, train loss <loss>=0.398084353317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:50 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch[0] avg_epoch_loss=0.355190\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=0.355190455914\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch[5] avg_epoch_loss=0.404535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=0.404534791907\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] Epoch[65] Batch [5]#011Speed: 1057.11 samples/sec#011loss=0.404535\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1437.5288486480713, \"sum\": 1437.5288486480713, \"min\": 1437.5288486480713}}, \"EndTime\": 1591047051.841138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047050.403137}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.519210951 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] #quality_metric: host=algo-1, epoch=65, train loss <loss>=0.416255965829\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:51 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch[0] avg_epoch_loss=0.345558\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=0.34555798769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch[5] avg_epoch_loss=0.396587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=0.396586852769\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:52 INFO 140176366790464] Epoch[66] Batch [5]#011Speed: 969.49 samples/sec#011loss=0.396587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] Epoch[66] Batch[10] avg_epoch_loss=0.376723\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, batch=10 train loss <loss>=0.352887293696\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] Epoch[66] Batch [10]#011Speed: 528.33 samples/sec#011loss=0.352887\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1586.1620903015137, \"sum\": 1586.1620903015137, \"min\": 1586.1620903015137}}, \"EndTime\": 1591047053.427831, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047051.841221}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=409.132306593 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #progress_metric: host=algo-1, completed 67 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] #quality_metric: host=algo-1, epoch=66, train loss <loss>=0.376723416827\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:53 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[0] avg_epoch_loss=0.474371\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=0.474370598793\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[5] avg_epoch_loss=0.440296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=0.440295691291\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch [5]#011Speed: 1049.52 samples/sec#011loss=0.440296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch[10] avg_epoch_loss=0.369463\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, batch=10 train loss <loss>=0.284464401007\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] Epoch[67] Batch [10]#011Speed: 541.68 samples/sec#011loss=0.284464\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1486.0057830810547, \"sum\": 1486.0057830810547, \"min\": 1486.0057830810547}}, \"EndTime\": 1591047054.914367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047053.427911}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=444.113986193 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] #quality_metric: host=algo-1, epoch=67, train loss <loss>=0.369463286617\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:54 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch[0] avg_epoch_loss=0.355923\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=0.355922937393\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch[5] avg_epoch_loss=0.388219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=0.388218899568\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:55 INFO 140176366790464] Epoch[68] Batch [5]#011Speed: 1043.86 samples/sec#011loss=0.388219\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[68] Batch[10] avg_epoch_loss=0.378072\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, batch=10 train loss <loss>=0.365894705057\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[68] Batch [10]#011Speed: 538.87 samples/sec#011loss=0.365895\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] processed a total of 665 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1485.0718975067139, \"sum\": 1485.0718975067139, \"min\": 1485.0718975067139}}, \"EndTime\": 1591047056.39997, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047054.914433}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.7532507 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #progress_metric: host=algo-1, completed 69 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=68, train loss <loss>=0.378071538427\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] Epoch[69] Batch[0] avg_epoch_loss=0.525512\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:56 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=0.52551150322\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Epoch[69] Batch[5] avg_epoch_loss=0.379894\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=0.379894350966\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Epoch[69] Batch [5]#011Speed: 1037.18 samples/sec#011loss=0.379894\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1425.5170822143555, \"sum\": 1425.5170822143555, \"min\": 1425.5170822143555}}, \"EndTime\": 1591047057.826033, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047056.400049}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.497100325 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] #quality_metric: host=algo-1, epoch=69, train loss <loss>=0.349440449476\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:57 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_e243123e-7368-44e8-b7bc-42f404be0891-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.690202713012695, \"sum\": 20.690202713012695, \"min\": 20.690202713012695}}, \"EndTime\": 1591047057.847335, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047057.826099}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch[0] avg_epoch_loss=0.400950\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=0.400950103998\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch[5] avg_epoch_loss=0.397989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=0.397989327709\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:58 INFO 140176366790464] Epoch[70] Batch [5]#011Speed: 1032.68 samples/sec#011loss=0.397989\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1435.9488487243652, \"sum\": 1435.9488487243652, \"min\": 1435.9488487243652}}, \"EndTime\": 1591047059.283412, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047057.847402}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=445.660663605 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #progress_metric: host=algo-1, completed 71 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=70, train loss <loss>=0.385269227624\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] Epoch[71] Batch[0] avg_epoch_loss=0.390674\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:30:59 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=0.390673756599\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch[5] avg_epoch_loss=0.448717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=0.448717311025\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch [5]#011Speed: 1038.02 samples/sec#011loss=0.448717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch[10] avg_epoch_loss=0.421969\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, batch=10 train loss <loss>=0.389870655537\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] Epoch[71] Batch [10]#011Speed: 535.95 samples/sec#011loss=0.389871\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1515.0949954986572, \"sum\": 1515.0949954986572, \"min\": 1515.0949954986572}}, \"EndTime\": 1591047060.799046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047059.283496}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=429.642831922 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] #quality_metric: host=algo-1, epoch=71, train loss <loss>=0.421968831257\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:00 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch[0] avg_epoch_loss=0.471578\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=0.471578240395\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch[5] avg_epoch_loss=0.369161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=0.369160860777\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:01 INFO 140176366790464] Epoch[72] Batch [5]#011Speed: 1013.41 samples/sec#011loss=0.369161\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1482.5160503387451, \"sum\": 1482.5160503387451, \"min\": 1482.5160503387451}}, \"EndTime\": 1591047062.2821, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047060.799125}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=416.829398046 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #progress_metric: host=algo-1, completed 73 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=72, train loss <loss>=0.382378193736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] Epoch[73] Batch[0] avg_epoch_loss=0.337137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:02 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=0.337137371302\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch[5] avg_epoch_loss=0.374672\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=0.374671916167\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch [5]#011Speed: 1020.31 samples/sec#011loss=0.374672\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch[10] avg_epoch_loss=0.477594\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=0.601100707054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] Epoch[73] Batch [10]#011Speed: 544.68 samples/sec#011loss=0.601101\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1480.7050228118896, \"sum\": 1480.7050228118896, \"min\": 1480.7050228118896}}, \"EndTime\": 1591047063.763321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047062.28217}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.21770767 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #progress_metric: host=algo-1, completed 74 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] #quality_metric: host=algo-1, epoch=73, train loss <loss>=0.477594093843\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:03 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch[0] avg_epoch_loss=0.290773\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=0.290773153305\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch[5] avg_epoch_loss=0.304497\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=0.30449689428\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:04 INFO 140176366790464] Epoch[74] Batch [5]#011Speed: 1044.18 samples/sec#011loss=0.304497\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[74] Batch[10] avg_epoch_loss=0.342296\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, batch=10 train loss <loss>=0.387655472755\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[74] Batch [10]#011Speed: 518.28 samples/sec#011loss=0.387655\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1508.5639953613281, \"sum\": 1508.5639953613281, \"min\": 1508.5639953613281}}, \"EndTime\": 1591047065.272417, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047063.7634}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=430.84105976 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #progress_metric: host=algo-1, completed 75 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=74, train loss <loss>=0.342296248133\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_013f5611-ef0a-4b87-92e5-f572d50a8871-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.178951263427734, \"sum\": 31.178951263427734, \"min\": 31.178951263427734}}, \"EndTime\": 1591047065.304184, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047065.272494}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] Epoch[75] Batch[0] avg_epoch_loss=0.473744\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:05 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=0.473743975163\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch[5] avg_epoch_loss=0.422812\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=0.422811823587\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch [5]#011Speed: 1022.76 samples/sec#011loss=0.422812\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch[10] avg_epoch_loss=0.432398\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=0.443900996447\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] Epoch[75] Batch [10]#011Speed: 534.88 samples/sec#011loss=0.443901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1540.1380062103271, \"sum\": 1540.1380062103271, \"min\": 1540.1380062103271}}, \"EndTime\": 1591047066.844462, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047065.304257}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=418.106353266 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #progress_metric: host=algo-1, completed 76 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] #quality_metric: host=algo-1, epoch=75, train loss <loss>=0.43239781125\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:06 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch[0] avg_epoch_loss=0.473007\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=0.473006844521\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch[5] avg_epoch_loss=0.423262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=0.423261702061\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:07 INFO 140176366790464] Epoch[76] Batch [5]#011Speed: 1039.57 samples/sec#011loss=0.423262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1423.961877822876, \"sum\": 1423.961877822876, \"min\": 1423.961877822876}}, \"EndTime\": 1591047068.268999, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047066.844563}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=437.479410076 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #progress_metric: host=algo-1, completed 77 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=76, train loss <loss>=0.389875084162\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] Epoch[77] Batch[0] avg_epoch_loss=0.322719\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:08 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=0.322719097137\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] Epoch[77] Batch[5] avg_epoch_loss=0.378475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=0.378475437562\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] Epoch[77] Batch [5]#011Speed: 1053.76 samples/sec#011loss=0.378475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1431.480884552002, \"sum\": 1431.480884552002, \"min\": 1431.480884552002}}, \"EndTime\": 1591047069.701046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047068.26907}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=435.875043342 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #progress_metric: host=algo-1, completed 78 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] #quality_metric: host=algo-1, epoch=77, train loss <loss>=0.39159155488\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:09 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch[0] avg_epoch_loss=0.406873\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=0.406873047352\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch[5] avg_epoch_loss=0.348675\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=0.348674642543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:10 INFO 140176366790464] Epoch[78] Batch [5]#011Speed: 1059.96 samples/sec#011loss=0.348675\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[78] Batch[10] avg_epoch_loss=0.352660\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=0.357442069054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[78] Batch [10]#011Speed: 487.22 samples/sec#011loss=0.357442\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1570.1401233673096, \"sum\": 1570.1401233673096, \"min\": 1570.1401233673096}}, \"EndTime\": 1591047071.271717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047069.701129}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=427.955697814 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #progress_metric: host=algo-1, completed 79 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=78, train loss <loss>=0.352659836411\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] Epoch[79] Batch[0] avg_epoch_loss=0.286215\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:11 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=0.286214739084\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Epoch[79] Batch[5] avg_epoch_loss=0.309093\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=0.309093082945\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Epoch[79] Batch [5]#011Speed: 877.88 samples/sec#011loss=0.309093\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1513.5161876678467, \"sum\": 1513.5161876678467, \"min\": 1513.5161876678467}}, \"EndTime\": 1591047072.785735, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047071.271796}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=416.876420812 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #progress_metric: host=algo-1, completed 80 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] #quality_metric: host=algo-1, epoch=79, train loss <loss>=0.340787410736\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:12 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_83b90d2e-54a0-495a-9cf0-e31a1ab772d5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 32.57298469543457, \"sum\": 32.57298469543457, \"min\": 32.57298469543457}}, \"EndTime\": 1591047072.818917, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047072.785819}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch[0] avg_epoch_loss=0.305913\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=0.305913299322\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch[5] avg_epoch_loss=0.338112\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=0.338111894826\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:13 INFO 140176366790464] Epoch[80] Batch [5]#011Speed: 1041.21 samples/sec#011loss=0.338112\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[80] Batch[10] avg_epoch_loss=0.285901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, batch=10 train loss <loss>=0.223248907924\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[80] Batch [10]#011Speed: 535.74 samples/sec#011loss=0.223249\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] processed a total of 666 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1511.6100311279297, \"sum\": 1511.6100311279297, \"min\": 1511.6100311279297}}, \"EndTime\": 1591047074.330673, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047072.818994}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=440.556610936 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #progress_metric: host=algo-1, completed 81 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=80, train loss <loss>=0.285901446234\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c91fb5b5-91b5-4826-8464-e399c6197e87-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.08001708984375, \"sum\": 21.08001708984375, \"min\": 21.08001708984375}}, \"EndTime\": 1591047074.352346, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047074.330748}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] Epoch[81] Batch[0] avg_epoch_loss=0.331740\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:14 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=0.331739842892\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch[5] avg_epoch_loss=0.388631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=0.388631403446\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch [5]#011Speed: 1038.01 samples/sec#011loss=0.388631\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch[10] avg_epoch_loss=0.359602\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, batch=10 train loss <loss>=0.324767175317\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] Epoch[81] Batch [10]#011Speed: 531.54 samples/sec#011loss=0.324767\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] processed a total of 689 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1504.5628547668457, \"sum\": 1504.5628547668457, \"min\": 1504.5628547668457}}, \"EndTime\": 1591047075.857043, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047074.352412}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=457.897367626 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #progress_metric: host=algo-1, completed 82 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] #quality_metric: host=algo-1, epoch=81, train loss <loss>=0.359602208842\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:15 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch[0] avg_epoch_loss=0.272171\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=0.272170960903\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch[5] avg_epoch_loss=0.355211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=0.355210602283\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:16 INFO 140176366790464] Epoch[82] Batch [5]#011Speed: 1042.88 samples/sec#011loss=0.355211\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1430.3970336914062, \"sum\": 1430.3970336914062, \"min\": 1430.3970336914062}}, \"EndTime\": 1591047077.288031, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047075.857141}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=443.18615664 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #progress_metric: host=algo-1, completed 83 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=82, train loss <loss>=0.377050116658\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] Epoch[83] Batch[0] avg_epoch_loss=0.376422\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:17 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=0.376422405243\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] Epoch[83] Batch[5] avg_epoch_loss=0.416822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=0.416822135448\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] Epoch[83] Batch [5]#011Speed: 827.71 samples/sec#011loss=0.416822\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] processed a total of 623 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1526.9649028778076, \"sum\": 1526.9649028778076, \"min\": 1526.9649028778076}}, \"EndTime\": 1591047078.815676, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047077.288144}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=407.969533878 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #progress_metric: host=algo-1, completed 84 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] #quality_metric: host=algo-1, epoch=83, train loss <loss>=0.415880054235\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:18 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch[0] avg_epoch_loss=0.268475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=0.268475234509\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch[5] avg_epoch_loss=0.294922\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=0.29492200911\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:19 INFO 140176366790464] Epoch[84] Batch [5]#011Speed: 1057.35 samples/sec#011loss=0.294922\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1401.9479751586914, \"sum\": 1401.9479751586914, \"min\": 1401.9479751586914}}, \"EndTime\": 1591047080.218232, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047078.815753}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=447.195004693 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #progress_metric: host=algo-1, completed 85 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=84, train loss <loss>=0.26953766644\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_c04774ce-bc6e-4d12-b743-04909a761792-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.13509178161621, \"sum\": 21.13509178161621, \"min\": 21.13509178161621}}, \"EndTime\": 1591047080.239971, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047080.218316}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] Epoch[85] Batch[0] avg_epoch_loss=0.503598\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:20 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=0.503598093987\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] Epoch[85] Batch[5] avg_epoch_loss=0.325381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=0.325381316245\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] Epoch[85] Batch [5]#011Speed: 1013.55 samples/sec#011loss=0.325381\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] processed a total of 633 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1439.296007156372, \"sum\": 1439.296007156372, \"min\": 1439.296007156372}}, \"EndTime\": 1591047081.679405, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047080.240044}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.764469938 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #progress_metric: host=algo-1, completed 86 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] #quality_metric: host=algo-1, epoch=85, train loss <loss>=0.304662819207\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:21 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch[0] avg_epoch_loss=0.396947\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=0.396947056055\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch[5] avg_epoch_loss=0.346896\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=0.346895505985\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:22 INFO 140176366790464] Epoch[86] Batch [5]#011Speed: 1037.63 samples/sec#011loss=0.346896\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1418.9798831939697, \"sum\": 1418.9798831939697, \"min\": 1418.9798831939697}}, \"EndTime\": 1591047083.098941, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047081.679482}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=446.766821576 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #progress_metric: host=algo-1, completed 87 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=86, train loss <loss>=0.350696727633\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] Epoch[87] Batch[0] avg_epoch_loss=0.254176\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:23 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=0.254176408052\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] Epoch[87] Batch[5] avg_epoch_loss=0.363797\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=0.363797277212\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] Epoch[87] Batch [5]#011Speed: 541.97 samples/sec#011loss=0.363797\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] processed a total of 597 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1421.9858646392822, \"sum\": 1421.9858646392822, \"min\": 1421.9858646392822}}, \"EndTime\": 1591047084.521496, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047083.099012}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=419.803441417 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #progress_metric: host=algo-1, completed 88 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] #quality_metric: host=algo-1, epoch=87, train loss <loss>=0.396937271953\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:24 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch[0] avg_epoch_loss=0.289145\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=0.289145261049\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch[5] avg_epoch_loss=0.392511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=0.392511074742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:25 INFO 140176366790464] Epoch[88] Batch [5]#011Speed: 981.72 samples/sec#011loss=0.392511\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[88] Batch[10] avg_epoch_loss=0.340901\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=0.278968140483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[88] Batch [10]#011Speed: 529.18 samples/sec#011loss=0.278968\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.72096824646, \"sum\": 1510.72096824646, \"min\": 1510.72096824646}}, \"EndTime\": 1591047086.032762, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047084.521568}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=448.096602424 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #progress_metric: host=algo-1, completed 89 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=88, train loss <loss>=0.340900650079\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch[0] avg_epoch_loss=0.335540\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=0.335540384054\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch[5] avg_epoch_loss=0.382849\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=0.382848918438\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:26 INFO 140176366790464] Epoch[89] Batch [5]#011Speed: 1029.79 samples/sec#011loss=0.382849\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] Epoch[89] Batch[10] avg_epoch_loss=0.352780\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, batch=10 train loss <loss>=0.316697520018\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] Epoch[89] Batch [10]#011Speed: 502.08 samples/sec#011loss=0.316698\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] processed a total of 660 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1552.7589321136475, \"sum\": 1552.7589321136475, \"min\": 1552.7589321136475}}, \"EndTime\": 1591047087.586005, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047086.032839}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=425.02116707 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #progress_metric: host=algo-1, completed 90 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] #quality_metric: host=algo-1, epoch=89, train loss <loss>=0.352780100974\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:27 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch[0] avg_epoch_loss=0.313433\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=0.313433468342\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch[5] avg_epoch_loss=0.367483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=0.367483228445\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:28 INFO 140176366790464] Epoch[90] Batch [5]#011Speed: 1022.45 samples/sec#011loss=0.367483\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] processed a total of 614 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1422.2710132598877, \"sum\": 1422.2710132598877, \"min\": 1422.2710132598877}}, \"EndTime\": 1591047089.008802, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047087.586078}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=431.6702916 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #progress_metric: host=algo-1, completed 91 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=90, train loss <loss>=0.355760231614\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch[0] avg_epoch_loss=0.386939\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=0.386939287186\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch[5] avg_epoch_loss=0.337555\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=0.337554509441\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:29 INFO 140176366790464] Epoch[91] Batch [5]#011Speed: 1041.99 samples/sec#011loss=0.337555\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] Epoch[91] Batch[10] avg_epoch_loss=0.315931\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, batch=10 train loss <loss>=0.289982205629\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] Epoch[91] Batch [10]#011Speed: 536.53 samples/sec#011loss=0.289982\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1495.9590435028076, \"sum\": 1495.9590435028076, \"min\": 1495.9590435028076}}, \"EndTime\": 1591047090.505342, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047089.008873}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=434.468213817 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #progress_metric: host=algo-1, completed 92 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] #quality_metric: host=algo-1, epoch=91, train loss <loss>=0.315930734981\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:30 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch[0] avg_epoch_loss=0.166290\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=0.166289806366\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch[5] avg_epoch_loss=0.299475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=0.299474984407\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:31 INFO 140176366790464] Epoch[92] Batch [5]#011Speed: 1017.66 samples/sec#011loss=0.299475\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[92] Batch[10] avg_epoch_loss=0.379207\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, batch=10 train loss <loss>=0.474884578586\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[92] Batch [10]#011Speed: 491.61 samples/sec#011loss=0.474885\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1586.287021636963, \"sum\": 1586.287021636963, \"min\": 1586.287021636963}}, \"EndTime\": 1591047092.09218, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047090.505424}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=417.926620594 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #progress_metric: host=algo-1, completed 93 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=92, train loss <loss>=0.379206618125\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] Epoch[93] Batch[0] avg_epoch_loss=0.199889\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:32 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=0.199889168143\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch[5] avg_epoch_loss=0.356010\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=0.356009580195\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch [5]#011Speed: 1051.38 samples/sec#011loss=0.356010\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch[10] avg_epoch_loss=0.448569\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, batch=10 train loss <loss>=0.559640912712\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] Epoch[93] Batch [10]#011Speed: 501.86 samples/sec#011loss=0.559641\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1549.9329566955566, \"sum\": 1549.9329566955566, \"min\": 1549.9329566955566}}, \"EndTime\": 1591047093.642651, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047092.092259}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=421.277051504 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #progress_metric: host=algo-1, completed 94 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] #quality_metric: host=algo-1, epoch=93, train loss <loss>=0.448569276793\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:33 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch[0] avg_epoch_loss=0.254592\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=0.254591703415\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch[5] avg_epoch_loss=0.293480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=0.293480438491\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:34 INFO 140176366790464] Epoch[94] Batch [5]#011Speed: 1026.93 samples/sec#011loss=0.293480\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1451.9438743591309, \"sum\": 1451.9438743591309, \"min\": 1451.9438743591309}}, \"EndTime\": 1591047095.095101, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047093.64273}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=439.380910331 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #progress_metric: host=algo-1, completed 95 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=94, train loss <loss>=0.320347319543\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch[0] avg_epoch_loss=0.343025\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=0.343025177717\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch[5] avg_epoch_loss=0.346083\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=0.346083333095\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:35 INFO 140176366790464] Epoch[95] Batch [5]#011Speed: 1041.85 samples/sec#011loss=0.346083\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1432.0080280303955, \"sum\": 1432.0080280303955, \"min\": 1432.0080280303955}}, \"EndTime\": 1591047096.527696, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047095.095165}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=436.417442713 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #progress_metric: host=algo-1, completed 96 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] #quality_metric: host=algo-1, epoch=95, train loss <loss>=0.301443825662\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:36 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch[0] avg_epoch_loss=0.244632\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=0.244631811976\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch[5] avg_epoch_loss=0.376419\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=0.376419280966\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:37 INFO 140176366790464] Epoch[96] Batch [5]#011Speed: 1009.08 samples/sec#011loss=0.376419\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[96] Batch[10] avg_epoch_loss=0.365389\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=0.352151852846\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[96] Batch [10]#011Speed: 537.39 samples/sec#011loss=0.352152\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] processed a total of 647 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1510.0839138031006, \"sum\": 1510.0839138031006, \"min\": 1510.0839138031006}}, \"EndTime\": 1591047098.038372, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047096.527763}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=428.417838138 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #progress_metric: host=algo-1, completed 97 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=96, train loss <loss>=0.365388631821\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch[0] avg_epoch_loss=0.251817\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=0.251817017794\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch[5] avg_epoch_loss=0.284140\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=0.284139735003\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:38 INFO 140176366790464] Epoch[97] Batch [5]#011Speed: 1048.45 samples/sec#011loss=0.284140\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1426.9750118255615, \"sum\": 1426.9750118255615, \"min\": 1426.9750118255615}}, \"EndTime\": 1591047099.465906, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047098.038456}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=444.95802743 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #progress_metric: host=algo-1, completed 98 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] #quality_metric: host=algo-1, epoch=97, train loss <loss>=0.292433904111\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:39 INFO 140176366790464] loss did not improve\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[0] avg_epoch_loss=0.298185\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=0.298184663057\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[5] avg_epoch_loss=0.278262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=0.278261783222\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch [5]#011Speed: 1058.38 samples/sec#011loss=0.278262\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch[10] avg_epoch_loss=0.250298\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=0.216742414236\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Epoch[98] Batch [10]#011Speed: 534.39 samples/sec#011loss=0.216742\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1488.5509014129639, \"sum\": 1488.5509014129639, \"min\": 1488.5509014129639}}, \"EndTime\": 1591047100.95501, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047099.465992}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=433.944986536 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #progress_metric: host=algo-1, completed 99 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] #quality_metric: host=algo-1, epoch=98, train loss <loss>=0.250298433683\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:40 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_b067c683-1682-40eb-b98c-1634f320691e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 31.255006790161133, \"sum\": 31.255006790161133, \"min\": 31.255006790161133}}, \"EndTime\": 1591047100.986819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047100.955089}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch[0] avg_epoch_loss=0.244077\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=0.244077175856\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch[5] avg_epoch_loss=0.288435\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=0.288434584936\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:41 INFO 140176366790464] Epoch[99] Batch [5]#011Speed: 1021.09 samples/sec#011loss=0.288435\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 1450.3068923950195, \"sum\": 1450.3068923950195, \"min\": 1450.3068923950195}}, \"EndTime\": 1591047102.437268, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047100.986893}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #throughput_metric: host=algo-1, train throughput=410.913189961 records/second\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #quality_metric: host=algo-1, epoch=99, train loss <loss>=0.248870485276\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/state_8df91455-93ef-442c-9291-5871817a62bc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 20.274877548217773, \"sum\": 20.274877548217773, \"min\": 20.274877548217773}}, \"EndTime\": 1591047102.45819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.437347}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Final loss: 0.248870485276 (occurred at epoch 99)\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] #quality_metric: host=algo-1, train final_loss <loss>=0.248870485276\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 WARNING 140176366790464] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 265.9471035003662, \"sum\": 265.9471035003662, \"min\": 265.9471035003662}}, \"EndTime\": 1591047102.72481, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.458267}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 325.5000114440918, \"sum\": 325.5000114440918, \"min\": 325.5000114440918}}, \"EndTime\": 1591047102.784321, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.724895}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 9.264945983886719, \"sum\": 9.264945983886719, \"min\": 9.264945983886719}}, \"EndTime\": 1591047102.793689, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.784379}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:42 INFO 140176366790464] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.031948089599609375, \"sum\": 0.031948089599609375, \"min\": 0.031948089599609375}}, \"EndTime\": 1591047102.794466, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.793744}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:45 INFO 140176366790464] Number of test batches scored: 10\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:48 INFO 140176366790464] Number of test batches scored: 20\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:51 INFO 140176366790464] Number of test batches scored: 30\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:54 INFO 140176366790464] Number of test batches scored: 40\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:31:57 INFO 140176366790464] Number of test batches scored: 50\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:00 INFO 140176366790464] Number of test batches scored: 60\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:03 INFO 140176366790464] Number of test batches scored: 70\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 22959.881067276, \"sum\": 22959.881067276, \"min\": 22959.881067276}}, \"EndTime\": 1591047125.754312, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047102.794517}\n",
      "\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, RMSE): 0.724123434059\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, mean_absolute_QuantileLoss): 1720.1710181790002\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, mean_wQuantileLoss): 0.4346061187920667\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.1]): 0.5391333572234263\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.2]): 0.5227500572602042\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.3]): 0.49925785692074076\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.4]): 0.47178476693552457\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.5]): 0.4414293351376853\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.6]): 0.4089470922154905\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.7]): 0.3755974119798784\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.8]): 0.343263300519414\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #test_score (algo-1, wQuantileLoss[0.9]): 0.30929189093623677\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.434606118792\u001b[0m\n",
      "\u001b[34m[06/01/2020 21:32:05 INFO 140176366790464] #quality_metric: host=algo-1, test RMSE <loss>=0.724123434059\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 178027.48608589172, \"sum\": 178027.48608589172, \"min\": 178027.48608589172}, \"setuptime\": {\"count\": 1, \"max\": 8.664131164550781, \"sum\": 8.664131164550781, \"min\": 8.664131164550781}}, \"EndTime\": 1591047125.772278, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1591047125.754382}\n",
      "\u001b[0m\n",
      "\n",
      "2020-06-01 21:32:14 Uploading - Uploading generated training model\n",
      "2020-06-01 21:32:14 Completed - Training job completed\n",
      "Training seconds: 235\n",
      "Billable seconds: 235\n",
      "CPU times: user 900 ms, sys: 76.3 ms, total: 976 ms\n",
      "Wall time: 6min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\n",
    "    \"train\": s3_data_path + \"/train/train.json\",\n",
    "    \"test\": s3_data_path + \"/test/test.json\"\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    content_type=\"application/json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_request(instance, num_samples, quantiles):\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\"],\n",
    "            \"quantiles\": quantiles\n",
    "        }\n",
    "        \n",
    "        http_request_data = {\n",
    "            \"instances\": [instance],\n",
    "            \"configuration\": configuration\n",
    "        }\n",
    "        \n",
    "        return json.dumps(http_request_data).encode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictor = sagemaker.predictor.RealTimePredictor(endpoint='MLEND-Capstone-Project-2020-05-31-23-12-35-007')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prediction(ticker,date):\n",
    "    try:\n",
    "        date_pred = pd.Timestamp(date, freq='D')\n",
    "        date_start = date_pred-50\n",
    "        pred_df = stock_data_preprocessed.loc[(slice(str(date_start),str(date_pred)), ticker), :]\n",
    "        result_df = pred_df.loc[(slice(str(date_pred),str(date_pred)), ticker), :]\n",
    "\n",
    "        pred = {\n",
    "                \"start\": str(date_pred),\n",
    "                \"target\": pred_df['target'][date_start:date_pred-1].tolist(),\n",
    "                \"dynamic_feat\": pred_df[['Adj Close','Volume']][date_start:date_pred].values.T.tolist()\n",
    "        }\n",
    "\n",
    "        req = encode_request(instance=pred, num_samples=50, quantiles=['0.1', '0.5', '0.9'])\n",
    "        res = predictor.predict(req)\n",
    "\n",
    "        prediction_data = json.loads(res.decode('utf-8'))\n",
    "        pred = round(prediction_data['predictions'][0]['quantiles']['0.5'][0])\n",
    "        result_df['prediction'] = pred\n",
    "        return result_df\n",
    "    except:\n",
    "        print('{} did not trade today.'.format(ticker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:10: FutureWarning: Addition/subtraction of integers and integer-arrays to Timestamp is deprecated, will be removed in a future version.  Instead of adding/subtracting `n`, use `n * self.freq`\n",
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>target</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <th>AAPL</th>\n",
       "      <td>0.03886</td>\n",
       "      <td>0.026907</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Adj Close    Volume  target  prediction\n",
       "Date       Ticker                                         \n",
       "2019-01-23 AAPL      0.03886  0.026907       1           1"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_prediction('AAPL', '2019-01-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_index = pd.read_csv('date_index.csv')\n",
    "date_index = date_index.values.reshape(252).tolist()\n",
    "\n",
    "def get_prediction_accuracy(ticker, date_range):\n",
    "    ticker = str(ticker)\n",
    "    i = 0\n",
    "    target = []\n",
    "    prediction = []\n",
    "\n",
    "    for date in date_range:\n",
    "        target.append(get_stock_prediction(ticker, date)['target'].values[0])\n",
    "        prediction.append(int(get_stock_prediction(ticker, date)['prediction'].values[0]))\n",
    "    target = list(np.array(target).reshape(252))\n",
    "    prediction = list(np.array(prediction).reshape(252))\n",
    "    data = {'target': list(target), 'prediction': list(prediction)}\n",
    "    prediction_df = pd.DataFrame(data=data,index=date_index, columns=['target','prediction'])\n",
    "    \n",
    "    return accuracy_score(target, prediction), prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_prediction_accuracy('AAPL', date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
